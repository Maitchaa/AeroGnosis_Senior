# -*- coding: utf-8 -*-
"""SSS Configuration 1 + Quantification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ip6pjlqIbLZI-tjy4Vy8sEptxSVnFY-b

# 1. Setting up the Enivronment
"""

#mount drive
from google.colab import drive
drive.mount('/content/drive')

#install dependencies
!pip install -q torch torchvision
!pip install -q segmentation-models-pytorch
!pip install -q tensorboard
!pip install -q pyyaml
!pip install -q opencv-python
!pip install -q matplotlib

"""# 2. Dataset Configuration and Verification"""

DATA_ROOT = "/content/drive/MyDrive/aircraft_voc"

#verifying dataset structure
import os
from pathlib import Path

def verify_dataset_structure(data_root):
    required_dirs = [
        'JPEGImages',
        'SegmentationClass',
        'ImageSets/Segmentation'
    ]

    all_exist = True

    for dir_path in required_dirs:
        full_path = os.path.join(data_root, dir_path)
        exists = os.path.exists(full_path)

        if exists:
            # Count files
            if os.path.isdir(full_path):
                num_files = len([f for f in os.listdir(full_path) if os.path.isfile(os.path.join(full_path, f))])
                print(f"  {dir_path}: {num_files} files")
        else:
            print(f" {dir_path} is NOT FOUND")
            all_exist = False

    return all_exist

dataset_valid = verify_dataset_structure(DATA_ROOT)

if not dataset_valid:
    print("\n Something wrong")
else:
    print("\n Dataset all good")

"""#3. Create UNet++ with MobileNetV2"""

#create junior UNet++ model
unetpp_model_code = '''import torch
import torch.nn as nn
import torch.nn.functional as F
import segmentation_models_pytorch as smp

class UNetPlusPlus(nn.Module):
    #UNet++ model with MobileNetV2 encoder for semantic segmentation

    def __init__(self, num_classes, encoder_name='mobilenet_v2', pretrained=True):
        super(UNetPlusPlus, self).__init__()

        self.num_classes = num_classes
        self.encoder_name = encoder_name

        #creation of UNet++ model with MobileNetV2 encoder
        self.backbone = smp.UnetPlusPlus(
            encoder_name=encoder_name,
            encoder_weights='imagenet' if pretrained else None,
            in_channels=3,
            classes=num_classes,
            activation=None,  #no activation, return logits
        )

    def forward(self, x, need_fp=False):
           # x: input tensor [B, 3, H, W]
           # need_fp: whether to return feature maps (for consistency regularization)
        #Returns:
            #logits [B, num_classes, H, W] or (logits, features) if need_fp
        # Get output from UNet++
        out = self.backbone(x)

        if need_fp:
            # Use the output as feature representation for consistency loss
            return out, out

        return out

    def decode(self, features):
        return features

def UNetPlusPlusMobileNetV2(num_classes, pretrained=True):
    return UNetPlusPlus(num_classes=num_classes, encoder_name='mobilenet_v2', pretrained=pretrained)
'''

#writing UNet++ model to file
os.makedirs('/content/my_project/model/semseg', exist_ok=True)
with open('/content/my_project/model/semseg/unetplusplus.py', 'w') as f:
    f.write(unetpp_model_code)

print("Created UNet++ model file: model/semseg/unetplusplus.py")
print("  Encoder: MobileNetV2 (lightweight and efficient)")
print("  Decoder: UNet++ nested skip connections")

"""# 5. Create Semi-Supervised Training Script"""

#comprehensive semi-supervised training script
training_script = f'''#!/usr/bin/env python3

#config 1 of Semi-Supervised Training UNet++ only implements Pseudo-labeling with confidence thresholding

import argparse
import os
import random

import numpy as np
import torch
from torch import nn
from torch.optim import SGD, Adam
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as T
import albumentations as A
from albumentations.pytorch import ToTensorV2
import sys

sys.path.insert(0, '/content/my_project')

from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler #mixed precision training

DATA_ROOT = "{DATA_ROOT}"
NUM_CLASSES = 2

# Simple logger and utility classes
class SimpleLogger:
    def __init__(self, name):
        self.name = name

    def info(self, msg):
        print(f"[INFO] {{msg}}")

class AverageMeter:
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def dice_loss(preds, targets, smooth=1e-6):
    #Dice loss for binary segmentation matching our crack detection pipeline EXACTLY
    probs = torch.softmax(preds, dim=1)[:, 1, :, :]  #class 1 (crack)
    targets = (targets == 1).float()

    probs = probs.contiguous().view(-1)
    targets = targets.contiguous().view(-1)

    intersection = (probs * targets).sum()
    dice = (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)

    return 1 - dice

def compute_metrics(pred, target, num_classes=2, ignore_index=255):
    pred = pred.flatten()
    target = target.flatten()

    # Filter out ignore index
    valid_mask = (target != ignore_index)
    pred = pred[valid_mask]
    target = target[valid_mask]

    metrics = {{}}

    #per-class metrics
    ious = []
    dices = []
    precisions = []
    recalls = []

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        #True Positives, False Positives, False Negatives
        tp = (pred_cls & target_cls).sum().float()
        fp = (pred_cls & ~target_cls).sum().float()
        fn = (~pred_cls & target_cls).sum().float()

        #IoU
        union = tp + fp + fn
        iou = tp / (union + 1e-8)
        ious.append(iou.item())

        #Dice
        dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)
        dices.append(dice.item())

        #Precision
        precision = tp / (tp + fp + 1e-8)
        precisions.append(precision.item())

        #Recall
        recall = tp / (tp + fn + 1e-8)
        recalls.append(recall.item())

    #Accuracy
    accuracy = (pred == target).float().mean().item()

    metrics['iou'] = np.mean(ious)
    metrics['dice'] = np.mean(dices)
    metrics['accuracy'] = accuracy
    metrics['precision'] = np.mean(precisions)
    metrics['recall'] = np.mean(recalls)

    return metrics

def validate(model, dataloader, device, num_classes):
    model.eval()
    all_metrics = {{'iou': [], 'dice': [], 'accuracy': [], 'precision': [], 'recall': []}}

    with torch.no_grad():
        for images, masks in dataloader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)

            batch_metrics = compute_metrics(preds, masks, num_classes)
            for k in all_metrics:
                all_metrics[k].append(batch_metrics[k])

    return {{k: np.mean(v) for k, v in all_metrics.items()}}

def parse_args():
    parser = argparse.ArgumentParser(description='Semi-Supervised Semantic Segmentation')
    parser.add_argument('--config', type=str, default='configs/aircraft.yaml')
    parser.add_argument('--labeled-id-path', type=str, default='splits/aircraft/labeled.txt')
    parser.add_argument('--unlabeled-id-path', type=str, default='splits/aircraft/unlabeled.txt')
    parser.add_argument('--save-path', type=str, default='exp/aircraft_unetpp')
    parser.add_argument('--local_rank', type=int, default=0)
    parser.add_argument('--port', default=None, type=int)

    args = parser.parse_args([])  # Empty list for Colab
    return args

def create_split_files(data_root, labeled_ratio=0.1):
    """Create labeled/unlabeled splits for semi-supervised learning"""
    import random

    #Read all training images
    train_file = os.path.join(data_root, 'ImageSets/Segmentation/train.txt')

    with open(train_file, 'r') as f:
        all_ids = [line.strip() for line in f.readlines()]

    if len(all_ids) == 0:
        print("Error: No image IDs found in train file!")
        return False

    # Shuffle and split
    random.shuffle(all_ids)
    num_labeled = max(1, int(len(all_ids) * labeled_ratio))
    labeled_ids = all_ids[:num_labeled]
    unlabeled_ids = all_ids[num_labeled:]

    # Create split directory
    os.makedirs('splits/aircraft', exist_ok=True)

    # Save splits
    with open('splits/aircraft/labeled.txt', 'w') as f:
        f.write('\\n'.join(labeled_ids))

    with open('splits/aircraft/unlabeled.txt', 'w') as f:
        f.write('\\n'.join(unlabeled_ids))

    print(f" Created splits: {{num_labeled}} labeled, {{len(unlabeled_ids)}} unlabeled")
    print(f"  Labeled ratio: {{labeled_ratio*100:.1f}}%")

    return True

def get_default_config():
    """Default configuration for training. Matching our aircraft crack pipeline"""
    return {{
        'dataset': 'aircraft',
        'data_root': DATA_ROOT,
        'batch_size': 4,
        'lr': 0.0005,  # max_lr
        'min_lr': 0.000001,
        'epochs': 40,
        'crop_size': 512,
        'backbone': 'unetplusplus',
        'num_classes': NUM_CLASSES,

        # Semi-supervised settings
        'labeled_ratio': 0.7,
        'unsup_weight': 1.0,
        'pseudo_threshold': 0.95,
        'use_strong_aug': True,

        # Optimizer
        'optimizer': 'adam',
        'momentum': 0.9,
        'weight_decay': 0,
        'lr_decay_type': 'cos',

        # Training settings
        'num_workers': 4,
        'mixed_precision': True,  # 16-bit AMP
        'sync_bn': True,

        # Early stopping
        'early_stopping': True,
        'patience': 10,
        'monitor': 'val_dice',  # Monitor dice score

        # Logging
        'print_interval': 10,
        'eval_interval': 1,  # Validate every epoch
        'save_period': 20,
    }}

class SimpleDataset(Dataset):
    #Dataset with augmentations matching our pipeline EXACTLY"""
    def __init__(self, data_root, split_file, phase='train', size=512):
        self.data_root = data_root
        self.phase = phase
        self.size = size

        with open(split_file, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]

        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]

        if phase == 'train':
            self.transform = A.Compose([
                A.RandomScale(scale_limit=0.5, p=0.5),
                A.Rotate(limit=10, p=0.5),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.Resize(size, size),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])
        else:
            self.transform = A.Compose([
                A.Resize(size, size),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]

        #Load image
        img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.jpg")
        if not os.path.exists(img_path):
            img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.png")
        img = Image.open(img_path).convert('RGB')

        #Load mask
        mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.png")
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.jpg")
        mask = Image.open(mask_path)

        #IMP FIX: Resize BEFORE converting to numpy for same dimensions
        img = img.resize((self.size, self.size), Image.BILINEAR)
        mask = mask.resize((self.size, self.size), Image.NEAREST)

        #Convert to numpy for Albumentations
        img = np.array(img)
        mask = np.array(mask)

        #Apply transforms (for both image and mask)
        augmented = self.transform(image=img, mask=mask)
        img = augmented['image']
        mask = augmented['mask'].long()

        return img, mask

def main():
    args = parse_args()
    cfg = get_default_config()

    #Setup logging
    os.makedirs(args.save_path, exist_ok=True)
    logger = SimpleLogger('global')

    #Set random seed
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)

    # Create labeled/unlabeled splits
    print("\\nCreating semi-supervised data splits...")
    if not create_split_files(DATA_ROOT, labeled_ratio=cfg['labeled_ratio']):
        print("Error creating splits!")
        return

    #Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\\nUsing device: {{device}}")

    #Create model
    print("\\nCreating UNet++ model with MobileNetV2 encoder...")
    model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=True)
    model = model.to(device)

    #Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  Total parameters: {{total_params:,}}")

    #Optimizer
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=cfg['lr'],
        weight_decay=cfg['weight_decay']
    )

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg['epochs'],
        eta_min=cfg['min_lr']
    )

    # Loss functions. HYBRID LOSS like our pipeline
    criterion_ce = nn.CrossEntropyLoss(ignore_index=255).to(device)  # CrossEntropy
    criterion_u = nn.CrossEntropyLoss(ignore_index=255, reduction='none').to(device)

    # Mixed precision training scaler
    scaler = GradScaler() if cfg['mixed_precision'] else None

    # Load datasets
    print(f"\\nLoading datasets from {{DATA_ROOT}}...")

    trainset_l = SimpleDataset(DATA_ROOT, 'splits/aircraft/labeled.txt', phase='train', size=cfg['crop_size'])
    trainset_u = SimpleDataset(DATA_ROOT, 'splits/aircraft/unlabeled.txt', phase='train', size=cfg['crop_size'])

    trainloader_l = DataLoader(trainset_l, batch_size=cfg['batch_size'],
                               shuffle=True, num_workers=cfg['num_workers'], drop_last=True, pin_memory=True)
    trainloader_u = DataLoader(trainset_u, batch_size=cfg['batch_size'],
                               shuffle=True, num_workers=cfg['num_workers'], drop_last=True, pin_memory=True)

    print(f"  Labeled samples: {{len(trainset_l)}}")
    print(f"  Unlabeled samples: {{len(trainset_u)}}")

    #Training loop
    print(f"\\nStarting semi-supervised training for {{cfg['epochs']}} epochs...")
    print(f"Mixed Precision: {{cfg['mixed_precision']}}")
    print(f"Early Stopping: {{cfg['early_stopping']}} (patience={{cfg['patience']}})")
    print("=" * 80)

    best_loss = float('inf')
    best_dice = 0.0
    patience_counter = 0

    for epoch in range(cfg['epochs']):
        model.train()

        losses_l = AverageMeter()
        losses_u = AverageMeter()
        losses_total = AverageMeter()

        loader_l_iter = iter(trainloader_l)
        loader_u_iter = iter(trainloader_u)

        num_batches = min(len(trainloader_l), len(trainloader_u))

        for i in range(num_batches):
            # Get labeled batch
            try:
                img_l, mask_l = next(loader_l_iter)
            except StopIteration:
                loader_l_iter = iter(trainloader_l)
                img_l, mask_l = next(loader_l_iter)

            # Get unlabeled batch
            try:
                img_u, _ = next(loader_u_iter)
            except StopIteration:
                loader_u_iter = iter(trainloader_u)
                img_u, _ = next(loader_u_iter)

            img_l, mask_l = img_l.to(device), mask_l.to(device)
            img_u = img_u.to(device)

            optimizer.zero_grad()

            # Mixed precision training
            if cfg['mixed_precision']:
                with autocast():
                    # Forward pass on labeled data with HYBRID LOSS
                    pred_l = model(img_l)
                    ce_loss = criterion_ce(pred_l, mask_l)
                    d_loss = dice_loss(pred_l, mask_l)
                    loss_l = ce_loss + 0.5 * d_loss  # HYBRID: CE + 0.5*Dice

                    # Forward pass on unlabeled data (pseudo-labeling)
                    with torch.no_grad():
                        pred_u = model(img_u)
                        prob_u = F.softmax(pred_u, dim=1)
                        conf_u, pseudo_u = torch.max(prob_u, dim=1)
                        mask_u = (conf_u >= cfg['pseudo_threshold']).float()

                    # Supervised loss on pseudo-labels
                    pred_u_strong = model(img_u)
                    loss_u = criterion_u(pred_u_strong, pseudo_u)
                    loss_u = (loss_u * mask_u).sum() / (mask_u.sum() + 1e-8)

                    # Total loss
                    loss = loss_l + cfg['unsup_weight'] * loss_u

                # Backward with gradient scaling
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                #Standard training (no mixed precision) with HYBRID LOSS
                pred_l = model(img_l)
                ce_loss = criterion_ce(pred_l, mask_l)
                d_loss = dice_loss(pred_l, mask_l)
                loss_l = ce_loss + 0.5 * d_loss  # HYBRID: CE + 0.5*Dice

                with torch.no_grad():
                    pred_u = model(img_u)
                    prob_u = F.softmax(pred_u, dim=1)
                    conf_u, pseudo_u = torch.max(prob_u, dim=1)
                    mask_u = (conf_u >= cfg['pseudo_threshold']).float()

                pred_u_strong = model(img_u)
                loss_u = criterion_u(pred_u_strong, pseudo_u)
                loss_u = (loss_u * mask_u).sum() / (mask_u.sum() + 1e-8)

                loss = loss_l + cfg['unsup_weight'] * loss_u

                loss.backward()
                optimizer.step()

            #Update metrics
            losses_l.update(loss_l.item())
            losses_u.update(loss_u.item())
            losses_total.update(loss.item())

            if (i + 1) % cfg['print_interval'] == 0:
                print(f"Epoch [{{epoch+1}}/{{cfg['epochs']}}] "
                      f"Iter [{{i+1}}/{{num_batches}}] "
                      f"Loss: {{losses_total.avg:.4f}} "
                      f"(L: {{losses_l.avg:.4f}}, U: {{losses_u.avg:.4f}})")

        # Validation phase
        if (epoch + 1) % cfg['eval_interval'] == 0:
            model.eval()
            val_metrics = validate(model, trainloader_l, device, cfg['num_classes'])
            model.train()

            print(f"\\nEpoch {{epoch+1}} completed:")
            print(f"  Train Loss: {{losses_total.avg:.4f}} (Labeled: {{losses_l.avg:.4f}}, Unlabeled: {{losses_u.avg:.4f}})")
            print(f"  Val IoU: {{val_metrics['iou']:.4f}}")
            print(f"  Val Dice: {{val_metrics['dice']:.4f}}")
            print(f"  Val Accuracy: {{val_metrics['accuracy']:.4f}}")
            print(f"  Val Precision: {{val_metrics['precision']:.4f}}")
            print(f"  Val Recall: {{val_metrics['recall']:.4f}}")
            print(f"  Learning Rate: {{optimizer.param_groups[0]['lr']:.6f}}")

            # Save best model based on dice score
            if val_metrics['dice'] > best_dice:
                best_dice = val_metrics['dice']
                patience_counter = 0
                checkpoint = {{
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'dice': best_dice,
                    'loss': losses_total.avg,
                }}
                torch.save(checkpoint, os.path.join(args.save_path, f'best_model_dice={{best_dice:.2f}}.pth'))
                print(f"  Saved best model (dice: {{best_dice:.4f}})")
            else:
                patience_counter += 1
                print(f"  No improvement (patience: {{patience_counter}}/{{cfg['patience']}})")

            # Early stopping
            if cfg['early_stopping'] and patience_counter >= cfg['patience']:
                print(f"\\n  Early stopping triggered! No improvement for {{cfg['patience']}} epochs.")
                break

        # Step the learning rate scheduler
        scheduler.step()

        # Periodic checkpoint saving
        if (epoch + 1) % cfg['save_period'] == 0:
            checkpoint = {{
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': losses_total.avg,
            }}
            torch.save(checkpoint, os.path.join(args.save_path, f'epoch_{{epoch+1}}.pth'))
            print(f"  Saved periodic checkpoint: epoch_{{epoch+1}}.pth")

        print("=" * 80)

    print("\\n Training complete!")
    print(f"Best loss: {{best_loss:.4f}}")
    print(f"Model saved to: {{args.save_path}}/best_model.pth")

if __name__ == '__main__':
    main()
'''

with open('/content/my_project/train_semi.py', 'w') as f:
    f.write(training_script)

print("✓ Created semi-supervised training script: train_semi.py")

"""# 6. Evaluation"""

eval_script = f'''#!/usr/bin/env python3
"""
Evaluation script for trained UNet++ model
"""

import os
import sys
import torch
import numpy as np
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as T
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

sys.path.insert(0, '/content/my_project')
from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2

DATA_ROOT = "{DATA_ROOT}"
NUM_CLASSES = 2
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class EvalDataset(Dataset):
    def __init__(self, data_root, split_file, transform=None, size=512):
        self.data_root = data_root
        self.transform = transform
        self.size = size

        with open(split_file, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]

        # Load image
        img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.jpg")
        if not os.path.exists(img_path):
            img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.png")
        img = Image.open(img_path).convert('RGB')

        #Load mask
        mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.png")
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.jpg")
        mask = Image.open(mask_path)

        # Store original as numpy array (not PIL Image)
        img_orig = np.array(img.copy())

        #Resize
        img = img.resize((self.size, self.size), Image.BILINEAR)
        mask = mask.resize((self.size, self.size), Image.NEAREST)

        if self.transform:
            img = self.transform(img)

        mask = torch.from_numpy(np.array(mask)).long()

        return img, mask, img_orig, img_id

def compute_iou(pred, target, num_classes, ignore_index=255):
    """Compute mean IoU"""
    ious = []
    pred = pred.flatten()
    target = target.flatten()

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        # Ignore pixels marked as ignore_index
        valid = (target != ignore_index)
        pred_cls = pred_cls & valid
        target_cls = target_cls & valid

        intersection = (pred_cls & target_cls).sum()
        union = (pred_cls | target_cls).sum()

        if union == 0:
            continue

        iou = intersection.float() / union.float()
        ious.append(iou.item())

    return np.mean(ious) if ious else 0.0

def compute_all_metrics(pred, target, num_classes, ignore_index=255):
    #Compute IoU, Dice, Precision, Recall, Accuracy
    pred = pred.flatten()
    target = target.flatten()

    # Filter out ignore index
    valid_mask = (target != ignore_index)
    pred = pred[valid_mask]
    target = target[valid_mask]

    ious = []
    dices = []
    precisions = []
    recalls = []

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        #True Positives, False Positives, False Negatives
        tp = (pred_cls & target_cls).sum().float()
        fp = (pred_cls & ~target_cls).sum().float()
        fn = (~pred_cls & target_cls).sum().float()

        # IoU
        union = tp + fp + fn
        iou = tp / (union + 1e-8)
        ious.append(iou.item())

        #Dice (F1)
        dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)
        dices.append(dice.item())

        #Precision
        precision = tp / (tp + fp + 1e-8)
        precisions.append(precision.item())

        #Recall
        recall = tp / (tp + fn + 1e-8)
        recalls.append(recall.item())

    # Overall Accuracy
    accuracy = (pred == target).float().mean().item()

    return {{
        'iou': ious,
        'dice': dices,
        'precision': precisions,
        'recall': recalls,
        'accuracy': accuracy,
        'mean_iou': np.mean(ious),
        'mean_dice': np.mean(dices),
        'mean_precision': np.mean(precisions),
        'mean_recall': np.mean(recalls)
    }}

#Load model
print("Loading trained model...")
model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=False)

# Find the best model file (with dice score in filename)
import glob
checkpoint_dir = 'exp/aircraft_unetpp'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'best_model_dice=*.pth'))

if checkpoint_files:
    # Get the file with highest dice score
    checkpoint_path = max(checkpoint_files, key=lambda x: float(x.split('dice=')[1].split('.pth')[0]))
    print(f"Found best checkpoint: {{os.path.basename(checkpoint_path)}}")
else:
    #Fallback to generic name if it exists
    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')
    if not os.path.exists(checkpoint_path):
        print("Error: No checkpoint found!")
        print(f"Looking in: {{checkpoint_dir}}")
        import sys
        sys.exit(1)

checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(DEVICE)
model.eval()

print(f"Model loaded from epoch {{checkpoint['epoch']}}")
if 'dice' in checkpoint:
    print(f"Best Dice score: {{checkpoint['dice']:.4f}}")

#Load validation dataset
val_transform = T.Compose([
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_file = os.path.join(DATA_ROOT, 'ImageSets/Segmentation/val.txt')
val_dataset = EvalDataset(DATA_ROOT, val_file, val_transform, size=512)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)

print(f"\\nEvaluating on {{len(val_dataset)}} validation images...")

# Evaluate
os.makedirs('results', exist_ok=True)
all_ious = []
all_dices = []
all_precisions = []
all_recalls = []
all_accuracies = []
all_losses = []
all_preds = []
all_targets = []

#Loss function for evaluation
criterion = torch.nn.CrossEntropyLoss(ignore_index=255)

with torch.no_grad():
    for idx, (image, mask, img_orig, img_id) in enumerate(tqdm(val_loader)):
        image = image.to(DEVICE)
        mask = mask.to(DEVICE)

        output = model(image)

        #Compute loss
        loss = criterion(output, mask)
        all_losses.append(loss.item())

        pred = output.argmax(dim=1).cpu().numpy()[0]
        mask_np = mask.cpu().numpy()[0]

        # Store for confusion matrix
        valid_mask = (mask_np != 255)
        all_preds.extend(pred[valid_mask].flatten())
        all_targets.extend(mask_np[valid_mask].flatten())

        #compute all metrics
        metrics = compute_all_metrics(torch.from_numpy(pred), torch.from_numpy(mask_np), NUM_CLASSES)
        all_ious.append(metrics['mean_iou'])
        all_dices.append(metrics['mean_dice'])
        all_precisions.append(metrics['mean_precision'])
        all_recalls.append(metrics['mean_recall'])
        all_accuracies.append(metrics['accuracy'])

        #Save visualizations for first 10 images
        if idx < 10:
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            # Original image
            axes[0].imshow(img_orig[0])
            axes[0].set_title('Input Image')
            axes[0].axis('off')

            axes[1].imshow(mask_np, cmap='tab20', vmin=0, vmax=NUM_CLASSES-1)
            axes[1].set_title('Ground Truth')
            axes[1].axis('off')

            axes[2].imshow(pred, cmap='tab20', vmin=0, vmax=NUM_CLASSES-1)
            axes[2].set_title(f'Prediction\\nIoU: {{metrics["mean_iou"]:.3f}} | Dice: {{metrics["mean_dice"]:.3f}}')
            axes[2].axis('off')

            plt.tight_layout()
            plt.savefig(f'results/prediction_{{idx}}.png', dpi=150, bbox_inches='tight')
            plt.close()

mean_iou = np.mean(all_ious)
mean_dice = np.mean(all_dices)
mean_precision = np.mean(all_precisions)
mean_recall = np.mean(all_recalls)
mean_accuracy = np.mean(all_accuracies)
mean_loss = np.mean(all_losses)

#Create confusion matrix
cm = confusion_matrix(all_targets, all_preds, labels=list(range(NUM_CLASSES)))

#Normalize confusion matrix for percentages
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

#Plot confusion matrix with both counts and percentages
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
class_names = ['Background', 'Aircraft'] if NUM_CLASSES == 2 else [f'Class {{i}}' for i in range(NUM_CLASSES)]

#Confusion Matrix Raw Counts
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
            xticklabels=class_names,
            yticklabels=class_names,
            cbar_kws={{'label': 'Count'}},
            annot_kws={{'size': 14, 'weight': 'bold'}})
ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold', pad=15)
ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax1.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

# Confusion Matrix Percentages
sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens', ax=ax2,
            xticklabels=class_names,
            yticklabels=class_names,
            cbar_kws={{'label': 'Percentage (%)'}},
            annot_kws={{'size': 14, 'weight': 'bold'}})
ax2.set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold', pad=15)
ax2.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax2.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

plt.suptitle(f'Model Performance: IoU={{mean_iou:.4f}} | Dice={{mean_dice:.4f}} | Accuracy={{mean_accuracy:.4f}}',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('results/confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.close()

print(f"\\n{{'='*70}}")
print(f"EVALUATION RESULTS")
print(f"{{'='*70}}")
print(f"\\n  Overall Metrics:")
print(f"{{'─'*70}}")
print(f"  Loss:           {{mean_loss:.4f}} (±{{np.std(all_losses):.4f}})")
print(f"  Accuracy:       {{mean_accuracy:.4f}} (±{{np.std(all_accuracies):.4f}})")
print(f"  Mean IoU:       {{mean_iou:.4f}} (±{{np.std(all_ious):.4f}})")
print(f"  Mean Dice:      {{mean_dice:.4f}} (±{{np.std(all_dices):.4f}})")
print(f"  Mean Precision: {{mean_precision:.4f}} (±{{np.std(all_precisions):.4f}})")
print(f"  Mean Recall:    {{mean_recall:.4f}} (±{{np.std(all_recalls):.4f}})")
print(f"{{'─'*70}}")
print(f"\\n Best Performance:")
print(f"{{'─'*70}}")
print(f"  Max IoU:        {{np.max(all_ious):.4f}}")
print(f"  Max Dice:       {{np.max(all_dices):.4f}}")
print(f"  Max Precision:  {{np.max(all_precisions):.4f}}")
print(f"  Max Recall:     {{np.max(all_recalls):.4f}}")
print(f"  Max Accuracy:   {{np.max(all_accuracies):.4f}}")
print(f"{{'─'*70}}")
print(f"\\n Per-Class Performance:")
print(f"{{'─'*70}}")
for i, class_name in enumerate(class_names):
    tp = cm[i, i]
    fp = cm[:, i].sum() - tp
    fn = cm[i, :].sum() - tp
    class_iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
    class_accuracy = tp / cm[i, :].sum() if cm[i, :].sum() > 0 else 0

    print(f"\\n  {{class_name}}:")
    print(f"    True Positives:  {{tp:,}}")
    print(f"    False Positives: {{fp:,}}")
    print(f"    False Negatives: {{fn:,}}")
    print(f"    Class IoU:       {{class_iou:.4f}}")
    print(f"    Class Accuracy:  {{class_accuracy:.4f}}")
print(f"\\n{{'='*70}}")
print("\\n Evaluation Complete!")
print(f"{{'─'*70}}")
print("  Saved Files:")
print("     Predictions: results/prediction_*.png (first 10 samples)")
print("     Confusion Matrix: results/confusion_matrix.png")
print(f"{{'='*70}}")
'''

with open('/content/my_project/eval_model.py', 'w') as f:
    f.write(eval_script)

print("Created evaluation script: eval_model.py")

"""# 7. Run and Evaluate"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/my_project
!python train_semi.py

!python eval_model.py

from IPython.display import Image, display
import glob

for img_path in sorted(glob.glob('results/prediction_*.png')):
    display(Image(img_path))

"""# 8. Checking for Overfitting"""

#!/usr/bin/env python3
#Simple Overfitting Check. Training vs Validation

import os
import sys
import torch
import numpy as np
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torchvision.transforms as T #T for transforms

sys.path.insert(0, '/content/my_project')
from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2

#import the dataset and metrics from eval_model
DATA_ROOT = "/content/drive/MyDrive/aircraft_voc"
NUM_CLASSES = 2
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#Iumport existing classes
exec(open('eval_model.py').read().split('# Load model')[0])

val_transform = T.Compose([
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])


def quick_evaluate(data_file, set_name):
    print(f"\n Evaluating {set_name}...")
    print("─"*70)

    #load model
    model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=False)

    import glob
    checkpoint_files = glob.glob('exp/aircraft_unetpp/best_model_dice=*.pth')
    if checkpoint_files:
        checkpoint_path = max(checkpoint_files, key=lambda x: float(x.split('dice=')[1].split('.pth')[0]))
    else:
        checkpoint_path = 'exp/aircraft_unetpp/best_model.pth'

    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(DEVICE)
    model.eval()

    # Load dataset
    val_dataset = EvalDataset(DATA_ROOT, data_file, val_transform, size=512)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)

    # Evaluate
    all_metrics = []
    all_losses = []
    criterion = torch.nn.CrossEntropyLoss(ignore_index=255)

    with torch.no_grad():
        for image, mask, img_orig, img_id in val_loader:
            image = image.to(DEVICE)
            mask = mask.to(DEVICE)

            output = model(image)
            loss = criterion(output, mask.long())
            all_losses.append(loss.item())

            pred = output.argmax(dim=1).cpu().numpy()[0]
            mask_np = mask.cpu().numpy()[0]

            metrics = compute_all_metrics(torch.from_numpy(pred), torch.from_numpy(mask_np), NUM_CLASSES)
            all_metrics.append(metrics)

    #avg results
    results = {
        'loss': np.mean(all_losses),
        'iou': np.mean([m['mean_iou'] for m in all_metrics]),
        'dice': np.mean([m['mean_dice'] for m in all_metrics]),
        'precision': np.mean([m['mean_precision'] for m in all_metrics]),
        'recall': np.mean([m['mean_recall'] for m in all_metrics]),
        'accuracy': np.mean([m['accuracy'] for m in all_metrics]),
    }

    print(f"  Loss:      {results['loss']:.4f}")
    print(f"  IoU:       {results['iou']:.4f}")
    print(f"  Dice:      {results['dice']:.4f}")
    print(f"  Precision: {results['precision']:.4f}")
    print(f"  Recall:    {results['recall']:.4f}")
    print(f"  Accuracy:  {results['accuracy']:.4f}")

    return results

print("="*70)
print("OVERFITTING ANALYSIS")
print("="*70)

#Evaluate training
train_results = quick_evaluate('splits/aircraft/labeled.txt', 'Training Set (Labeled)')

#evaluate validation
val_file = os.path.join(DATA_ROOT, 'ImageSets/Segmentation/val.txt')
val_results = quick_evaluate(val_file, 'Validation Set')

print("\n" + "="*70)
print("COMPARISON")
print("="*70)

print(f"\n{'Metric':<12} {'Training':<12} {'Validation':<12} {'Gap':<12} {'Status':<15}")
print("─"*70)

metrics = ['loss', 'dice', 'iou', 'precision', 'recall', 'accuracy']
overfitting_detected = False

for metric in metrics:
    train_val = train_results[metric]
    val_val = val_results[metric]

    if metric == 'loss':
        gap = val_val - train_val
        gap_pct = (gap / train_val) * 100
    else:
        gap = train_val - val_val
        gap_pct = (gap / train_val) * 100 if train_val > 0 else 0

    #thresholds
    if abs(gap_pct) < 2:
        status = " Excellent"
    elif abs(gap_pct) < 5:
        status = "Good"
    elif abs(gap_pct) < 10:
        status = " Slight overfit"
        if metric in ['dice', 'iou']:
            overfitting_detected = True
    else:
        status = "Overfitting!"
        overfitting_detected = True

    print(f"{metric.upper():<12} {train_val:.4f}       {val_val:.4f}       {abs(gap):.4f} ({abs(gap_pct):.1f}%)  {status}")

print("─"*70)


# simple comparison plot
fig, ax = plt.subplots(1, 1, figsize=(10, 6))

metrics_to_plot = ['dice', 'iou', 'precision', 'recall', 'accuracy']
x = np.arange(len(metrics_to_plot))
width = 0.35

train_vals = [train_results[m] for m in metrics_to_plot]
val_vals = [val_results[m] for m in metrics_to_plot]

bars1 = ax.bar(x - width/2, train_vals, width, label='Training', color='#2ecc71', alpha=0.8)
bars2 = ax.bar(x + width/2, val_vals, width, label='Validation', color='#3498db', alpha=0.8)

ax.set_xlabel('Metrics', fontweight='bold', fontsize=12)
ax.set_ylabel('Score', fontweight='bold', fontsize=12)
ax.set_title('Training vs Validation Performance', fontweight='bold', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels([m.upper() for m in metrics_to_plot], fontweight='bold')
ax.legend()
ax.grid(axis='y', alpha=0.3)
ax.set_ylim([0.7, 1.0])

#Add labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{height:.3f}',
               ha='center', va='bottom', fontsize=9)

plt.tight_layout()
os.makedirs('results', exist_ok=True)
plt.savefig('results/overfitting_comparison.png', dpi=150, bbox_inches='tight')
print("\nComparison plot saved: results/overfitting_comparison.png")
print("="*70)

"""# 9. Implementing Quantification Method"""

import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

def blue_mask_to_binary(image_path, threshold=128):
  #current model outputs blue not bw binary masks. convert blue to black
    img = Image.open(image_path)
    img_array = np.array(img)

    # If RGB/RGBA, convert to grayscale
    if len(img_array.shape) == 3:
        # Check if it's a blue mask (bright pixels are cracks)
        #use any channel
        grayscale = img_array[:, :, 0]  #red channel

    else:
        grayscale = img_array

    # Binarize: pixels above threshold = 1 (crack), below = 0 (background)
    binary_mask = (grayscale > threshold).astype(np.uint8)

    return binary_mask


def visualize_conversion(original_path, binary_mask):
    #Visualize original blue mask and converted binary mask
    original = Image.open(original_path)

    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    axes[0].imshow(original)
    axes[0].set_title('Original Blue Mask')
    axes[0].axis('off')

    axes[1].imshow(binary_mask, cmap='gray')
    axes[1].set_title('Binary Mask (Black & White)')
    axes[1].axis('off')

    plt.tight_layout()
    plt.show()


prediction_path = '/content/download (1).png'

binary_pred = blue_mask_to_binary(prediction_path)
clean_mask = binary_pred
visualize_conversion(prediction_path, binary_pred)

import numpy as np
import cv2
from scipy.ndimage import distance_transform_edt
from skimage.morphology import skeletonize as ski_skeletonize, binary_dilation, binary_erosion
from skimage.measure import label, regionprops
import matplotlib.pyplot as plt


def measure_crack_width_from_binary(binary_mask, skeleton, px_to_mm=0.077, visualize=False):
    #Measure crack width from binary mask using distance transform method. more robust than edge detection for binary masks.

    # width at each skeleton point = 2 * distance_transform_value (distance from skeleton to nearest background on both sides)

    # Ensure binary
    mask = (binary_mask > 0).astype(np.uint8)

    # Compute distance transform (distance from each foreground pixel to background)
    distance_map = distance_transform_edt(mask)

    # Get skeleton coordinates
    skel_coords = np.column_stack(np.where(skeleton))

    if len(skel_coords) == 0:
        return 0.0, 0.0, np.array([])

    #extract distance values at skeleton points
    # width = 2 * distance (since distance is to one edge, width includes both sides)
    widths_px = []
    for y, x in skel_coords:
        width = 2 * distance_map[y, x]
        widths_px.append(width)

    width_array_px = np.array(widths_px)

    #calculate stats
    max_width_px = np.max(width_array_px) if len(width_array_px) > 0 else 0
    mean_width_px = np.mean(width_array_px) if len(width_array_px) > 0 else 0

    #convert to mm
    max_width_mm = max_width_px * px_to_mm
    mean_width_mm = mean_width_px * px_to_mm
    width_array_mm = width_array_px * px_to_mm

    #visualize
    if visualize:
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        #Original mask
        axes[0, 0].imshow(mask, cmap='gray')
        axes[0, 0].set_title('Binary Mask')
        axes[0, 0].axis('off')

        #distance transform
        im1 = axes[0, 1].imshow(distance_map, cmap='hot')
        axes[0, 1].set_title('Distance Transform')
        axes[0, 1].axis('off')
        plt.colorbar(im1, ax=axes[0, 1])

        #skeleton
        axes[1, 0].imshow(skeleton, cmap='gray')
        axes[1, 0].set_title('Skeleton')
        axes[1, 0].axis('off')

        #width heatmap on skeleton
        width_map = np.zeros_like(mask, dtype=float)
        for (y, x), width in zip(skel_coords, widths_px):
            width_map[y, x] = width

        im2 = axes[1, 1].imshow(mask, cmap='gray', alpha=0.3)
        im3 = axes[1, 1].imshow(width_map, cmap='jet', alpha=0.7)
        axes[1, 1].set_title(f'Width Heatmap\nMax: {max_width_mm:.2f}mm, Mean: {mean_width_mm:.2f}mm')
        axes[1, 1].axis('off')
        plt.colorbar(im3, ax=axes[1, 1], label='Width (pixels)')

        plt.tight_layout()
        plt.show()

    return max_width_mm, mean_width_mm, width_array_mm


def measure_crack_width_edge_based(binary_mask, skeleton, px_to_mm=0.077):
    #Alternative method using edge detection (following Rakshitha et al.)

    mask = (binary_mask > 0).astype(np.uint8)

    #Detect edges using Canny
    mask_uint8 = (mask * 255).astype(np.uint8)
    edges = cv2.Canny(mask_uint8, 100, 200)

    #Get coordinates
    edge_coords = np.column_stack(np.where(edges > 0))
    skel_coords = np.column_stack(np.where(skeleton))

    if len(edge_coords) == 0 or len(skel_coords) == 0:
        return 0.0, 0.0, np.array([])

    widths = []

    for y, x in skel_coords:
        #calculate distances to all edge pixels
        distances = np.sqrt((edge_coords[:, 0] - y)**2 + (edge_coords[:, 1] - x)**2)

        #find points on opposite sides
        #sort distances to find nearest edges
        sorted_idx = np.argsort(distances)

        if len(sorted_idx) >= 2:
            #get two nearest edge points
            nearest_edges = edge_coords[sorted_idx[:2]]

            #find width as sum of distances
            d1 = distances[sorted_idx[0]]
            d2 = distances[sorted_idx[1]]

            width = d1 + d2
            widths.append(width)

    width_array_px = np.array(widths)

    if len(width_array_px) == 0:
        return 0.0, 0.0, width_array_px

    max_width_mm = np.max(width_array_px) * px_to_mm
    mean_width_mm = np.mean(width_array_px) * px_to_mm

    return max_width_mm, mean_width_mm, width_array_px * px_to_mm


def quantify_crack_complete(binary_mask, px_to_mm=0.077, method='distance_transform', visualize=True):
    #compares both methods

    mask = (binary_mask > 0).astype(np.uint8)

    #step 1:skeletonization
    skeleton = ski_skeletonize(mask.astype(bool))

    if skeleton.sum() == 0:
        print("Warning: Empty skeleton!")
        return {
            'length_mm': 0.0,
            'max_width_mm': 0.0,
            'mean_width_mm': 0.0,
            'skeleton': skeleton,
            'width_array': np.array([])
        }

    #step 2;find crack length
    length_mm = calculate_crack_length(skeleton, px_to_mm)

    #step 3: crack width
    if method == 'distance_transform': #distance transform method
        max_width_mm, mean_width_mm, width_array = measure_crack_width_from_binary(
            mask, skeleton, px_to_mm, visualize=visualize
        )
    else:  # edge based method
        max_width_mm, mean_width_mm, width_array = measure_crack_width_edge_based(
            mask, skeleton, px_to_mm
        )

    #visualization
    if visualize:
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        #output original mask
        axes[0].imshow(mask, cmap='gray')
        axes[0].set_title('Binary Crack Mask')
        axes[0].axis('off')

        #add skeleton overlay
        overlay = np.stack([mask * 255] * 3, axis=-1).astype(np.uint8)
        skel_coords = np.column_stack(np.where(skeleton))
        for y, x in skel_coords:
            cv2.circle(overlay, (int(x), int(y)), 1, (255, 0, 0), -1)

        axes[1].imshow(overlay)
        axes[1].set_title(f'Skeleton\nLength: {length_mm:.2f} mm')
        axes[1].axis('off')

        #Distance transform heatmap
        distance_map = distance_transform_edt(mask)
        im = axes[2].imshow(distance_map, cmap='hot')
        axes[2].contour(skeleton, colors='blue', linewidths=1)
        axes[2].set_title(f'Distance Transform\nMax Width: {max_width_mm:.2f} mm\nMean Width: {mean_width_mm:.2f} mm')
        axes[2].axis('off')
        plt.colorbar(im, ax=axes[2], label='Distance (pixels)')

        plt.tight_layout()
        plt.show()

    results = {
        'length_mm': length_mm,
        'max_width_mm': max_width_mm,
        'mean_width_mm': mean_width_mm,
        'skeleton': skeleton,
        'width_array': width_array,
        'method': method
    }

    return results


#results
print("Method 1: Distance Transform") #distance transform method
results_dt = quantify_crack_complete(
    clean_mask,
    px_to_mm=0.077,
    method='distance_transform',
    visualize=True
)

print("\n" + "="*60)
print("CRACK QUANTIFICATION (Distance Transform Method)")
print("="*60)
print(f"Total Crack Length: {results_dt['length_mm']:.2f} mm")
print(f"Maximum Width:      {results_dt['max_width_mm']:.2f} mm")
print(f"Mean Width:         {results_dt['mean_width_mm']:.2f} mm")
print("="*60)

#Edge-based method
print("\n\nMethod 2: Edge-Based (Paper's Algorithm)")
results_edge = quantify_crack_complete(
    clean_mask,
    px_to_mm=0.077,
    method='edge_based',
    visualize=False
)

print("\n" + "="*60)
print("CRACK QUANTIFICATION (Edge-Based Method)")
print("="*60)
print(f"Total Crack Length: {results_edge['length_mm']:.2f} mm")
print(f"Maximum Width:      {results_edge['max_width_mm']:.2f} mm")
print(f"Mean Width:         {results_edge['mean_width_mm']:.2f} mm")
print("="*60)

# ground truth vs prediction
if 'gt_mask' in locals() and 'pred_mask' in locals():
    results_gt = quantify_crack_complete(gt_mask, px_to_mm=0.077, visualize=False)
    results_pred = quantify_crack_complete(pred_mask, px_to_mm=0.077, visualize=False)

    # get relative errors
    length_error = abs(results_pred['length_mm'] - results_gt['length_mm']) / results_gt['length_mm'] * 100
    max_width_error = abs(results_pred['max_width_mm'] - results_gt['max_width_mm']) / results_gt['max_width_mm'] * 100
    mean_width_error = abs(results_pred['mean_width_mm'] - results_gt['mean_width_mm']) / results_gt['mean_width_mm'] * 100

    print("\n" + "="*60)
    print("COMPARISON: Ground Truth vs Prediction")
    print("="*60)
    print(f"{'Metric':<20} {'Ground Truth':<15} {'Prediction':<15} {'Error %':<10}")
    print("-"*60)
    print(f"{'Length (mm)':<20} {results_gt['length_mm']:<15.2f} {results_pred['length_mm']:<15.2f} {length_error:<10.2f}")
    print(f"{'Max Width (mm)':<20} {results_gt['max_width_mm']:<15.2f} {results_pred['max_width_mm']:<15.2f} {max_width_error:<10.2f}")
    print(f"{'Mean Width (mm)':<20} {results_gt['mean_width_mm']:<15.2f} {results_pred['mean_width_mm']:<15.2f} {mean_width_error:<10.2f}")
    print("="*60)