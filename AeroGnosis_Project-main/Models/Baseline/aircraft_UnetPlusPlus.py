# -*- coding: utf-8 -*-
"""Aircraft UnetPlusPlus.ipynb

Automatically generated by Colab.
"""

# only use to reset!!!
!kill -9 -1

"""# Identifying the images and masks (counting)

"""

from google.colab import drive
drive.mount('/content/drive')

#moving the dataset to google drive to prevent issues with colab:
# Clear the mountpoint before mounting: if the below fails, manually delete in the file explorer in the left tab.

# Proceed with the mounting
#!fusermount -u /content/drive
from google.colab import drive
drive.mount('/content/drive')

import os

raw_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/images"
mask_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/masks"

print("RAW IMAGES:")
raw_files = sorted([f for f in os.listdir(raw_dir) if f.lower().endswith('.png')])
for f in raw_files[:65]:
    print("   ", f)
print("Total raw images:", len(raw_files))

print("\nMASK IMAGES:")
mask_files = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith('.png')])
for f in mask_files[:65]:
    print("   ", f)
print("Total mask images:", len(mask_files))

import os

raw_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/images"
mask_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/masks"

raw_files = sorted([f for f in os.listdir(raw_dir) if f.lower().endswith('.png')])
mask_files = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith('.png')])

mask_basenames = [m.replace("_mask.png", "") for m in mask_files]

unmatched = [r for r in raw_files if os.path.splitext(r)[0] not in mask_basenames]
print("Images without masks:")
for f in unmatched:
    print("   ", f)
print("Total:", len(unmatched))

"""
# Loading the Dataset and Formatting the Directory"""

#defining the dataset path:
DATASET_PATH = "/content/drive/MyDrive/Aircraft_Crack_Dataset" #change

import os

#exploring the current directory:
dataset_path =  "/content/drive/MyDrive/Aircraft_Crack_Dataset"

for root, dirs, files in os.walk(dataset_path):
    print(f" Directory: {root}")
    print(f"  ->  Subdirectories: {dirs}")
    print(f"  ->  Files: {len(files)}")
    print(f"    * Sample: {files[:5]}")
    print("-" * 50)

raw_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/images"
mask_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/masks"

raw_files = sorted([f for f in os.listdir(raw_dir) if f.lower().endswith('.png')])
mask_files = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith('.png')])

print("Sample RAW:", raw_files[:10])
print("Sample MASKS:", mask_files[:10])

import os
from PIL import Image
from sklearn.model_selection import train_test_split

raw_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/images"
mask_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/masks"
base_new = "/content/drive/MyDrive/aircraft_test_formatted"  #make sure previous one is deleted before running
target_size = (512, 512)

# Create output folders
os.makedirs(f"{base_new}/JPEGImages", exist_ok=True)
os.makedirs(f"{base_new}/SegmentationClass", exist_ok=True)
os.makedirs(f"{base_new}/ImageSets/Segmentation", exist_ok=True)

pairs = []
missing = []

# Match exactly by name
for img_file in sorted(os.listdir(raw_dir)):
    if not img_file.lower().endswith(".png"):
        continue
    base = os.path.splitext(img_file)[0]
    mask_file = f"{base}_mask.png"
    mask_path = os.path.join(mask_dir, mask_file)
    if os.path.exists(mask_path):
        pairs.append((img_file, mask_file))
    else:
        missing.append(img_file)

print(f"{len(pairs)} pairs found")
if missing:
    print(f"{len(missing)} images without masks:")
    for m in missing:
        print("   ", m)

#Split into train/val/test
if len(pairs) > 0:
    train_pairs, temp = train_test_split(pairs, test_size=0.3, random_state=42)
    val_pairs, test_pairs = train_test_split(temp, test_size=0.5, random_state=42)
    splits = {"train": train_pairs, "val": val_pairs, "test": test_pairs}

    for split, pair_list in splits.items():
        txt_path = f"{base_new}/ImageSets/Segmentation/{split}.txt"
        with open(txt_path, "w") as f:
            for img_file, mask_file in pair_list:
                img_id = os.path.splitext(img_file)[0]

                img = Image.open(os.path.join(raw_dir, img_file)).convert("RGB").resize(target_size, Image.BILINEAR)
                mask = Image.open(os.path.join(mask_dir, mask_file)).convert("L").resize(target_size, Image.NEAREST)

                img.save(f"{base_new}/JPEGImages/{img_id}.png")
                mask.save(f"{base_new}/SegmentationClass/{img_id}.png")
                f.write(f"{img_id}\n")

    print("Dataset rebuilt with correct image-mask pairing!")
else:
    print("No valid pairs found â€” check your filenames!")

"""# Verifying Dataset"""

import os

base_path = "/content/drive/MyDrive/aircraft_test_formatted"  # <-- change to your dataset

splits = ['train', 'val', 'test']
for split in splits:
    txt_file = os.path.join(base_path, "ImageSets", "Segmentation", f"{split}.txt")

    # read IDs (without extensions)
    with open(txt_file) as f:
        lines = [line.strip() for line in f.readlines()]

    # count matching files
    n_images = sum([
        os.path.exists(os.path.join(base_path, "JPEGImages", f"{id}.png"))
        for id in lines
    ])
    n_masks = sum([
        os.path.exists(os.path.join(base_path, "SegmentationClass", f"{id}.png"))
        for id in lines
    ])

    print(f" {split.upper()} set: {len(lines)} IDs")
    print(f"    Found {n_images} images and {n_masks} masks\n")

from PIL import Image
import matplotlib.pyplot as plt

sample_id = "segment_3_image11"  # pick one from train.txt
img = Image.open(f"{base_new}/JPEGImages/{sample_id}.png")
mask = Image.open(f"{base_new}/SegmentationClass/{sample_id}.png")

plt.figure(figsize=(10,4))
plt.subplot(1,2,1); plt.imshow(img); plt.title("Image"); plt.axis("off")
plt.subplot(1,2,2); plt.imshow(mask, cmap="gray"); plt.title("Mask"); plt.axis("off")
plt.show()

base_path = "/content/drive/MyDrive/aircraft_test_formatted"
for split in ['train', 'val', 'test']:
    txt = f"{base_path}/ImageSets/Segmentation/{split}.txt"
    with open(txt) as f:
        ids = [line.strip() for line in f]
    print(f"{split}: {len(ids)} IDs")

import random
from PIL import Image
import matplotlib.pyplot as plt
import os

base = "/content/drive/MyDrive/aircraft_test_formatted"
ids = open(f"{base}/ImageSets/Segmentation/train.txt").read().splitlines()
samples = random.sample(ids, 3)

for s in samples:
    img = Image.open(f"{base}/JPEGImages/{s}.png")
    mask = Image.open(f"{base}/SegmentationClass/{s}.png")
    plt.figure(figsize=(10,4))
    plt.subplot(1,2,1); plt.imshow(img); plt.title(s + " (Image)"); plt.axis("off")
    plt.subplot(1,2,2); plt.imshow(mask, cmap="gray"); plt.title(s + " (Mask)"); plt.axis("off")
    plt.show()

"""# Preprocessing, Augmentation, and Dataset Class

"""

import os.path as osp

def create_dir(path):
    """
    Creates lists of image and mask paths for training, validation, and testing
    following the PASCAL VOC dataset format.
    """

    # Use .png because your saved images are PNGs
    image_path_general = osp.join(path, 'JPEGImages', '%s.png')
    mask_path_general  = osp.join(path, 'SegmentationClass', '%s.png')

    # Paths to the split text files
    train_id_names = osp.join(path, 'ImageSets', 'Segmentation', 'train.txt')
    val_id_names   = osp.join(path, 'ImageSets', 'Segmentation', 'val.txt')
    test_id_names  = osp.join(path, 'ImageSets', 'Segmentation', 'test.txt')

    # Lists
    train_img_list, train_mask_list = [], []
    val_img_list, val_mask_list     = [], []
    test_img_list, test_mask_list   = [], []

    # Fill train
    with open(train_id_names) as f:
        for line in f:
            file_id = line.strip()
            train_img_list.append(image_path_general % file_id)
            train_mask_list.append(mask_path_general % file_id)

    # Fill val
    with open(val_id_names) as f:
        for line in f:
            file_id = line.strip()
            val_img_list.append(image_path_general % file_id)
            val_mask_list.append(mask_path_general % file_id)

    # Fill test
    with open(test_id_names) as f:
        for line in f:
            file_id = line.strip()
            test_img_list.append(image_path_general % file_id)
            test_mask_list.append(mask_path_general % file_id)

    return (
        train_img_list, train_mask_list,
        val_img_list, val_mask_list,
        test_img_list, test_mask_list
    )

import albumentations as A
from albumentations.pytorch import ToTensorV2

class DataTransform:
    def __init__(self, size=(512, 512)):
        """
        Data augmentation pipeline for train, val, test splits.
        Uses Albumentations to apply the same spatial transforms to image & mask.
        """
        mean = (0.485, 0.456, 0.406)
        std = (0.229, 0.224, 0.225)

        self.data_transform = {
            'train': A.Compose([
                A.RandomScale(scale_limit=0.5, p=0.5),
                A.Rotate(limit=10, p=0.5),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),  # optional: can add more variation
                A.Resize(size[0], size[1]),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ]),
            'val': A.Compose([
                A.Resize(size[0], size[1]),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ]),
            'test': A.Compose([
                A.Resize(size[0], size[1]),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])
        }

    def __call__(self, phase, image, mask):
        """
        Args:
            phase: 'train' | 'val' | 'test'
            image: numpy array (H, W, 3)
            mask: numpy array (H, W)
        Returns:
            image_tensor: torch.FloatTensor
            mask_tensor: torch.LongTensor
        """
        augmented = self.data_transform[phase](image=image, mask=mask)
        image = augmented['image']
        mask = augmented['mask'].long()  # ensure mask is LongTensor for CE Loss
        return image, mask

import torch
import torch.utils.data as data
import numpy as np
from PIL import Image

class VOCDataset(data.Dataset):
    def __init__(self, img_list, anno_list, phase, transform, n_classes=2, input_shape=(512, 512)):
        """
        Args:
            img_list: list of image file paths
            anno_list: list of mask file paths
            phase: 'train', 'val', 'test'
            transform: DataTransform instance
            n_classes: number of segmentation classes (default 2)
            input_shape: desired (width, height)
        """
        self.img_list = img_list
        self.anno_list = anno_list
        self.phase = phase
        self.transform = transform
        self.n_classes = n_classes
        self.input_shape = input_shape

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, index):
        img, mask = self.pull_item(index)
        return img, mask

    def pull_item(self, index):
        image_file_path = self.img_list[index]
        mask_file_path = self.anno_list[index]

        # Load image and mask
        img = Image.open(image_file_path).convert("RGB")
        mask = Image.open(mask_file_path).convert("L")  # grayscale

        # Resize both (for safety)
        img = img.resize(self.input_shape, resample=Image.BILINEAR)
        mask = mask.resize(self.input_shape, resample=Image.NEAREST)

        # Convert to numpy
        img = np.array(img)
        mask = np.array(mask).astype('int64')
        mask[mask > 0] = 1  # convert all nonzero to 1

        # Safety check
        if img.shape[:2] != mask.shape:
            print(f"Shape mismatch at index {index}: img={img.shape}, mask={mask.shape}")

        # Apply Albumentations transforms
        img, mask = self.transform(self.phase, img, mask)

        return img, mask

formatted_data_path = "/content/drive/MyDrive/aircraft_test_formatted/"
train_img_list, train_mask_list, val_img_list, val_mask_list, test_img_list, test_mask_list = create_dir(formatted_data_path)

train_dataset = VOCDataset(
    img_list=train_img_list,
    anno_list=train_mask_list,
    phase='train',
    transform=DataTransform(),
    input_shape=(512, 512)
)

from torch.utils.data import DataLoader
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

batch = next(iter(train_loader))
images, masks = batch
print("Batch:", images.shape, masks.shape)

import matplotlib.pyplot as plt

def visualize_batch(images, masks, num=4):
    for i in range(num):
        img = images[i].permute(1, 2, 0).cpu().numpy()
        mask = masks[i].cpu().numpy()

        # Unnormalize the image (reverse ImageNet normalization)
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        img = (img * std) + mean
        img = (img * 255).astype('uint8')

        plt.figure(figsize=(8, 4))
        plt.subplot(1, 2, 1)
        plt.imshow(img)
        plt.title(f"Image {i}")
        plt.axis("off")

        plt.subplot(1, 2, 2)
        plt.imshow(img)
        plt.imshow(mask, cmap='jet', alpha=0.4)
        plt.title(f"Mask Overlay {i}")
        plt.axis("off")
        plt.show()

visualize_batch(images, masks)

for i in range(3):
    img, mask = train_dataset[i]
    print(f"ðŸ”¹ Sample {i}: Image shape = {img.shape}, Mask shape = {mask.shape}, Mask unique = {mask.unique().tolist()}")

import cv2
import numpy as np
import matplotlib.pyplot as plt
#mariam's one
def visualize_overlay(dataset, index=0, alpha=0.5):
    image, mask = dataset[index]  # image: tensor [3,H,W], mask: [H,W]
    image_np = image.permute(1, 2, 0).cpu().numpy()
    mask_np = mask.cpu().numpy()

    # Unnormalize (reverse ImageNet normalization)
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    image_np = (image_np * std) + mean
    image_np = np.clip(image_np, 0, 1)

    # Create red overlay where mask == 1
    mask_rgb = np.zeros_like(image_np)
    if mask_np.max() > 0:
        mask_rgb[..., 0] = mask_np / mask_np.max()  # normalize to 0â€“1
    else:
        mask_rgb[..., 0] = mask_np

    overlay = (1 - alpha) * image_np + alpha * mask_rgb

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1); plt.imshow(image_np); plt.title("Image"); plt.axis("off")
    plt.subplot(1, 3, 2); plt.imshow(mask_np, cmap="gray"); plt.title("Mask"); plt.axis("off")
    plt.subplot(1, 3, 3); plt.imshow(overlay); plt.title("Overlay"); plt.axis("off")
    plt.show()

# Try a few examples
visualize_overlay(train_dataset, index=0)
visualize_overlay(train_dataset, index=5)
visualize_overlay(train_dataset, index=10)

"""# Creating YAML File for Training Configuration"""

yaml_content = """
batch_size: 4
test_batch_size: 4
num_workers: 4
input_shape: [512, 512]
n_classes: 2
pretrained: True

optimizer: adam
max_lr: 0.0005
min_lr: 0.000001
momentum: 0.9
weight_decay: 0
lr_decay_type: 'cos'

cuda: True
distributed: False
sync_bn: False

save_dir: "/content/drive/MyDrive/aircraft_test_formatted/weights/"
data_path: "/content/drive/MyDrive/aircraft_test_formatted/"
model_path: ""
save_period: 20
result_dir: "/content/drive/MyDrive/aircraft_test_formatted/results/"
"""
with open("/content/drive/MyDrive/aircraft_test_formatted/train_config.yaml", "w") as f:
    f.write(yaml_content)

print("YAML config file saved at /content/drive/MyDrive/aircraft_test_formatted/train_config.yaml")

"""# Model Creation (DeepLab) --not used"""

!pip install lightning

!pip install torchmetrics

import matplotlib.pyplot as plt
import os

# Save overlays in your aircraft project folder
save_dir = "/content/drive/MyDrive/aircraft_test_formatted/visuals"
os.makedirs(save_dir, exist_ok=True)

def overlay_prediction(image_tensor, mask_tensor, pred_tensor, save_path):
    """
    Save visualization showing:
      - Original image
      - Ground truth mask
      - Predicted mask
    """
    image = image_tensor.permute(1, 2, 0).cpu().numpy()  # [C, H, W] â†’ [H, W, C]
    mask = mask_tensor.cpu().numpy()
    pred = pred_tensor.cpu().numpy()

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1)
    plt.imshow(image)
    plt.title('Original Image')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(mask, cmap='gray')
    plt.title('Ground Truth')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(pred, cmap='gray')
    plt.title('Prediction')
    plt.axis('off')

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path)
    plt.close()
    print(f"ðŸ“¸ Saved overlay â†’ {save_path}")

"""# Model Creation (Unet)"""

!pip uninstall pytorch-lightning -y
!pip install pytorch-lightning==2.0.7

!pip install segmentation-models-pytorch

import torch.nn as nn
import pytorch_lightning as pl
from torchmetrics.classification import (
    MulticlassAccuracy,
    MulticlassJaccardIndex,
    MulticlassF1Score,
    MulticlassPrecision,
    MulticlassRecall
)
from segmentation_models_pytorch import UnetPlusPlus

# Helper dice loss
def dice_loss(preds, targets, smooth=1e-6):
    probs = torch.softmax(preds, dim=1)[:, 1, :, :]  # class 1
    targets = (targets == 1).float()

    probs = probs.contiguous().view(-1)
    targets = targets.contiguous().view(-1)

    intersection = (probs * targets).sum()
    dice = (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)
    return 1 - dice

# Simple U-Net block
class ConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.block(x)

# Full U-Net
class UNet(nn.Module):
    def __init__(self, n_classes=2):
        super().__init__()

        self.down1 = ConvBlock(3, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.down2 = ConvBlock(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.down3 = ConvBlock(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        self.down4 = ConvBlock(256, 512)
        self.pool4 = nn.MaxPool2d(2)

        self.bottleneck = ConvBlock(512, 1024)

        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = ConvBlock(1024, 512)
        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = ConvBlock(512, 256)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = ConvBlock(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = ConvBlock(128, 64)

        self.final = nn.Conv2d(64, n_classes, kernel_size=1)

    def forward(self, x):
      return self.model(x)


    def crop_and_concat(self, upsampled, bypass):
        if upsampled.size()[2:] != bypass.size()[2:]:
            diffY = bypass.size()[2] - upsampled.size()[2]
            diffX = bypass.size()[3] - upsampled.size()[3]
            bypass = bypass[:, :, diffY//2:diffY//2 + upsampled.size()[2], diffX//2:diffX//2 + upsampled.size()[3]]
        return torch.cat([upsampled, bypass], dim=1)

# Lightning wrapper
class UNetCrackSegmentation(pl.LightningModule):
    def __init__(self, num_classes=2, lr=1e-4):
        super().__init__()
        self.save_hyperparameters()
        self.model = UnetPlusPlus(
        encoder_name="mobilenet_v2",               # you can try efficientnet-b0 etc.
        encoder_weights="imagenet",                 # or None if training from scratch
        in_channels=3,
        classes=num_classes                        # usually 2 for crack vs background
      )


        self.criterion = nn.CrossEntropyLoss()
        self.iou = MulticlassJaccardIndex(num_classes=num_classes)
        self.accuracy = MulticlassAccuracy(num_classes=num_classes)
        self.dice = MulticlassF1Score(num_classes=num_classes, average='macro')
        self.precision = MulticlassPrecision(num_classes=num_classes, average='macro')
        self.recall = MulticlassRecall(num_classes=num_classes, average='macro')

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        imgs, masks = batch
        preds = self(imgs)
        ce = self.criterion(preds, masks)
        d = dice_loss(preds, masks)
        loss = ce + 0.5 * d
        preds_class = torch.argmax(preds, dim=1)

        self.log("train_loss", loss, on_epoch=True, prog_bar=True)
        self.log("train_dice", self.dice(preds_class, masks), on_epoch=True)
        self.log("train_iou", self.iou(preds_class, masks), on_epoch=True)
        self.log("train_acc", self.accuracy(preds_class, masks), on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        imgs, masks = batch
        preds = self(imgs)
        ce = self.criterion(preds, masks)
        d = dice_loss(preds, masks)
        loss = ce + 0.5 * d
        preds_class = torch.argmax(preds, dim=1)

        self.log("val_loss", loss, on_epoch=True, prog_bar=True)
        self.log("val_dice", self.dice(preds_class, masks), on_epoch=True)
        self.log("val_iou", self.iou(preds_class, masks), on_epoch=True)
        self.log("val_acc", self.accuracy(preds_class, masks), on_epoch=True)
        self.log("val_precision", self.precision(preds_class, masks), on_epoch=True)
        self.log("val_recall", self.recall(preds_class, masks), on_epoch=True)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)

"""# Training Code

Consider making Unet more lightweight using different encoder

Consider adding Focal Loss good for very small cracks
"""

import yaml

def load_config(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

# Commented out IPython magic to ensure Python compatibility.
# %env CUDA_LAUNCH_BLOCKING=1

# U-Net++ Training Script for Aircraft Crack Dataset
import torch
from torch.utils.data import DataLoader
import pytorch_lightning as l
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.strategies import SingleDeviceStrategy

# import UNetCrackSegmentation, VOCDataset, DataTransform, create_dir, load_config

print("CUDA available:", torch.cuda.is_available())
print("Device name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None")

# Define the main training function
def main(args):

    # Load YAML config
    train_config = load_config(args.train_config_path)
    input_size = tuple(train_config["input_shape"])

    l.seed_everything(1744)

    # Initialize model
    model = UNetCrackSegmentation(
        num_classes=train_config['n_classes'],
        lr=train_config['max_lr']
    )

    # Load dataset paths
    train_img_list, train_anno_list, val_img_list, val_anno_list, _, _ = create_dir(train_config['data_path'])

    # Build datasets
    train_dataset = VOCDataset(
        img_list=train_img_list,
        anno_list=train_anno_list,
        phase='train',
        n_classes=train_config['n_classes'],
        input_shape=input_size,
        transform=DataTransform()
    )

    val_dataset = VOCDataset(
        img_list=val_img_list,
        anno_list=val_anno_list,
        phase='val',
        n_classes=train_config['n_classes'],
        input_shape=input_size,
        transform=DataTransform()
    )

    # Safe DataLoaders
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=train_config['batch_size'],
        num_workers=0,          # prevent CUDA assert
        shuffle=True,
        pin_memory=False,
        drop_last=True
    )

    val_dataloader = DataLoader(
        val_dataset,
        batch_size=train_config['batch_size'],
        num_workers=0,
        shuffle=False,
        pin_memory=False,
        drop_last=True
    )

    # Callbacks
    model_checkpoint = ModelCheckpoint(
        dirpath=train_config["save_dir"],
        filename="{epoch}-{val_loss:.2f}-{val_dice:.2f}",
        save_last=True,
        save_top_k=1,
        monitor="val_dice",
        mode="max"
    )

    early_stop_callback = EarlyStopping(
        monitor='val_dice',
        patience=10,
        mode='max',
        verbose=True
    )

    # Trainer
    trainer = l.Trainer(
        strategy=SingleDeviceStrategy(device="cuda:0"),
        precision="16-mixed",
        max_epochs=40,
        sync_batchnorm=True,
        callbacks=[model_checkpoint, early_stop_callback],
        logger=False,
        enable_progress_bar=True
    )

    # Train!
    trainer.fit(
        model=model,
        train_dataloaders=train_dataloader,
        val_dataloaders=val_dataloader
    )

# Simulate argparse
class Args:
    train_config_path = "/content/drive/MyDrive/aircraft_test_formatted/train_config.yaml"

# Run training
main(Args())

import glob

ckpts = glob.glob("/content/drive/MyDrive/aircraft_test_formatted/weights/*.ckpt")
print(" Found checkpoints:")
for c in ckpts:
    print(c)

best_model_path = "/content/drive/MyDrive/aircraft_test_formatted/weights/epoch=11-val_loss=0.17-val_dice=0.88.ckpt" #change
model = UNetCrackSegmentation.load_from_checkpoint(best_model_path)

"""# Evaluation"""

from torch.utils.data import DataLoader
from pytorch_lightning import Trainer
from pytorch_lightning.strategies import SingleDeviceStrategy

# 1. Load your best checkpoint (aircraft dataset)
best_model_path = "/content/drive/MyDrive/aircraft_test_formatted/weights/epoch=11-val_loss=0.17-val_dice=0.88.ckpt" #change
model = UNetCrackSegmentation.load_from_checkpoint(best_model_path)
model.eval()

# 2. Collect test image & mask paths
test_img_list, test_mask_list = [], []
test_ids_path = "/content/drive/MyDrive/aircraft_test_formatted/ImageSets/Segmentation/val.txt"  # or test.txt if you made one
base_img = "/content/drive/MyDrive/aircraft_test_formatted/JPEGImages/{}.png"
base_mask = "/content/drive/MyDrive/aircraft_test_formatted/SegmentationClass/{}.png"

with open(test_ids_path, 'r') as f:
    for line in f:
        file_id = line.strip()
        test_img_list.append(base_img.format(file_id))
        test_mask_list.append(base_mask.format(file_id))

print(f" Found {len(test_img_list)} test images")

# 3. Build dataset
test_dataset = VOCDataset(
    img_list=test_img_list,
    anno_list=test_mask_list,
    phase='test',
    transform=DataTransform(),
    n_classes=2,
    input_shape=(256, 256)
)

test_loader = DataLoader(
    test_dataset,
    batch_size=1,
    shuffle=False,
    num_workers=0,
    pin_memory=False
)

# 4. Setup trainer
trainer = Trainer(
    strategy=SingleDeviceStrategy(device="cuda:0"),
    precision="16-mixed",
    logger=False,
    enable_progress_bar=True,
)

# 5. Run evaluation
results = trainer.validate(model, dataloaders=test_loader)
print("Evaluation results:", results)

import torch
import numpy as np
import matplotlib.pyplot as plt

def visualize_fp_fn(model, dataset, index=0, threshold=0.5):
    model.eval().cuda()
    img, mask = dataset[index]  # (C,H,W), (H,W)
    img_t = img.unsqueeze(0).cuda()

    with torch.no_grad():
        pred = model(img_t)
        probs = torch.softmax(pred, dim=1)[0,1].cpu().numpy()
        pred_mask = (probs > threshold).astype(np.uint8)

    gt_mask = mask.cpu().numpy().astype(np.uint8)

    # Calculate confusion components
    TP = np.logical_and(pred_mask == 1, gt_mask == 1)
    FP = np.logical_and(pred_mask == 1, gt_mask == 0)
    FN = np.logical_and(pred_mask == 0, gt_mask == 1)
    TN = np.logical_and(pred_mask == 0, gt_mask == 0)

    # Color-coded overlay
    overlay = np.zeros((*gt_mask.shape, 3), dtype=np.uint8)
    overlay[TP] = [0, 255, 0]    # Green = True Positive
    overlay[FP] = [255, 0, 0]    # Red = False Positive
    overlay[FN] = [0, 0, 255]    # Blue = False Negative

    # Display
    plt.figure(figsize=(14,5))
    plt.subplot(1,4,1)
    plt.imshow(img.permute(1,2,0).cpu().numpy())
    plt.title("Original Image"); plt.axis("off")

    plt.subplot(1,4,2)
    plt.imshow(gt_mask, cmap='gray')
    plt.title("Ground Truth"); plt.axis("off")

    plt.subplot(1,4,3)
    plt.imshow(pred_mask, cmap='gray')
    plt.title("Prediction"); plt.axis("off")

    plt.subplot(1,4,4)
    plt.imshow(overlay)
    plt.title("FP/FN/TP Map")
    plt.axis("off")
    plt.tight_layout()
    plt.show()

    # Print summary
    tp, fp, fn = TP.sum(), FP.sum(), FN.sum()
    precision = tp / (tp + fp + 1e-6)
    recall = tp / (tp + fn + 1e-6)
    dice = (2 * tp) / (2 * tp + fp + fn + 1e-6)
    print(f"Precision: {precision:.3f} | Recall: {recall:.3f} | Dice: {dice:.3f}")

best_model_path = "/content/drive/MyDrive/aircraft_test_formatted/weights/epoch=11-val_loss=0.17-val_dice=0.88.ckpt"

for i in range(1, 304):   # 1 â†’ 303 inclusive
    visualize_fp_fn(model, test_dataset, index=i)

import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import torchvision.transforms as T

def visualize_fp_fn_from_paths(model, image_path, mask_path, threshold=0.5):
    """
    Same idea as visualize_fp_fn, but:
      - loads an image from `image_path`
      - loads the GT mask from `mask_path`
    """

    # 1) put model in eval + GPU
    model.eval().cuda()

    # 2) load image + mask
    img_pil = Image.open(image_path).convert("RGB")
    mask_pil = Image.open(mask_path).convert("L")   # grayscale mask

    # transforms similar to your dataset (adjust if needed)
    img_transform = T.Compose([
        T.Resize((mask_pil.height, mask_pil.width)),  # or fixed (H,W) if you prefer
        T.ToTensor(),
    ])

    # For mask: resize with NEAREST to avoid interpolation artefacts
    mask_transform = T.Compose([
        T.Resize((mask_pil.height, mask_pil.width), interpolation=Image.NEAREST),
        T.PILToTensor(),   # keeps it integer
    ])

    img = img_transform(img_pil)             # (C,H,W), float in [0,1]
    mask = mask_transform(mask_pil)[0]       # (H,W), single-channel

    # binarize mask (assuming 0/255)
    gt_mask = (mask.numpy() > 0).astype(np.uint8)

    img_t = img.unsqueeze(0).cuda()          # (1,C,H,W)

    # 3) forward pass
    with torch.no_grad():
        pred = model(img_t)                  # (1, num_classes, H, W)
        probs = torch.softmax(pred, dim=1)[0, 1].cpu().numpy()  # class 1 prob
        pred_mask = (probs > threshold).astype(np.uint8)

    # 4) confusion components
    TP = np.logical_and(pred_mask == 1, gt_mask == 1)
    FP = np.logical_and(pred_mask == 1, gt_mask == 0)
    FN = np.logical_and(pred_mask == 0, gt_mask == 1)
    TN = np.logical_and(pred_mask == 0, gt_mask == 0)

    # color overlay
    overlay = np.zeros((*gt_mask.shape, 3), dtype=np.uint8)
    overlay[TP] = [0, 255, 0]    # green
    overlay[FP] = [255, 0, 0]    # red
    overlay[FN] = [0, 0, 255]    # blue

    # 5) plots (same style as your original)
    plt.figure(figsize=(14, 5))

    plt.subplot(1,4,1)
    plt.imshow(img.permute(1,2,0).cpu().numpy())
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1,4,2)
    plt.imshow(gt_mask, cmap='gray')
    plt.title("Ground Truth")
    plt.axis("off")

    plt.subplot(1,4,3)
    plt.imshow(pred_mask, cmap='gray')
    plt.title("Prediction")
    plt.axis("off")

    plt.subplot(1,4,4)
    plt.imshow(overlay)
    plt.title("FP/FN/TP Map")
    plt.axis("off")

    plt.tight_layout()
    plt.show()

    # 6) metrics
    tp, fp, fn = TP.sum(), FP.sum(), FN.sum()
    precision = tp / (tp + fp + 1e-6)
    recall = tp / (tp + fn + 1e-6)
    dice = (2 * tp) / (2 * tp + fp + fn + 1e-6)
    print(f"Precision: {precision:.3f} | Recall: {recall:.3f} | Dice: {dice:.3f}")

visualize_fp_fn_from_paths(
    model,
    "/content/segment_4_image122.png",
    "/content/segment_4_image122_mask.png"
)
