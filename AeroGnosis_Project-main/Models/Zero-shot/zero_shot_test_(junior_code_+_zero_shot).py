# -*- coding: utf-8 -*-
"""Zero-Shot Test (Junior Code + Zero-Shot)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tNEsqEdRtCzkSyWHTL2PTlqlKgTaPaMv
"""

#moving the dataset to google drive to prevent issues with colab:
from google.colab import drive
drive.mount('/content/drive')

"""# Model Creation (Unet++) [JUNIOR CODE]"""

!pip uninstall pytorch-lightning -y
!pip install pytorch-lightning==2.0.7

"""Building a standard unet++ model with minor changes to adapt to binary crack segmentation. The changes are as follows:
  - Instead of single output with sigmoid activation, we output 2 logits per pixel and use CrossEntropyLoss. This is so we can use multi-class metrics like IoU, F1

- We added a hybrid loss function which combines dice loss with standard cross entropy.


- We also wrap the model in LightningModule to simplify training loops

"""

import torch.nn as nn
import pytorch_lightning as pl
from torchmetrics.classification import (
    MulticlassAccuracy,
    MulticlassJaccardIndex,
    MulticlassF1Score,
    MulticlassPrecision,
    MulticlassRecall
)


# defining dice loss
def dice_loss(preds, targets, smooth=1e-6):
    probs = torch.softmax(preds, dim=1)[:, 1, :, :]  # class 1
    targets = (targets == 1).float()

    probs = probs.contiguous().view(-1)
    targets = targets.contiguous().view(-1)

    intersection = (probs * targets).sum()
    dice = (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)
    return 1 - dice

# simple U-Net block with 2 convolutional layers
class ConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.block(x)

# full U-Net architecture
class UNet(nn.Module):
    def __init__(self, n_classes=2):
        super().__init__()

        #downsampling for encoder)
        self.down1 = ConvBlock(3, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.down2 = ConvBlock(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.down3 = ConvBlock(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        self.down4 = ConvBlock(256, 512)
        self.pool4 = nn.MaxPool2d(2)

        #bottleneck
        self.bottleneck = ConvBlock(512, 1024)

        #upsampling (decoder)
        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = ConvBlock(1024, 512)
        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = ConvBlock(512, 256)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = ConvBlock(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = ConvBlock(128, 64)

        #final classifier layer:
        self.final = nn.Conv2d(64, n_classes, kernel_size=1)

    def forward(self, x):
        #encoder pathway:
        d1 = self.down1(x)
        d2 = self.down2(self.pool1(d1))
        d3 = self.down3(self.pool2(d2))
        d4 = self.down4(self.pool3(d3))

        #bottleneck:
        bn = self.bottleneck(self.pool4(d4))

        #decoder with skip connections:
        u4 = self.up4(bn)
        d4 = self.crop_and_concat(u4, d4)
        d4 = self.dec4(d4)

        u3 = self.up3(d4)
        d3 = self.crop_and_concat(u3, d3)
        d3 = self.dec3(d3)

        u2 = self.up2(d3)
        d2 = self.crop_and_concat(u2, d2)
        d2 = self.dec2(d2)

        u1 = self.up1(d2)
        d1 = self.crop_and_concat(u1, d1)
        d1 = self.dec1(d1)

        return self.final(d1)

    #this function ensures skip connections match in spatial size, and crops encoder feature map if needed
    def crop_and_concat(self, upsampled, bypass):
        if upsampled.size()[2:] != bypass.size()[2:]:
            diffY = bypass.size()[2] - upsampled.size()[2]
            diffX = bypass.size()[3] - upsampled.size()[3]
            bypass = bypass[:, :, diffY//2:diffY//2 + upsampled.size()[2], diffX//2:diffX//2 + upsampled.size()[3]]
        return torch.cat([upsampled, bypass], dim=1)

# defining PyTorch Lightning Module to handle training
class UNetCrackSegmentation(pl.LightningModule):
    def __init__(self, num_classes=2, lr=1e-4):
        super().__init__()
        self.save_hyperparameters()
        self.model = UNet(n_classes=num_classes)

        self.train_losses= []
        self.val_losses = []
        self.val_dice_scores = []

        #loss functions and metrics:
        self.criterion = nn.CrossEntropyLoss()
        self.iou = MulticlassJaccardIndex(num_classes=num_classes)
        self.accuracy = MulticlassAccuracy(num_classes=num_classes)
        self.dice = MulticlassF1Score(num_classes=num_classes, average='macro')
        self.precision = MulticlassPrecision(num_classes=num_classes, average='macro')
        self.recall = MulticlassRecall(num_classes=num_classes, average='macro')

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        imgs, masks = batch
        preds = self(imgs)
        ce = self.criterion(preds, masks)
        d = dice_loss(preds, masks)
        loss = ce + 0.5 * d #defining the loss function to be cross entropy + 0.5*dice
        preds_class = torch.argmax(preds, dim=1)

        self.log("train_loss", loss, on_epoch= True, prog_bar = True)
        self.log("train_dice", self.dice(preds_class, masks), on_epoch=True)
        self.log("train_iou", self.iou(preds_class, masks), on_epoch=True)
        self.log("train_acc", self.accuracy(preds_class, masks), on_epoch=True)
        self.log("train_precision", self.precision(preds_class, masks), on_epoch=True)
        self.log("train_recall", self.recall(preds_class, masks), on_epoch=True)


        return loss


    def validation_step(self, batch, batch_idx):
        imgs, masks = batch
        preds = self(imgs)
        ce = self.criterion(preds, masks)
        d = dice_loss(preds, masks)
        loss = ce + 0.5 * d
        preds_class = torch.argmax(preds, dim=1)

        self.log("val_loss", loss, on_epoch=True, prog_bar=True)
        self.log("val_dice", self.dice(preds_class, masks), on_epoch=True)
        self.log("val_iou", self.iou(preds_class, masks), on_epoch=True)
        self.log("val_acc", self.accuracy(preds_class, masks), on_epoch=True)
        self.log("val_precision", self.precision(preds_class, masks), on_epoch=True)
        self.log("val_recall", self.recall(preds_class, masks), on_epoch=True)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)


    def on_train_epoch_end(self):
        train_loss = self.trainer.callback_metrics.get("train_loss")
        train_dice = self.trainer.callback_metrics.get("train_dice")
        if train_loss and train_dice:
            self.train_losses.append(train_loss.item())
            self.val_dice_scores.append(train_dice.item())

    def on_validation_epoch_end(self):
        val_loss = self.trainer.callback_metrics.get("val_loss")
        if val_loss:
            self.val_losses.append(val_loss.item())

"""# Zero-Shot on our Aircraft Dataset"""

#mount drive
from google.colab import drive
drive.mount('/content/drive')

!pip uninstall pytorch-lightning -y
!pip install pytorch-lightning==2.0.7

from pytorch_lightning import LightningModule
import os
import time
from PIL import Image
import numpy as np
import torch
from torchvision import transforms

# 1. Load model
ckpt_path = "/content/drive/MyDrive/crack500_formatted/weights/epoch=28-val_loss=0.15-val_dice=0.90.ckpt"
model = UNetCrackSegmentation.load_from_checkpoint(ckpt_path)
model.eval().cuda()

# 2. Paths (VOC-style)
root_dir  = "/content/drive/MyDrive/aircraft_voc"
image_dir = os.path.join(root_dir, "JPEGImages")
mask_dir  = os.path.join(root_dir, "SegmentationClass")

# 3. Transforms
img_size = (1024, 1024)

img_transform = transforms.Compose([
    transforms.Resize(img_size),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

def load_mask(mask_path, size=img_size):

    mask = Image.open(mask_path)
    mask = mask.resize(size, resample=Image.NEAREST)
    mask = np.array(mask)

    #if mask values are 0/255 or multi-class, binarize
    if mask.max() > 1:
        mask = (mask > 0).astype(np.uint8)

    return mask  # (H, W) with {0,1}


# 4. Metric helpers
def compute_confusion(pred, gt):
    """
    pred, gt: numpy arrays of shape (H, W), values {0,1}
    returns: tp, fp, fn, tn
    """
    assert pred.shape == gt.shape

    tp = np.logical_and(pred == 1, gt == 1).sum()
    fp = np.logical_and(pred == 1, gt == 0).sum()
    fn = np.logical_and(pred == 0, gt == 1).sum()
    tn = np.logical_and(pred == 0, gt == 0).sum()
    return tp, fp, fn, tn

# 5. Loop over all PNG images
image_paths = sorted([
    os.path.join(image_dir, f)
    for f in os.listdir(image_dir)
    if f.lower().endswith(".png")
])

eps = 1e-7
total_tp = total_fp = total_fn = total_tn = 0
total_time = 0.0
count = 0

for img_path in image_paths:
    fname = os.path.basename(img_path)
    mask_path = os.path.join(mask_dir, fname)   # same name for GT

    if not os.path.exists(mask_path):
        print(f" No mask found for {fname}, skipping.")
        continue

    #  Load & preprocess image
    img = Image.open(img_path).convert("RGB")
    input_tensor = img_transform(img).unsqueeze(0).cuda()

    #  Forward pass + timing
    start = time.time()
    with torch.no_grad():
        output = model(input_tensor)            # (1, C, H, W)
        probs = torch.softmax(output, dim=1)
        pred_mask = torch.argmax(probs, dim=1)  # (1, H, W)
        pred_mask = pred_mask.squeeze().cpu().numpy().astype(np.uint8)
    end = time.time()

    inference_time = end - start
    total_time += inference_time
    count += 1

    #  Load GT mask
    gt_mask = load_mask(mask_path, size=img_size).astype(np.uint8)

    #  Confusion for this image
    tp, fp, fn, tn = compute_confusion(pred_mask, gt_mask)
    total_tp += tp
    total_fp += fp
    total_fn += fn
    total_tn += tn

    # (Optional) per-image metrics
    dice_img = (2 * tp) / (2 * tp + fp + fn + eps)
    iou_img  = tp / (tp + fp + fn + eps)
    print(f"{fname}: Dice={dice_img:.4f}, IoU={iou_img:.4f}, time={inference_time:.4f}s")

# 6. Dataset-level metrics
dice = (2 * total_tp) / (2 * total_tp + total_fp + total_fn + eps)
iou  = total_tp / (total_tp + total_fp + total_fn + eps)
precision = total_tp / (total_tp + total_fp + eps)
recall    = total_tp / (total_tp + total_fn + eps)
accuracy  = (total_tp + total_tn) / (total_tp + total_fp + total_fn + total_tn + eps)

avg_time = total_time / max(count, 1)

print("\n========== METRICS ==========")
print(f"Dice       : {dice:.4f}")
print(f"IoU        : {iou:.4f}")
print(f"Precision  : {precision:.4f}")
print(f"Recall     : {recall:.4f}")
print(f"Accuracy   : {accuracy:.4f}")
print(f"Avg time   : {avg_time:.4f} sec/image")
print("=====================================")