# -*- coding: utf-8 -*-
"""Transfer learning approach.ipynb

Automatically generated by Colab.
"""

#mounting drive
from google.colab import drive
drive.mount('/content/drive')

"""# initial training on large source

### Unet++ model creation
"""

!pip install pytorch_lightning
!pip install segmentation_models_pytorch
!pip install pytorch_lightning
!pip install torchmetrics

import torch.nn as nn
import pytorch_lightning as pl
from torchmetrics.classification import (
    MulticlassAccuracy,
    MulticlassJaccardIndex,
    MulticlassF1Score,
    MulticlassPrecision,
    MulticlassRecall
)
from segmentation_models_pytorch import UnetPlusPlus

# helper dice loss
def dice_loss(preds, targets, smooth=1e-6):
    probs = torch.softmax(preds, dim=1)[:, 1, :, :]  # class 1
    targets = (targets == 1).float()

    probs = probs.contiguous().view(-1)
    targets = targets.contiguous().view(-1)

    intersection = (probs * targets).sum()
    dice = (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)
    return 1 - dice

# unet block simple
class ConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.block(x)

# full unet
class UNet(nn.Module):
    def __init__(self, n_classes=2):
        super().__init__()

        self.down1 = ConvBlock(3, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.down2 = ConvBlock(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.down3 = ConvBlock(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        self.down4 = ConvBlock(256, 512)
        self.pool4 = nn.MaxPool2d(2)

        self.bottleneck = ConvBlock(512, 1024)

        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = ConvBlock(1024, 512)
        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = ConvBlock(512, 256)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = ConvBlock(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = ConvBlock(128, 64)

        self.final = nn.Conv2d(64, n_classes, kernel_size=1)

    def forward(self, x):
      return self.model(x)


    def crop_and_concat(self, upsampled, bypass):
        if upsampled.size()[2:] != bypass.size()[2:]:
            diffY = bypass.size()[2] - upsampled.size()[2]
            diffX = bypass.size()[3] - upsampled.size()[3]
            bypass = bypass[:, :, diffY//2:diffY//2 + upsampled.size()[2], diffX//2:diffX//2 + upsampled.size()[3]]
        return torch.cat([upsampled, bypass], dim=1)

# lightning wrapper
class UNetCrackSegmentation(pl.LightningModule):
    def __init__(self, num_classes=2, lr=1e-4, weight_decay=0):
        super().__init__()
        self.save_hyperparameters()
        self.model = UnetPlusPlus( # unet++ model
        encoder_name="mobilenet_v2", # encoder
        encoder_weights="imagenet", # pretrained encoder on imagenet
        in_channels=3,
        classes=num_classes  # usually 2 for crack vs background
      )


        self.criterion = nn.CrossEntropyLoss()
        self.iou = MulticlassJaccardIndex(num_classes=num_classes)
        self.accuracy = MulticlassAccuracy(num_classes=num_classes)
        self.dice = MulticlassF1Score(num_classes=num_classes, average='macro')
        self.precision = MulticlassPrecision(num_classes=num_classes, average='macro')
        self.recall = MulticlassRecall(num_classes=num_classes, average='macro')

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        imgs, masks = batch
        preds = self(imgs)
        ce = self.criterion(preds, masks)
        d = dice_loss(preds, masks)
        loss = ce + 0.5 * d
        preds_class = torch.argmax(preds, dim=1)

        self.log("train_loss", loss, on_epoch=True, prog_bar=True)
        self.log("train_dice", self.dice(preds_class, masks), on_epoch=True)
        self.log("train_iou", self.iou(preds_class, masks), on_epoch=True)
        self.log("train_acc", self.accuracy(preds_class, masks), on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        imgs, masks = batch
        preds = self(imgs)
        ce = self.criterion(preds, masks)
        d = dice_loss(preds, masks)
        loss = ce + 0.5 * d
        preds_class = torch.argmax(preds, dim=1)

        self.log("val_loss", loss, on_epoch=True, prog_bar=True)
        self.log("val_dice", self.dice(preds_class, masks), on_epoch=True)
        self.log("val_iou", self.iou(preds_class, masks), on_epoch=True)
        self.log("val_acc", self.accuracy(preds_class, masks), on_epoch=True)
        self.log("val_precision", self.precision(preds_class, masks), on_epoch=True)
        self.log("val_recall", self.recall(preds_class, masks), on_epoch=True)

    def configure_optimizers(self):
        return torch.optim.Adam(
            self.parameters(),
            lr=self.hparams.lr,
            weight_decay=self.hparams.weight_decay)

"""### Preprocessing, Augmentation, and Dataset Class

"""

import os.path as osp
from PIL import Image
import torch.utils.data as data
import numpy as np
import os
import torch
import torchvision.utils as vutils


def create_dir(path):
    """
    creating lists of image and mask paths for training, validation, and testing
    following PASCAL VOC dataset format which is widely used in for semantic segmentation tasks
    """

    #specifying the path template for the image paths and the masks path:
    image_path_general = osp.join(path, 'RGB image', '%s.jpg')
    mask_path_general = osp.join(path, 'ground truth', '%s.png')

    #specifying the paths to the text files containing lists of image IDs for each split:
    train_id_names = osp.join(path, 'ImageSets', 'Segmentation', 'train.txt')
    val_id_names = osp.join(path, 'ImageSets', 'Segmentation', 'val.txt')
    test_id_names = osp.join(path, 'ImageSets', 'Segmentation', 'test.txt')


    #lists to hold the file paths for training images and masks:
    train_img_list = list()
    train_mask_list = list()

    #filling the lists above:
    for line in open(train_id_names):
        file_id = line.strip()
        img_path = (image_path_general % file_id)
        anno_path = (mask_path_general % file_id)
        train_img_list.append(img_path)
        train_mask_list.append(anno_path)

    #repeat the process for validation and testing file paths
    val_img_list = list()
    val_mask_list = list()

    for line in open(val_id_names):
        file_id = line.strip()
        img_path = (image_path_general % file_id)
        anno_path = (mask_path_general % file_id)
        val_img_list.append(img_path)
        val_mask_list.append(anno_path)

    test_img_list = list()
    test_mask_list = list()

    for line in open(test_id_names):
        file_id = line.strip()
        img_path = (image_path_general % file_id)
        anno_path = (mask_path_general % file_id)
        test_img_list.append(img_path)
        test_mask_list.append(anno_path)

    return train_img_list, train_mask_list, val_img_list, val_mask_list, test_img_list, test_mask_list

#defining augmentations
import albumentations as A
from albumentations.pytorch import ToTensorV2

class DataTransform:
    def __init__(self):
      #defining mean and std for normalization based on ImageNet
        mean = (0.485, 0.456, 0.406)
        std = (0.229, 0.224, 0.225)

        self.data_transform = {
            #applying random zoom in/zoom out, random rotation, random mirror, resize
            #to required model input, and normalizing the pixel values on the training data
            'train': A.Compose([
                A.RandomScale(scale_limit=0.5, p=0.5),
                A.Rotate(limit=10, p=0.5),
                A.HorizontalFlip(p=0.5),
                A.Resize(256, 256),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ]),
            #for the validation and testing data we only resize and normalize.
            'val': A.Compose([
                A.Resize(256, 256),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ]),
            'test': A.Compose([
                A.Resize(256, 256),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])
        }

    def __call__(self, phase, image, mask):
        augmented = self.data_transform[phase](image=image, mask=mask)
        return augmented['image'], augmented['mask']

class VOCDataset(data.Dataset):

    def __init__(self, img_list, anno_list, phase, transform, n_classes, input_shape):
        self.img_list = img_list
        self.anno_list = anno_list
        self.phase = phase
        self.transform = transform
        self.n_classes = n_classes
        self.input_shape = input_shape

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, index):
        img, pngs = self.pull_item(index)
        return img, pngs

    def pull_item(self, index):
      image_file_path = self.img_list[index]
      mask_file_path = self.anno_list[index]

      # opening image & mask using PIL
      img = Image.open(image_file_path).convert("RGB")
      mask = Image.open(mask_file_path)

      # resizing both to match input shape
      img = img.resize(self.input_shape, resample=Image.BILINEAR)
      mask = mask.resize(self.input_shape, resample=Image.NEAREST)

      # converting numpy for albumentations
      img = np.array(img)
      mask = np.array(mask).astype('int64')

      # mask values to 0 or 1
      mask[mask > 0] = 1

      # just in case theres a shape mismatch
      if img.shape[:2] != mask.shape:
          print(f"[!] Shape mismatch @ {index}: image={img.shape}, mask={mask.shape}")

      # applying albumentations
      img, mask = self.transform(self.phase, img, mask)
      mask = mask.long()

      return img, mask

import matplotlib.pyplot as plt
import torch

def visualize_batch(dataset, index=0):
    image, mask = dataset[index]

    # convert tensors to numpy
    image_np = image.permute(1, 2, 0).cpu().numpy()  # [Channels, Height, Width] â†’ [H, W, C]
    mask_np = mask.cpu().numpy()

    # un-normalizing for display for imagenet
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    image_np = (image_np * std) + mean
    image_np = (image_np * 255).astype('uint8')

    plt.figure(figsize=(10, 4))

    plt.subplot(1, 2, 1)
    plt.imshow(image_np)
    plt.title("Augmented Image")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(mask_np, cmap='gray')
    plt.title("Mask")
    plt.axis("off")

    plt.show()

"""### Large Source Dataset

#### Liu et al's dataset (7k images) : https://zenodo.org/records/17335521
"""

!wget "https://zenodo.org/records/17335521/files/Dataset.zip?download=1"

#unzipping
!unzip -q Dataset.zip?download=1 -d Dataset_extracted

# saving dataset on MyDrive
!cp -r /content/Dataset_extracted/ /content/drive/MyDrive/

DATASET_PATH = '/content/drive/MyDrive/Dataset_extracted'

import os

#exploring the current directory:
dataset_path =  "/content/drive/MyDrive/Dataset_extracted"

for root, dirs, files in os.walk(dataset_path):
    print(f" Directory: {root}")
    print(f"  ->  Subdirectories: {dirs}")
    print(f"  ->  Files: {len(files)}")
    print(f"    * Sample: {files[:5]}")
    print("-" * 50)

# checking mask format (channels are 0 & 1 only?)
# test mask loading
from PIL import Image
import numpy as np

test_mask_path = "/content/drive/MyDrive/Dataset_extracted/ground truth/AEL-(1).png"
mask = Image.open(test_mask_path).convert("L")  # Convert to grayscale
mask_np = np.array(mask)

print(f"Mask shape: {mask_np.shape}")  # Should be (H, W)
print(f"Mask unique values: {np.unique(mask_np)}")  # Check what values it has
print(f"Mask dtype: {mask_np.dtype}")

"""**(5330 images) training set, validation set (701 images), and test set (1058 images) + 448 generated images**"""

import os
import random

# defining the dataset path
DATASET_PATH = '/content/drive/MyDrive/Dataset_extracted'
image_dir = os.path.join(DATASET_PATH, 'RGB image')
mask_dir = os.path.join(DATASET_PATH, 'ground truth')

# get list of all image and mask files
image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]
mask_files = [f for f in os.listdir(mask_dir) if f.endswith('.png')]

# ensuring images and masks match (assuming same filename without extension)
image_ids = [os.path.splitext(f)[0] for f in image_files]
mask_ids = [os.path.splitext(f)[0] for f in mask_files]

# finding common ids
common_ids = list(set(image_ids) & set(mask_ids))
common_ids.sort() # Sort for reproducibility

print(f"Found {len(common_ids)} matching image and mask pairs.")


random.seed(42) # for reproducibility
random.shuffle(common_ids)

# defining split (FROM PAPER)
train_count = 5330
val_count = 701
test_count = 1058

# checking if the requested counts match the total number of images
if train_count + val_count + test_count != len(common_ids):
    print(f"requested split counts ({train_count}, {val_count}, {test_count}) do not sum up to the total number of images ({len(common_ids)}). Adjusting test count.")
    test_count = len(common_ids) - train_count - val_count
    print(f"adjusted test count: {test_count}")


# splitting the ids
train_ids = common_ids[:train_count]
val_ids = common_ids[train_count:train_count + val_count]
test_ids = common_ids[train_count + val_count:train_count + val_count + test_count]

print(f"Train set size: {len(train_ids)}")
print(f"Validation set size: {len(val_ids)}")
print(f"Test set size: {len(test_ids)}")


# creating the ImageSets/Segmentation directory if it doesn't exist
imagesets_seg_dir = os.path.join(DATASET_PATH, 'ImageSets', 'Segmentation')
os.makedirs(imagesets_seg_dir, exist_ok=True)

# write the split IDs to the respective text files
with open(os.path.join(imagesets_seg_dir, 'train.txt'), 'w') as f:
    for id in train_ids:
        f.write(id + '\n')

with open(os.path.join(imagesets_seg_dir, 'val.txt'), 'w') as f:
    for id in val_ids:
        f.write(id + '\n')

with open(os.path.join(imagesets_seg_dir, 'test.txt'), 'w') as f:
    for id in test_ids:
        f.write(id + '\n')

print("split files (train.txt, val.txt, test.txt) created in", imagesets_seg_dir)

from torch.utils.data import DataLoader
train_img_list, train_mask_list, val_img_list, val_mask_list, test_img_list, test_mask_list = create_dir(DATASET_PATH)


# setting up the dataset
dataset = VOCDataset(
    img_list=train_img_list,
    anno_list=train_mask_list,
    phase='train',
    transform=DataTransform(),
    n_classes=2,
    input_shape=(256, 256)
)

# wrap in DataLoader
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

visualize_batch(dataset, index=0)
visualize_batch(dataset, index=1)
visualize_batch(dataset, index=2)
visualize_batch(dataset, index=30)
visualize_batch(dataset, index=14)
visualize_batch(dataset, index=22)

import os

base_path = "/content/drive/MyDrive/Dataset_extracted/"

splits = ['train', 'val', 'test']
for split in splits:
    txt_file = os.path.join(base_path, "ImageSets", "Segmentation", f"{split}.txt")

    # reading ids
    with open(txt_file) as f:
        lines = [line.strip() for line in f.readlines()]

    # counting matching files for masks & images
    n_images = sum([
        os.path.exists(os.path.join(base_path, "RGB image", f"{id}.jpg"))
        for id in lines
    ])
    n_masks = sum([
        os.path.exists(os.path.join(base_path, "ground truth", f"{id}.png"))
        for id in lines
    ])

    print(f" {split.upper()} set: {len(lines)} IDs")
    print(f" found {n_images} images and {n_masks} masks\n")

"""### Creating YAML File for Training Configuration"""

yaml_content = """
batch_size: 16
test_batch_size: 16
num_workers: 8
input_shape: [256, 256]
n_classes: 2 #classes are only crack, background
pretrained: True

optimizer: adam
max_lr: 0.0005
min_lr: 0.000001
momentum: 0.9
weight_decay: 0
lr_decay_type: 'cos'

cuda: True
distributed: False
sync_bn: False

save_dir: "/content/drive/MyDrive/Dataset_extracted/weights/"
data_path: "/content/drive/MyDrive/Dataset_extracted/"
model_path: ""
save_period: 20
result_dir: "/content/drive/MyDrive/Dataset_extracted/results/"
"""

with open("/content/drive/MyDrive/Dataset_extracted/train_config.yaml", "w") as f:
    f.write(yaml_content)

print("YAML config file saved!")

"""### Training (manualy parameter settings)"""

import torch
import torch.nn as nn
import torch.utils.data as data
from torch.utils.data import DataLoader
from PIL import Image
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
from pytorch_lightning import Trainer
from pytorch_lightning.strategies import SingleDeviceStrategy

# Training
import yaml

def load_config(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

import argparse
import torch
from torch.utils.data import DataLoader

import pytorch_lightning as l
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.strategies import SingleDeviceStrategy
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.strategies import SingleDeviceStrategy
from pytorch_lightning.loggers import CSVLogger



import torch

print("CUDA available:", torch.cuda.is_available())
print("Device name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None")


def main(args):

    train_config = load_config(args.train_config_path)
    input_size = train_config["input_shape"][0]


    l.seed_everything(1744)

    model = UNetCrackSegmentation(num_classes=train_config['n_classes'], lr=train_config['max_lr'])

    data_path = train_config['data_path']
    train_img_list, train_anno_list, val_img_list, val_anno_list, _, _ = create_dir(train_config['data_path'])

    # imagenet normalization

    color_mean = (0.485, 0.456, 0.406)
    color_std = (0.229, 0.224, 0.225)

    # setting up datasets train & val

    train_dataset = VOCDataset(train_img_list, train_anno_list, phase='train',
                               n_classes=train_config['n_classes'], input_shape=train_config['input_shape'],
                               transform=DataTransform())
    val_dataset = VOCDataset(val_img_list, val_anno_list, phase='val',
                             n_classes=train_config['n_classes'], input_shape=train_config['input_shape'],
                            transform=DataTransform())

    # setting up data loaders train & val

    train_dataloader = DataLoader(train_dataset,
                                  batch_size=train_config['batch_size'],
                                  num_workers=train_config['num_workers'],
                                   shuffle=True,
                                  pin_memory=True,
                                  drop_last=True)

    val_dataloader = DataLoader(val_dataset,
                                  batch_size=train_config['batch_size'],
                                  num_workers=train_config['num_workers'],
                                   shuffle=False,
                                  pin_memory=True,
                                  drop_last=True)

    # saving model checkpoint & early stopping for static model performance after 10 epochs

    model_checkpoint = ModelCheckpoint(
    dirpath=train_config["save_dir"],
    filename="{epoch}-{val_loss:.2f}-{val_dice:.2f}",
    save_last=True,
    save_top_k=1,
    monitor="val_dice",
    mode="max"
    )

    early_stop_callback = EarlyStopping(
    monitor='val_dice',
    patience=10,
    mode='max',
    verbose=True
    )

    # csv logger to save metrics
    csv_logger = CSVLogger(save_dir="logs/", name="my_model_logs")


    # trainer
    trainer = l.Trainer(
      strategy=SingleDeviceStrategy(device="cuda:0"),
      precision = "16-mixed",
      max_epochs=40,
      sync_batchnorm=True,
      callbacks=[model_checkpoint, early_stop_callback],
      logger=csv_logger,
      enable_progress_bar = True,
    )

    trainer.fit(model = model,
                train_dataloaders = train_dataloader,
                val_dataloaders = val_dataloader,
            )

class Args:
    train_config_path = "/content/drive/MyDrive/Dataset_extracted/train_config.yaml"

# calling main
main(Args())

import os
import pandas as pd

log_dir = "/content/logs/my_model_logs"

# finding all version directories (version_0, version_1, version_2...)
version_dirs = [d for d in os.listdir(log_dir) if os.path.isdir(os.path.join(log_dir, d)) and d.startswith('version_')]

# sorting directories by version number (ascending order)
version_dirs.sort(key=lambda x: int(x.split('_')[1]))

# getting the most recent version directory (last in sorted list)
latest_version_dir = os.path.join(log_dir, version_dirs[-1]) if version_dirs else None

if latest_version_dir:
    metrics_path = os.path.join(latest_version_dir, "metrics.csv")
    if os.path.exists(metrics_path):
        metrics_df = pd.read_csv(metrics_path)
        display(metrics_df)
    else:
        print(f"metrics.csv not found")
else:
    print(f"No version directories found")

import matplotlib.pyplot as plt
import pandas as pd

# process metrics_df to get the epoch-end values
# grouping by epoch and getting the last non null value for each metric
metrics_epoch_processed_df = metrics_df.groupby('epoch').tail(1).reset_index()


# plotting the training loss
plt.figure(figsize=(10, 6))
plt.plot(metrics_epoch_processed_df['epoch'], metrics_epoch_processed_df['train_loss_epoch'], label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss over Epochs')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

metrics_epoch_processed_df = metrics_df.groupby('epoch').last().reset_index()


# plotting the validation loss
plt.figure(figsize=(10, 6))
plt.plot(metrics_epoch_processed_df['epoch'], metrics_epoch_processed_df['val_loss'], label='Validation Loss', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Validation Loss over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# finding the epoch with the best validation Dice score
best_epoch_row = metrics_epoch_processed_df.loc[metrics_epoch_processed_df['val_dice'].idxmax()]

# metrics from best epoch
best_train_metrics = {
    'Metric': ['Loss', 'Dice', 'IoU', 'Accuracy'],
    'Value': [
        best_epoch_row['train_loss_epoch'],
        best_epoch_row['train_dice_epoch'],
        best_epoch_row['train_iou_epoch'],
        best_epoch_row['train_acc_epoch']
    ]
}

best_train_metrics_df = pd.DataFrame(best_train_metrics)

print(f"Training Metrics for Best Model (Epoch {int(best_epoch_row['epoch'])}):")
display(best_train_metrics_df)

"""### Training with automated hyperparameter tuning using Optuna

#### Tuning with Optuna
"""

!pip install optuna optuna-integration

import torch
import torch.nn as nn
import torch.utils.data as data
from torch.utils.data import DataLoader
from PIL import Image
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
from pytorch_lightning import Trainer
from pytorch_lightning.strategies import SingleDeviceStrategy

import argparse
import torch
from torch.utils.data import DataLoader

import pytorch_lightning as l
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.strategies import SingleDeviceStrategy
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.strategies import SingleDeviceStrategy
from pytorch_lightning.loggers import CSVLogger

def trial_overfitting_check(val_dice, train_dice, val_loss, train_loss, trial_num):
  gap_dice = train_dice - val_dice
  print("\n" + "="*60)
  print(f"Trial {trial_num} overfitting analysis!")
  print("="*60)
  print(f"\n Dice Scores:")
  print(f"  Train: {train_dice:.4f}")
  print(f"  Val:   {val_dice:.4f}")
  print(f"  Gap:   {gap_dice:.4f}")

  print(f"\n Loss:")
  print(f"  Train: {train_loss:.4f}")
  print(f"  Val:   {val_loss:.4f}")

  print(f"\nAssessment:")
  if gap_dice > 0.08:
      print("SEVERE OVERFITTING (gap > 0.08)")
  elif gap_dice > 0.05:
      print("MODERATE OVERFITTING (gap > 0.05)")
  elif gap_dice > 0.03:
      print("   SLIGHT OVERFITTING (gap > 0.03) - Acceptable")
  else:
      print("HEALTHY - No significant overfitting!")

import optuna

# https://optuna.org/

def objective(trial):
    # suggestes params
    lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    weight_decay = trial.suggest_float("weight_decay", 1e-6, 1e-3, log=True)
    batch_size = trial.suggest_categorical("batch_size", [8, 16, 32])

    # loading existing config (read only)
    train_config = load_config("/content/drive/MyDrive/Dataset_extracted/train_config.yaml")
    l.seed_everything(1744)

    # model with trial params
    model = UNetCrackSegmentation(
        num_classes=2,
        lr=lr,
        weight_decay=weight_decay
    )

    # setting up data
    train_img_list, train_anno_list, val_img_list, val_anno_list, _, _ = create_dir(train_config['data_path'])

    train_dataset = VOCDataset(train_img_list, train_anno_list, phase='train',
                               n_classes=2, input_shape=(256, 256), transform=DataTransform())
    val_dataset = VOCDataset(val_img_list, val_anno_list, phase='val',
                             n_classes=2, input_shape=(256, 256), transform=DataTransform())

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                             num_workers=8, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                           num_workers=8, pin_memory=True, drop_last=True)

    # training (reduced epochs for speed)
    trainer = l.Trainer(
        strategy=SingleDeviceStrategy(device="cuda:0"),
        precision="16-mixed",
        max_epochs=20, # here shorter time
        callbacks=[EarlyStopping(monitor='val_dice', patience=5, mode='max', verbose=False)],
        enable_progress_bar=True,
        enable_model_summary=False
    )

    trainer.fit(model, train_loader, val_loader)

    metrics = trainer.callback_metrics

    # saving metrics of each trial
    trial.set_user_attr("train_dice", metrics.get("train_dice", 0).item())
    trial.set_user_attr("train_loss", metrics.get("train_loss", 0).item())

    trial.set_user_attr("val_dice", metrics.get("val_dice", 0).item())
    trial.set_user_attr("val_iou", metrics.get("val_iou", 0).item())
    trial.set_user_attr("val_acc", metrics.get("val_acc", 0).item())
    trial.set_user_attr("val_precision", metrics.get("val_precision", 0).item())
    trial.set_user_attr("val_recall", metrics.get("val_recall", 0).item())
    trial.set_user_attr("val_loss", metrics.get("val_loss", 0).item())

    # overfitting check (per trial)
    train_dice = metrics.get("train_dice", 0).item()
    val_dice = metrics.get("val_dice", 0).item()
    train_loss = metrics.get("train_loss", 0).item()
    val_loss = metrics.get("val_loss",0).item()
    trial_overfitting_check(val_dice=val_dice, train_dice=train_dice,val_loss=val_loss,train_loss=train_loss,trial_num=trial.number)

    return trainer.callback_metrics["val_dice"].item()


# running optuna here!
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)

print(f"\nOptimization Complete!")

print(f"\n best Trial Results:")
print(f"Val Dice: {study.best_trial.user_attrs['val_dice']:.4f}")
print(f"Val IoU: {study.best_trial.user_attrs['val_iou']:.4f}")
print(f"Val Accuracy: {study.best_trial.user_attrs['val_acc']:.4f}")
print(f"Val Precision: {study.best_trial.user_attrs['val_precision']:.4f}")
print(f"Val Recall: {study.best_trial.user_attrs['val_recall']:.4f}")
print(f"Val Loss: {study.best_trial.user_attrs['val_loss']:.4f}")

print(f"\n Hyperparameters:")
print(f"  Learning Rate:  {study.best_params['lr']:.6f}")
print(f"  Weight Decay:   {study.best_params['weight_decay']:.6f}")
print(f"  Batch Size:     {study.best_params['batch_size']}")

# saving all trial results
import pandas as pd

results = []
for trial in study.trials:
    if trial.state == optuna.trial.TrialState.COMPLETE:
        results.append({
            'trial': trial.number,
            'lr': trial.params['lr'],
            'weight_decay': trial.params['weight_decay'],
            'batch_size': trial.params['batch_size'],
            'val_dice': trial.user_attrs.get('val_dice', None),
            'val_iou': trial.user_attrs.get('val_iou', None),
            'val_acc': trial.user_attrs.get('val_acc', None),
            'val_precision': trial.user_attrs.get('val_precision', None),
            'val_recall': trial.user_attrs.get('val_recall', None),
            'val_loss': trial.user_attrs.get('val_loss', None),
        })

df = pd.DataFrame(results)
df.to_csv('/content/drive/MyDrive/Dataset_extracted/optuna_results.csv', index=False)

print("\nresults saved to: optuna_results.csv")
print("\ntop 5 Trials:")
print(df.sort_values('val_dice', ascending=False).head(5))

"""#### Training with updated config"""

import torch
import torch.nn as nn
import torch.utils.data as data
from torch.utils.data import DataLoader
from PIL import Image
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
from pytorch_lightning import Trainer
from pytorch_lightning.strategies import SingleDeviceStrategy

import argparse
import torch
from torch.utils.data import DataLoader

import pytorch_lightning as l
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.strategies import SingleDeviceStrategy
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.strategies import SingleDeviceStrategy
from pytorch_lightning.loggers import CSVLogger

# getting the best hyperparameters from Optuna csv file on trial == 4
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/Dataset_extracted/optuna_results.csv')
df.sort_values(by='val_dice', ascending=False, inplace=True) # since our best trial had highest dice score
df.head()

# first item after sorting
best_lr = df.iloc[0]['lr']
best_wd = df.iloc[0]['weight_decay']
best_bs = int(df.iloc[0]['batch_size'])

print(f"Best Hyperparameters:")
print(f"Learning Rate:  {best_lr:.6f}")
print(f"Weight Decay:   {best_wd:.6f}")
print(f"Batch Size:     {best_bs}")

print("="*60)
print("Final Model training!")
print("="*60)

l.seed_everything(1744)

# loading config and setup data
train_config = load_config("/content/drive/MyDrive/Dataset_extracted/train_config.yaml")
train_img_list, train_anno_list, val_img_list, val_anno_list, _, _ = create_dir(train_config['data_path'])

# creating datasets
train_dataset = VOCDataset(
    train_img_list,
    train_anno_list,
    phase='train',
    n_classes=2,
    input_shape=(256, 256),
    transform=DataTransform())

val_dataset = VOCDataset(
    val_img_list,
    val_anno_list,
    phase='val',
    n_classes=2,
    input_shape=(256, 256),
    transform=DataTransform())

# creating dataloaders with BEST batch size
train_loader = DataLoader(
    train_dataset,
    batch_size=best_bs,
    num_workers=8,
    shuffle=True,
    pin_memory=True,
    drop_last=True)

val_loader = DataLoader(
    val_dataset,
    batch_size=best_bs,
    num_workers=8,
    shuffle=False,
    pin_memory=True,
    drop_last=True)


model = UNetCrackSegmentation(
    num_classes=2,
    lr=best_lr, # updated lr
    weight_decay=best_wd) # updated weight decay


model_checkpoint = ModelCheckpoint(
    dirpath=train_config["save_dir"],
    filename="FINAL-{epoch}-{val_loss:.2f}-{val_dice:.2f}",
    save_last=True,
    save_top_k=1,
    monitor="val_dice",
    mode="max")

early_stop_callback = EarlyStopping(
    monitor='val_dice',
    patience=5,
    mode='max',
    verbose=True)

csv_logger = CSVLogger(save_dir="/content/drive/MyDrive/Dataset_extracted/training_logs/", name="train_model")

# trainer (40 epochs unlike tuning -> 20)
trainer = l.Trainer(
    strategy=SingleDeviceStrategy(device="cuda:0"),
    precision="16-mixed",
    max_epochs=40,
    sync_batchnorm=True,
    callbacks=[model_checkpoint, early_stop_callback],
    logger=csv_logger,
    enable_progress_bar=True)

print("="*60)

trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)

print("="*60)
print("Training Complete!")
print("="*60)
print(f"Best model saved at:")
print(f"   {model_checkpoint.best_model_path}")
print("="*60)

"""###Evaluation"""

train_config = load_config("/content/drive/MyDrive/Dataset_extracted/train_config.yaml")

model_checkpoint = ModelCheckpoint(
    dirpath=train_config["save_dir"],
    filename="FINAL-{epoch}-{val_loss:.2f}-{val_dice:.2f}",
    save_last=True,
    save_top_k=1,
    monitor="val_dice",
    mode="max")

import matplotlib.pyplot as plt
import torch
import numpy as np
import os

def visualize_predictions(model, dataset, indices=[0, 1, 2], save=False, save_dir="/content/drive/MyDrive/Dataset_extracted/vis", dice_scores_list=None):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    if save:
        os.makedirs(save_dir, exist_ok=True)

    for idx in indices:
        # loading input gt
        image, true_mask = dataset[idx]
        input_tensor = image.unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(input_tensor)
            probs = torch.softmax(output, dim=1)
            pred_mask = torch.argmax(probs, dim=1).squeeze(0).cpu()

        # de-normalize for display
        image_np = image.permute(1, 2, 0).cpu().numpy()
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image_np = (image_np * std + mean).clip(0, 1)

        # convert masks to numpy
        true_mask_np = true_mask.cpu().numpy()
        pred_mask_np = pred_mask.numpy()

        # in case we gotta display dicee here
        current_dice_score = None
        if dice_scores_list is not None:
            # dice_scores_list is a list of (original_index, dice_value)
            # We need to find the dice value for the current 'idx'.
            for original_idx, score in dice_scores_list:
                if original_idx == idx:
                    current_dice_score = score
                    break

        # plotting 3 columns/ row
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        axes[0].imshow(image_np)
        axes[0].set_title(f"Original Image (Index: {idx})")

        axes[1].imshow(true_mask_np, cmap="gray")
        axes[1].set_title("Ground Truth")

        if current_dice_score is not None:
            axes[2].imshow(pred_mask_np, cmap="gray")
            axes[2].set_title(f"Predicted Mask (Dice: {current_dice_score:.3f})")
        else:
            axes[2].imshow(pred_mask_np, cmap="gray")
            axes[2].set_title("Predicted Mask")

        # Turn off all axes
        for ax in axes.flatten():
            ax.axis("off")

        plt.tight_layout()
        if save:
            plt.savefig(f"{save_dir}/predict_{idx}.png")
        plt.show()

"""#### visualizing 20 predictions from validation set"""

print("\n generating predictions...")

# load the best model
best_model = UNetCrackSegmentation.load_from_checkpoint(model_checkpoint.best_model_path)

# generate predictions on validation set
visualize_predictions(
    best_model,
    val_dataset,
    indices=range(20),
    save=True,
    save_dir="/content/drive/MyDrive/Dataset_extracted/final_predictions"
)

print("predictions saved")

"""#### Testing on manual params"""

from torch.utils.data import DataLoader
from pytorch_lightning import Trainer
from pytorch_lightning.strategies import SingleDeviceStrategy

# loading best model
best_model_path = "/content/drive/MyDrive/Dataset_extracted/weights/epoch=34-val_loss=0.18-val_dice=0.88.ckpt"
model = UNetCrackSegmentation.load_from_checkpoint(best_model_path)
model.eval()

# setting up test set
test_img_list, test_mask_list = [], []
test_ids_path = "/content/drive/MyDrive/Dataset_extracted/ImageSets/Segmentation/test.txt"
base_img = "/content/drive/MyDrive/Dataset_extracted/RGB image/{}.jpg"
base_mask = "/content/drive/MyDrive/Dataset_extracted/ground truth/{}.png"

with open(test_ids_path, 'r') as f:
    for line in f:
        file_id = line.strip()
        test_img_list.append(base_img.format(file_id))
        test_mask_list.append(base_mask.format(file_id))

# creating test dataset & dataloader
test_dataset = VOCDataset(
    img_list=test_img_list,
    anno_list=test_mask_list,
    phase='test',
    transform=DataTransform(),
    n_classes=2,
    input_shape=(256, 256)
)

test_loader = DataLoader(
    test_dataset,
    batch_size=1,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

# running eval
trainer = Trainer(
    strategy=SingleDeviceStrategy(device="cuda:0"),
    precision="16-mixed",
    logger=False,
    enable_progress_bar=True,
)

trainer.validate(model, dataloaders=test_loader)

visualize_predictions(model, dataset, indices=range(10))

"""```
Analysis 1:

it appears that the model hyperparameters need to be adjusted.
Overfitting is a possibility, especially when looking at predictions.
- learning rate (less learning, less overfitting?)
- lessen batch size (16 -> 8?)

Maybe, look into automated hyperparameter tuning (Raytune / Optuna)
resources:
Raytune docs: https://docs.ray.io/en/latest/tune/index.html
Optuna docs: https://optuna.readthedocs.io/en/stable/ (recommended)
```

#### Testing with updated params
"""

from torch.utils.data import DataLoader
from pytorch_lightning import Trainer
from pytorch_lightning.strategies import SingleDeviceStrategy

print("="*60)
print("evaluating on test set")
print("="*60)

# loading best model
best_model_path = "/content/drive/MyDrive/Dataset_extracted/weights/FINAL-epoch=22-val_loss=0.18-val_dice=0.88.ckpt"
model = UNetCrackSegmentation.load_from_checkpoint(best_model_path)
model.eval()

# setting up test set
test_img_list, test_mask_list = [], []
test_ids_path = "/content/drive/MyDrive/Dataset_extracted/ImageSets/Segmentation/test.txt"
base_img = "/content/drive/MyDrive/Dataset_extracted/RGB image/{}.jpg"
base_mask = "/content/drive/MyDrive/Dataset_extracted/ground truth/{}.png"

with open(test_ids_path, 'r') as f:
    for line in f:
        file_id = line.strip()
        test_img_list.append(base_img.format(file_id))
        test_mask_list.append(base_mask.format(file_id))

print(f"test set size: {len(test_img_list)} images")

# creating test dataset & dataloader
test_dataset = VOCDataset(
    img_list=test_img_list,
    anno_list=test_mask_list,
    phase='test',
    transform=DataTransform(),
    n_classes=2,
    input_shape=(256, 256)
)

test_loader = DataLoader(
    test_dataset,
    batch_size=1,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

# running eval
trainer = Trainer(
    strategy=SingleDeviceStrategy(device="cuda:0"),
    precision="16-mixed",
    logger=False,
    enable_progress_bar=True,
)

test_results = trainer.validate(model, dataloaders=test_loader)

import pandas as pd
train_metrics = "/content/logs/final_model/version_0/metrics.csv"
train_df = pd.read_csv(train_metrics)
train_df.head()

# getting best epoch metrics from train_df
best_epoch = train_df.loc[train_df['val_dice'].idxmax()]['epoch']
best_epoch_metrics = train_df.loc[train_df['epoch'] == best_epoch]
display(best_epoch_metrics)

# sorting best_epoch_metrics with highest dice score first
best_epoch_metrics.sort_values(by='val_dice', ascending=False, inplace=True)
validation_metrics = best_epoch_metrics[['val_acc', 'val_precision', 'val_recall', 'val_dice', 'val_iou', 'val_loss']].iloc[0]
display(validation_metrics)

val_acc = validation_metrics['val_acc']
val_precision = validation_metrics['val_precision']
val_recall = validation_metrics['val_recall']
val_dice = validation_metrics['val_dice']
val_iou = validation_metrics['val_iou']
val_loss = validation_metrics['val_loss']

"""
```
TRAINING:
  Train Accuracy: 0.884816
  Train Dice:     0.884274
  Train IoU:      0.809517
  Train Loss:     0.178585

VALIDATION:
  Val Accuracy:   0.881099
  Val Dice:       0.883093
  Val IoU:        0.808104
  Val Loss:       0.179092
  Val Precision:  0.885816
  Val Recall:     0.881099
```
"""

TP, FP, TN, FN = 0, 0, 0, 0

for idx in range(len(test_dataset)):
    image, true_mask = test_dataset[idx]
    with torch.no_grad():
        output = model(image.unsqueeze(0).to(device))
        pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()

    has_crack_gt = np.any(true_mask.numpy() == 1)
    has_crack_pred = np.any(pred_mask == 1)

    if has_crack_gt and has_crack_pred:
        TP += 1
    elif has_crack_gt and not has_crack_pred:
        FN += 1
    elif not has_crack_gt and has_crack_pred:
        FP += 1
    else:
        TN += 1

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# building confusion matrix based on the image level counts
cmat = np.array([[TP, FN],
                 [FP, TN]], dtype=np.int64)

# normalize
cmat_norm = cmat / np.sum(cmat)

# adding labels + percentages
labels = np.array([
    [f"TP = {TP}\n({cmat_norm[0,0]*100:.2f}%)",
     f"FN = {FN}\n({cmat_norm[0,1]*100:.2f}%)"],
    [f"FP = {FP}\n({cmat_norm[1,0]*100:.2f}%)",
     f"TN = {TN}\n({cmat_norm[1,1]*100:.2f}%)"]
])


plt.figure(figsize=(6,6))
sns.heatmap(cmat_norm, annot=labels, fmt='', cmap='Reds', square=True,
            xticklabels=['Pred: Crack', 'Pred: Background'],
            yticklabels=['Actual: Crack', 'Actual: Background'])
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('Confusion Matrix with TP, FP, FN, TN')
plt.show()

"""# Finetuning on Aircraft data

## Loading the Dataset and Formatting the Directory
"""

#defining the dataset path:
DATASET_PATH = "/content/drive/MyDrive/Aircraft_Crack_Dataset"

import os

#exploring the current directory:
dataset_path =  "/content/drive/MyDrive/Aircraft_Crack_Dataset"

for root, dirs, files in os.walk(dataset_path):
    print(f" Directory: {root}")
    print(f"  ->  Subdirectories: {dirs}")
    print(f"  ->  Files: {len(files)}")
    print(f"    * Sample: {files[:5]}")
    print("-" * 50)

raw_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/images"
mask_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/masks"

raw_files = sorted([f for f in os.listdir(raw_dir) if f.lower().endswith('.png')])
mask_files = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith('.png')])

print("sample raw images:", raw_files[:10])
print("sample masks:", mask_files[:10])

import os
from PIL import Image
from sklearn.model_selection import train_test_split

raw_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/images"
mask_dir = "/content/drive/MyDrive/Aircraft_Crack_Dataset/masks"
base_new = "/content/drive/MyDrive/aircraft_set_formatted"
target_size = (512, 512)

# creating o/p folders
os.makedirs(f"{base_new}/JPEGImages", exist_ok=True)
os.makedirs(f"{base_new}/SegmentationClass", exist_ok=True)
os.makedirs(f"{base_new}/ImageSets/Segmentation", exist_ok=True)

pairs = []
missing = []

# matching image mask pairs by name
for img_file in sorted(os.listdir(raw_dir)):
    if not img_file.lower().endswith(".png"):
        continue
    base = os.path.splitext(img_file)[0]
    mask_file = f"{base}_mask.png"
    mask_path = os.path.join(mask_dir, mask_file)
    if os.path.exists(mask_path):
        pairs.append((img_file, mask_file))
    else:
        missing.append(img_file)

print(f"{len(pairs)} pairs found")
if missing:
    print(f"{len(missing)} images without masks:")
    for m in missing:
        print("   ", m)

# train/val/test
if len(pairs) > 0:
    train_pairs, temp = train_test_split(pairs, test_size=0.3, random_state=42)
    val_pairs, test_pairs = train_test_split(temp, test_size=0.5, random_state=42)
    splits = {"train": train_pairs, "val": val_pairs, "test": test_pairs}

    for split, pair_list in splits.items():
        txt_path = f"{base_new}/ImageSets/Segmentation/{split}.txt"
        with open(txt_path, "w") as f:
            for img_file, mask_file in pair_list:
                img_id = os.path.splitext(img_file)[0]

                img = Image.open(os.path.join(raw_dir, img_file)).convert("RGB").resize(target_size, Image.BILINEAR)
                mask = Image.open(os.path.join(mask_dir, mask_file)).convert("L").resize(target_size, Image.NEAREST)

                img.save(f"{base_new}/JPEGImages/{img_id}.png")
                mask.save(f"{base_new}/SegmentationClass/{img_id}.png")
                f.write(f"{img_id}\n")

    print("dataset rebuilt with correct pairing!")
else:
    print("no valid pairs!")

"""## Verifying Dataset"""

base_path = "/content/drive/MyDrive/aircraft_set_formatted"
for split in ['train', 'val', 'test']:
    txt = f"{base_path}/ImageSets/Segmentation/{split}.txt"
    with open(txt) as f:
        ids = [line.strip() for line in f]
    print(f"{split}: {len(ids)} IDs")

from PIL import Image
import matplotlib.pyplot as plt

sample_id = "segment_3_image11"
img = Image.open(f"{base_path}/JPEGImages/{sample_id}.png")
mask = Image.open(f"{base_path}/SegmentationClass/{sample_id}.png")

plt.figure(figsize=(10,4))
plt.subplot(1,2,1); plt.imshow(img); plt.title("Image"); plt.axis("off")
plt.subplot(1,2,2); plt.imshow(mask, cmap="gray"); plt.title("Mask"); plt.axis("off")
plt.show()

# visualizing sample of image mask pairs
import random
from PIL import Image
import matplotlib.pyplot as plt
import os

base = "/content/drive/MyDrive/aircraft_set_formatted"
ids = open(f"{base}/ImageSets/Segmentation/train.txt").read().splitlines()
samples = random.sample(ids, 3)

for s in samples:
    img = Image.open(f"{base}/JPEGImages/{s}.png")
    mask = Image.open(f"{base}/SegmentationClass/{s}.png")
    plt.figure(figsize=(10,4))
    plt.subplot(1,2,1); plt.imshow(img); plt.title(s + " (Image)"); plt.axis("off")
    plt.subplot(1,2,2); plt.imshow(mask, cmap="gray"); plt.title(s + " (Mask)"); plt.axis("off")
    plt.show()

"""## Preprocessing, Augmentation, and Dataset Class

"""

import os.path as osp

def create_dir(path):
    """
    creates lists of image and mask paths for training, validation, and testing
    following the PASCAL VOC dataset format.
    """

    image_path_general = osp.join(path, 'JPEGImages', '%s.png')
    mask_path_general  = osp.join(path, 'SegmentationClass', '%s.png')

    # paths to txt files from the split
    train_id_names = osp.join(path, 'ImageSets', 'Segmentation', 'train.txt')
    val_id_names   = osp.join(path, 'ImageSets', 'Segmentation', 'val.txt')
    test_id_names  = osp.join(path, 'ImageSets', 'Segmentation', 'test.txt')


    train_img_list, train_mask_list = [], []
    val_img_list, val_mask_list     = [], []
    test_img_list, test_mask_list   = [], []


    # populating the lists based on split txt files
    with open(train_id_names) as f:
        for line in f:
            file_id = line.strip()
            train_img_list.append(image_path_general % file_id)
            train_mask_list.append(mask_path_general % file_id)


    with open(val_id_names) as f:
        for line in f:
            file_id = line.strip()
            val_img_list.append(image_path_general % file_id)
            val_mask_list.append(mask_path_general % file_id)


    with open(test_id_names) as f:
        for line in f:
            file_id = line.strip()
            test_img_list.append(image_path_general % file_id)
            test_mask_list.append(mask_path_general % file_id)

    return (
        train_img_list, train_mask_list,
        val_img_list, val_mask_list,
        test_img_list, test_mask_list
    )

import albumentations as A
from albumentations.pytorch import ToTensorV2

class DataTransform:
    def __init__(self, size=(512, 512)):
        """
        data augmentation pipeline for train, val, test splits.
        uses Albumentations to apply the same spatial transforms to image & mask.
        """
        mean = (0.485, 0.456, 0.406)
        std = (0.229, 0.224, 0.225)

        self.data_transform = {
            'train': A.Compose([
                A.RandomScale(scale_limit=0.5, p=0.5),
                A.Rotate(limit=10, p=0.5),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),  # optional: can add more variation
                A.Resize(size[0], size[1]),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ]),
            'val': A.Compose([
                A.Resize(size[0], size[1]),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ]),
            'test': A.Compose([
                A.Resize(size[0], size[1]),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])
        }

    def __call__(self, phase, image, mask):
        """
        args:
            phase: 'train' | 'val' | 'test'
            image: numpy array (H, W, 3)
            mask: numpy array (H, W)
        returns:
            image_tensor: torch.FloatTensor
            mask_tensor: torch.LongTensor
        """
        augmented = self.data_transform[phase](image=image, mask=mask)
        image = augmented['image']
        mask = augmented['mask'].long()  # ensure mask is LongTensor for CE Loss
        return image, mask

import torch
import torch.utils.data as data
import numpy as np
from PIL import Image

class VOCDataset(data.Dataset):
    def __init__(self, img_list, anno_list, phase, transform, n_classes=2, input_shape=(512, 512)):
        """
        Args:
            img_list: list of image file paths
            anno_list: list of mask file paths
            phase: 'train', 'val', 'test'
            transform: DataTransform instance
            n_classes: number of segmentation classes (default 2)
            input_shape: desired (width, height)
        """
        self.img_list = img_list
        self.anno_list = anno_list
        self.phase = phase
        self.transform = transform
        self.n_classes = n_classes
        self.input_shape = input_shape

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, index):
        img, mask = self.pull_item(index)
        return img, mask

    def pull_item(self, index):
        image_file_path = self.img_list[index]
        mask_file_path = self.anno_list[index]

        # loading image & mask
        img = Image.open(image_file_path).convert("RGB")
        mask = Image.open(mask_file_path).convert("L")  # grayscale

        # resizing both
        img = img.resize(self.input_shape, resample=Image.BILINEAR)
        mask = mask.resize(self.input_shape, resample=Image.NEAREST)

        # converting to numpy
        img = np.array(img)
        mask = np.array(mask).astype('int64')
        mask[mask > 0] = 1  # convert all nonzero to 1

        # safety check for shape issues
        if img.shape[:2] != mask.shape:
            print(f"[âš ï¸] Shape mismatch at index {index}: img={img.shape}, mask={mask.shape}")

        # applying albumentation transgforms
        img, mask = self.transform(self.phase, img, mask)

        return img, mask

formatted_data_path = "/content/drive/MyDrive/aircraft_set_formatted/"
train_img_list, train_mask_list, val_img_list, val_mask_list, test_img_list, test_mask_list = create_dir(formatted_data_path)

train_dataset = VOCDataset(
    img_list=train_img_list,
    anno_list=train_mask_list,
    phase='train',
    transform=DataTransform(),
    input_shape=(512, 512)
)

from torch.utils.data import DataLoader
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

batch = next(iter(train_loader))
images, masks = batch
print("Batch:", images.shape, masks.shape)

import cv2
import numpy as np
import matplotlib.pyplot as plt

#mariam's one
# to visualize the image and gt mask overlay :>

def visualize_overlay(dataset, index=0, alpha=0.5):
    image, mask = dataset[index]  # image: tensor [3,H,W], mask: [H,W]
    image_np = image.permute(1, 2, 0).cpu().numpy()
    mask_np = mask.cpu().numpy()

    # reversing imgagenet normalization
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    image_np = (image_np * std) + mean
    image_np = np.clip(image_np, 0, 1)

    # creating red overlay where mask == 1
    mask_rgb = np.zeros_like(image_np)
    if mask_np.max() > 0:
        mask_rgb[..., 0] = mask_np / mask_np.max()  # normalize to 0â€“1
    else:
        mask_rgb[..., 0] = mask_np

    overlay = (1 - alpha) * image_np + alpha * mask_rgb

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1); plt.imshow(image_np); plt.title("Image"); plt.axis("off")
    plt.subplot(1, 3, 2); plt.imshow(mask_np, cmap="gray"); plt.title("Mask"); plt.axis("off")
    plt.subplot(1, 3, 3); plt.imshow(overlay); plt.title("Overlay"); plt.axis("off")
    plt.show()

visualize_overlay(train_dataset, index=0)
visualize_overlay(train_dataset, index=5)
visualize_overlay(train_dataset, index=10)

"""## Creating YAML File for Training Configuration"""

yaml_content = """
batch_size: 4
test_batch_size: 4
num_workers: 4
input_shape: [512, 512]
n_classes: 2
pretrained: True

optimizer: adam
max_lr: 0.0005
min_lr: 0.000001
momentum: 0.9
weight_decay: 0
lr_decay_type: 'cos'

cuda: True
distributed: False
sync_bn: False

save_dir: "/content/drive/MyDrive/aircraft_set_formatted/weights/"
data_path: "/content/drive/MyDrive/aircraft_set_formatted/"
model_path: ""
save_period: 20
result_dir: "/content/drive/MyDrive/aircraft_set_formatted/results/"
"""
with open("/content/drive/MyDrive/aircraft_set_formatted/train_config.yaml", "w") as f:
    f.write(yaml_content)

print("YAML config file saved at /content/drive/MyDrive/aircraft_set_formatted/train_config.yaml")

"""## Training"""

import yaml

def load_config(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

"""

```
Phase 1: Frozen Encoder (15 epochs)

Encoder (MobileNetV2) frozen â†’ only decoder trains
Fast training (fewer parameters)
Learns task-specific decoder features
Higher LR (0.001) safe since only decoder updates

Phase 2: Unfrozen (25 epochs)

Unfreeze encoder â†’ full model trains
Slower training (all parameters)
Fine-tunes encoder features for cracks
Lower LR (0.0001) for stability

# phase 1 (frozen)
lr: 0.001          # higher - only decoder
epochs: 15         # fewer - decoder trains fast
patience: 5        # quick early stopping

# phase 2 (unfrozen)
lr: 0.0001         # 10x lower - full model
epochs: 25         # more - careful fine-tuning
patience: 10       # more patience
```

"""

# freezing functions sourced from https://github.com/qubvel-org/segmentation_models.pytorch/issues/79

def freeze_encoder(model):
    # freeze encoder to preserve crack detection features
    print("freezing encoder...")

    encoder = model.model.encoder

    for child in encoder.children():
        for param in child.parameters():
            param.requires_grad = False

    frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"  frozen (encoder):     {frozen:,} parameters")
    print(f"  trainable (decoder):  {trainable:,} parameters")

    return model


def unfreeze_encoder(model):
    # unfreeze encoder for full fine-tuning (both encoder & decoder train here)
    print("unfreezing encoder...")

    encoder = model.model.encoder

    for child in encoder.children():
        for param in child.parameters():
            param.requires_grad = True

    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  all parameters trainable: {trainable:,}")

    return model

"""### Phase 1: frozen encoder (decoder acclimatizes to aircraft data)"""

# finetuning (industrial) -> (our aircraft one)

import torch
import numpy as np
import pytorch_lightning as l
from torch.utils.data import DataLoader
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import CSVLogger
from pytorch_lightning.strategies import SingleDeviceStrategy
import os
import gc

# trying to fix issues here

import torch.multiprocessing as mp
mp.set_sharing_strategy('file_system')

# clearing cache
torch.cuda.empty_cache()
gc.collect()

# to resolve issues because my optuna model checkpoint saved with older Lightning
torch.serialization.add_safe_globals([
    np.core.multiarray.scalar,
    np.dtype,
    np.float64,
    np.dtypes.Float64DType
])


print("="*70)
print("Finetuning model on aircraft data")
print("="*70)


# loading our pretrained crack model (optuna one)
PRETRAINED_CRACK_MODEL = "/content/drive/MyDrive/Dataset_extracted/weights/FINAL-epoch=22-val_loss=0.18-val_dice=0.88.ckpt"

pretrained_model = UNetCrackSegmentation.load_from_checkpoint(
    PRETRAINED_CRACK_MODEL,
    num_classes=2,
    map_location="cpu",
    weights_only=False
)

# loading aircraft dataset (already setup prior to this)
aircraft_config_path = "/content/drive/MyDrive/aircraft_set_formatted/train_config.yaml"
aircraft_config = load_config(aircraft_config_path)

aircraft_train_img, aircraft_train_mask, aircraft_val_img, aircraft_val_mask, aircraft_test_img, aircraft_test_mask = create_dir(
    aircraft_config['data_path']
)

print(f"Our aircraft dataset:")
print(f"   training:   {len(aircraft_train_img)} images")
print(f"   validation: {len(aircraft_val_img)} images")
print(f"   test:       {len(aircraft_test_img)} images")

# train & val datasets creation
aircraft_train_dataset = VOCDataset(
    aircraft_train_img,
    aircraft_train_mask,
    phase='train',
    n_classes=2,
    input_shape=(512, 512),
    transform=DataTransform(size=(512, 512))
)

aircraft_val_dataset = VOCDataset(
    aircraft_val_img,
    aircraft_val_mask,
    phase='val',
    n_classes=2,
    input_shape=(512, 512),
    transform=DataTransform(size=(512, 512))
)

# fixing issue by reducing batch size to work with 0 workers for stability
aircraft_bs = 2  # reduced from 4 to 2 to save memory

aircraft_train_loader = DataLoader(
    aircraft_train_dataset,
    batch_size=aircraft_bs,
    num_workers=0,  # changed to 0  as this is the most reliable fix
    shuffle=True,
    pin_memory=False,  # disabled to reduce memory pressure
    drop_last=True
)

aircraft_val_loader = DataLoader(
    aircraft_val_dataset,
    batch_size=aircraft_bs,
    num_workers=0,  # changed to 0
    shuffle=False,
    pin_memory=False,  # disabled
    drop_last=False
)


# phase 1: frozen encoder

print("\n" + "="*70)
print("Phase 1: adapting decoder to aircraft by freezing encoder")
print("="*70)

l.seed_everything(1744) # for reproducability

# freezing encoder
model_phase1 = freeze_encoder(pretrained_model)

# higher LR for decoder adaptation
model_phase1.hparams.lr = 0.001
model_phase1.hparams.weight_decay = 0.00001


os.makedirs("/content/drive/MyDrive/aircraft_set_formatted/aircraft_finetuned/", exist_ok=True)

checkpoint_phase1 = ModelCheckpoint(
    dirpath="/content/drive/MyDrive/aircraft_set_formatted/aircraft_finetuned/",
    filename="AIRCRAFT-PHASE1-frozen-{epoch}-{val_dice:.4f}",
    save_top_k=1,
    monitor="val_dice",
    mode="max"
)

early_stop_phase1 = EarlyStopping(
    monitor='val_dice',
    patience=7,
    mode='max',
    verbose=True
)

csv_logger_phase1 = CSVLogger(
    save_dir="/content/drive/MyDrive/aircraft_set_formatted/aircraft_finetuned/logs1/",
    name="phase1_frozen"
)

# trainer -> added accumulate_grad_batches to simulate larger batch size
trainer_phase1 = l.Trainer(
    strategy=SingleDeviceStrategy(device="cuda:0"),
    precision="16-mixed",
    max_epochs=20,
    callbacks=[checkpoint_phase1, early_stop_phase1],
    logger=csv_logger_phase1,
    enable_progress_bar=True,
    gradient_clip_val=1.0,
    accumulate_grad_batches=2  # simulates batch_size=4 while using less memory
)

print("\nPhase 1 training...")
trainer_phase1.fit(
    model=model_phase1,
    train_dataloaders=aircraft_train_loader,
    val_dataloaders=aircraft_val_loader
)

print(f"\nPhase 1 complete!")
print(f"Best model: {checkpoint_phase1.best_model_path}")

# clean up
del model_phase1
torch.cuda.empty_cache()
gc.collect()

"""### Phase 2: unfreeze encoder and finetune full model!"""

print("\n" + "="*70)
print("Phase 2: finetune on full model")
print("="*70)


# loading phase 1 checkpoint here!
model_phase2 = UNetCrackSegmentation.load_from_checkpoint(
    checkpoint_phase1.best_model_path,
    num_classes=2,
    lr=0.0001,  # Lower LR
    weight_decay=0.00001
)

# unfreezing encoder
model_phase2 = unfreeze_encoder(model_phase2)


checkpoint_phase2 = ModelCheckpoint(
    dirpath="/content/drive/MyDrive/aircraft_set_formatted/aircraft_finetuned/",
    filename="AIRCRAFT-PHASE2-finetuned-{epoch}-{val_dice:.4f}",
    save_top_k=1,
    monitor="val_dice",
    mode="max"
)

early_stop_phase2 = EarlyStopping(
    monitor='val_dice',
    patience=10,
    mode='max',
    verbose=True
)

csv_logger_phase2 = CSVLogger(
    save_dir="/content/drive/MyDrive/aircraft_set_formatted/aircraft_finetuned/logs2/",
    name="phase2_finetuned"
)

# trainer
trainer_phase2 = l.Trainer(
    strategy=SingleDeviceStrategy(device="cuda:0"),
    precision="16-mixed",
    max_epochs=30,
    callbacks=[checkpoint_phase2, early_stop_phase2],
    logger=csv_logger_phase2,
    enable_progress_bar=True
)

print("\nPhase 2 fine-tuning...")
trainer_phase2.fit(
    model=model_phase2,
    train_dataloaders=aircraft_train_loader,
    val_dataloaders=aircraft_val_loader
)

print("\nFine-tuning complete!")
print(f"Final model: {checkpoint_phase2.best_model_path}")

"""## Evaluation"""

print("\n" + "="*70)
print("evaluating on aircraft test set...")
print("="*70)

# load final best model
final_aircraft_model = UNetCrackSegmentation.load_from_checkpoint(
    checkpoint_phase2.best_model_path,
    num_classes=2,
    lr=0.0001,
    weight_decay=0.00001,
    map_location="cpu",  # for safety
    weights_only=False   # for compatibility
)
final_aircraft_model.eval()

# creating test dataset
aircraft_test_dataset = VOCDataset(
    aircraft_test_img,
    aircraft_test_mask,
    phase='test',
    n_classes=2,
    input_shape=(512, 512),
    transform=DataTransform(size=(512, 512))
)

print(f"Test set: {len(aircraft_test_dataset)} images")

aircraft_test_loader = DataLoader(
    aircraft_test_dataset,
    batch_size=2,
    shuffle=False,
    num_workers=0,  # to avoid worker crashes
    pin_memory=False  # false for stability
)

# evaluation
test_trainer = l.Trainer(
    strategy=SingleDeviceStrategy(device="cuda:0"),
    precision="16-mixed",
    logger=False,
    enable_progress_bar=True
)

print("\ntest evaluation...")
aircraft_test_results = test_trainer.validate(
    final_aircraft_model,
    dataloaders=aircraft_test_loader
)

# Generalization analysis
val_dice = 0.8676
test_dice = aircraft_test_results[0]['val_dice']
gap = val_dice - test_dice

print(f"Generalization Analysis:")
print(f"Validation Dice: {val_dice:.4f}")
print(f"Test Dice:       {test_dice:.4f}")
print(f"Val-Test Gap:    {gap:+.4f} ({gap*100:+.2f}%)")

if abs(gap) < 0.03:
    print(" Excellent generalization!")
elif abs(gap) < 0.05:
    print("Acceptable generalization")
else:
    print("Some overfitting detected!")

print("\n" + "="*70)
print("TL SUMMARY")
print("="*70)
print(f"Source model (infrastructure test):  0.8301")
print(f"Phase 1 (frozen encoder):             0.8409 (+1.08%)")
print(f"Phase 2 (full fine-tuning, val):     0.8676 (+4.52%)")
print(f"Phase 2 (full fine-tuning, test):    {test_dice:.4f}")
print(f"\nOverall improvement: {(test_dice - 0.8301)*100:+.2f}%")
print("="*70)

import torch
import numpy as np
import matplotlib.pyplot as plt

# visualize false positives and false negatives with color coded overlay compared to gt
def visualize_fp_fn(model, dataset, index=0, threshold=0.5):
    model.eval().cuda()
    img, mask = dataset[index]  # (C,H,W), (H,W)
    img_t = img.unsqueeze(0).cuda()

    with torch.no_grad():
        pred = model(img_t)
        probs = torch.softmax(pred, dim=1)[0,1].cpu().numpy()
        pred_mask = (probs > threshold).astype(np.uint8)

    gt_mask = mask.cpu().numpy().astype(np.uint8)

    # calculate confusion components
    TP = np.logical_and(pred_mask == 1, gt_mask == 1)
    FP = np.logical_and(pred_mask == 1, gt_mask == 0)
    FN = np.logical_and(pred_mask == 0, gt_mask == 1)
    TN = np.logical_and(pred_mask == 0, gt_mask == 0)

    # color-coded overlay
    overlay = np.zeros((*gt_mask.shape, 3), dtype=np.uint8)
    overlay[TP] = [0, 255, 0]    # green = TP
    overlay[FP] = [255, 0, 0]    # red = FP
    overlay[FN] = [0, 0, 255]    # blue = FN

    # display results
    plt.figure(figsize=(14,5))
    plt.subplot(1,4,1)
    plt.imshow(img.permute(1,2,0).cpu().numpy())
    plt.title("Original Image"); plt.axis("off")

    plt.subplot(1,4,2)
    plt.imshow(gt_mask, cmap='gray')
    plt.title("Ground Truth"); plt.axis("off")

    plt.subplot(1,4,3)
    plt.imshow(pred_mask, cmap='gray')
    plt.title("Prediction"); plt.axis("off")

    plt.subplot(1,4,4)
    plt.imshow(overlay)
    plt.title("FP/FN/TP Map")
    plt.axis("off")
    plt.tight_layout()
    plt.show()

    # printting summary metrics
    tp, fp, fn = TP.sum(), FP.sum(), FN.sum()
    precision = tp / (tp + fp + 1e-6)
    recall = tp / (tp + fn + 1e-6)
    dice = (2 * tp) / (2 * tp + fp + fn + 1e-6)
    print(f"Precision: {precision:.3f} | Recall: {recall:.3f} | Dice: {dice:.3f}")

import random
random_iindex = random.randint(0, len(aircraft_test_dataset) - 1)
visualize_fp_fn(final_aircraft_model, aircraft_test_dataset, index=random_iindex)

"""## Visualize predictions"""

# visualize model predictions from test set

def visualize_predictions(model, dataset, indices=[0, 1, 2], save=False, save_dir="/content/drive/MyDrive/outputs/", display_limit=5):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    if save:
        os.makedirs(save_dir, exist_ok=True)

    for i, idx in enumerate(indices):
        img, mask = dataset[idx]
        img_tensor = img.unsqueeze(0).to(device)

        with torch.no_grad():
            pred = model(img_tensor)
            pred = torch.argmax(pred, dim=1).squeeze().cpu()

        # convert tensors to numpy
        img_np = img.permute(1, 2, 0).cpu().numpy()
        mask_np = mask.cpu().numpy()
        pred_np = pred.numpy()

        # denormalize image (reverse imagenet normalization)
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img_np = (img_np * std + mean)
        img_np = np.clip(img_np, 0, 1)


        fig, axs = plt.subplots(1, 3, figsize=(12, 4))
        axs[0].imshow(img_np)
        axs[0].set_title(f"Original Image {idx}")
        axs[1].imshow(mask_np, cmap="gray")
        axs[1].set_title("Ground Truth")
        axs[2].imshow(pred_np, cmap="gray")
        axs[2].set_title("Prediction")

        for ax in axs: ax.axis("off")

        plt.tight_layout()
        if save:
            plt.savefig(f"{save_dir}/prediction_{idx}.png", bbox_inches='tight', dpi=100)

        # only displaying a few here (svaing all predictions tho)
        if i < display_limit:
            plt.show()
        else:
            plt.close(fig)


visualize_predictions(
    final_aircraft_model,
    aircraft_test_dataset,
    indices=range(len(aircraft_test_dataset)),
    save=True,
    save_dir="/content/drive/MyDrive/aircraft_set_formatted/aircraft_finetuned/predictions/",
    display_limit=5
)