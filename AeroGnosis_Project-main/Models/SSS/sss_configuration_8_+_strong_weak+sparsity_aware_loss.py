# -*- coding: utf-8 -*-
"""sss configuration 8 + strong-weak+sparsity aware loss

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14HAS-GfWzaV6ytK8rDBI_-B2zmCLgDJC

# 1. Setting up the Enivronment
"""

#mount drive
from google.colab import drive
drive.mount('/content/drive')

#install dependencies
!pip install -q torch torchvision
!pip install -q segmentation-models-pytorch
!pip install -q tensorboard
!pip install -q pyyaml
!pip install -q opencv-python
!pip install -q matplotlib

"""# 2. Dataset Configuration and Verification"""

DATA_ROOT = "/content/drive/MyDrive/aircraft_voc"

#verifying dataset structure
import os
from pathlib import Path

def verify_dataset_structure(data_root):
    required_dirs = [
        'JPEGImages',
        'SegmentationClass',
        'ImageSets/Segmentation'
    ]

    all_exist = True

    for dir_path in required_dirs:
        full_path = os.path.join(data_root, dir_path)
        exists = os.path.exists(full_path)

        if exists:
            # Count files
            if os.path.isdir(full_path):
                num_files = len([f for f in os.listdir(full_path) if os.path.isfile(os.path.join(full_path, f))])
                print(f"  {dir_path}: {num_files} files")
        else:
            print(f" {dir_path} is NOT FOUND")
            all_exist = False

    return all_exist

dataset_valid = verify_dataset_structure(DATA_ROOT)

if not dataset_valid:
    print("\n Something wrong")
else:
    print("\n Dataset all good")

"""#3. Create UNet++ with MobileNetV2"""

#create junior UNet++ model
unetpp_model_code = '''import torch
import torch.nn as nn
import torch.nn.functional as F
import segmentation_models_pytorch as smp

class UNetPlusPlus(nn.Module):
    #UNet++ model with MobileNetV2 encoder for semantic segmentation

    def __init__(self, num_classes, encoder_name='mobilenet_v2', pretrained=True):
        super(UNetPlusPlus, self).__init__()

        self.num_classes = num_classes
        self.encoder_name = encoder_name

        #creation of UNet++ model with MobileNetV2 encoder
        self.backbone = smp.UnetPlusPlus(
            encoder_name=encoder_name,
            encoder_weights='imagenet' if pretrained else None,
            in_channels=3,
            classes=num_classes,
            activation=None,  #no activation, return logits
        )

    def forward(self, x, need_fp=False):
           # x: input tensor [B, 3, H, W]
           # need_fp: whether to return feature maps (for consistency regularization)
        #Returns:
            #logits [B, num_classes, H, W] or (logits, features) if need_fp
        # Get output from UNet++
        out = self.backbone(x)

        if need_fp:
            # Use the output as feature representation for consistency loss
            return out, out

        return out

    def decode(self, features):
        return features

def UNetPlusPlusMobileNetV2(num_classes, pretrained=True):
    return UNetPlusPlus(num_classes=num_classes, encoder_name='mobilenet_v2', pretrained=pretrained)
'''

#writing UNet++ model to file
os.makedirs('/content/my_project/model/semseg', exist_ok=True)
with open('/content/my_project/model/semseg/unetplusplus.py', 'w') as f:
    f.write(unetpp_model_code)

print("Created UNet++ model file: model/semseg/unetplusplus.py")
print("  Encoder: MobileNetV2 (lightweight and efficient)")
print("  Decoder: UNet++ nested skip connections")

"""# 6. Evaluation"""

eval_script = f'''#!/usr/bin/env python3
"""
Evaluation script for trained UNet++ model
"""

import os
import sys
import torch
import numpy as np
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as T
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

sys.path.insert(0, '/content/my_project')
from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2

DATA_ROOT = "{DATA_ROOT}"
NUM_CLASSES = 2
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class EvalDataset(Dataset):
    def __init__(self, data_root, split_file, transform=None, size=512):
        self.data_root = data_root
        self.transform = transform
        self.size = size

        with open(split_file, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]

        # Load image
        img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.jpg")
        if not os.path.exists(img_path):
            img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.png")
        img = Image.open(img_path).convert('RGB')

        #Load mask
        mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.png")
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.jpg")
        mask = Image.open(mask_path)

        # Store original as numpy array (not PIL Image)
        img_orig = np.array(img.copy())

        #Resize
        img = img.resize((self.size, self.size), Image.BILINEAR)
        mask = mask.resize((self.size, self.size), Image.NEAREST)

        if self.transform:
            img = self.transform(img)

        mask = torch.from_numpy(np.array(mask)).long()

        return img, mask, img_orig, img_id

def compute_iou(pred, target, num_classes, ignore_index=255):
    """Compute mean IoU"""
    ious = []
    pred = pred.flatten()
    target = target.flatten()

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        # Ignore pixels marked as ignore_index
        valid = (target != ignore_index)
        pred_cls = pred_cls & valid
        target_cls = target_cls & valid

        intersection = (pred_cls & target_cls).sum()
        union = (pred_cls | target_cls).sum()

        if union == 0:
            continue

        iou = intersection.float() / union.float()
        ious.append(iou.item())

    return np.mean(ious) if ious else 0.0

def compute_all_metrics(pred, target, num_classes, ignore_index=255):
    #Compute IoU, Dice, Precision, Recall, Accuracy
    pred = pred.flatten()
    target = target.flatten()

    # Filter out ignore index
    valid_mask = (target != ignore_index)
    pred = pred[valid_mask]
    target = target[valid_mask]

    ious = []
    dices = []
    precisions = []
    recalls = []

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        #True Positives, False Positives, False Negatives
        tp = (pred_cls & target_cls).sum().float()
        fp = (pred_cls & ~target_cls).sum().float()
        fn = (~pred_cls & target_cls).sum().float()

        # IoU
        union = tp + fp + fn
        iou = tp / (union + 1e-8)
        ious.append(iou.item())

        #Dice (F1)
        dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)
        dices.append(dice.item())

        #Precision
        precision = tp / (tp + fp + 1e-8)
        precisions.append(precision.item())

        #Recall
        recall = tp / (tp + fn + 1e-8)
        recalls.append(recall.item())

    # Overall Accuracy
    accuracy = (pred == target).float().mean().item()

    return {{
        'iou': ious,
        'dice': dices,
        'precision': precisions,
        'recall': recalls,
        'accuracy': accuracy,
        'mean_iou': np.mean(ious),
        'mean_dice': np.mean(dices),
        'mean_precision': np.mean(precisions),
        'mean_recall': np.mean(recalls)
    }}

#Load model
print("Loading trained model...")
model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=False)

# Find the best model file (with dice score in filename)
import glob
checkpoint_dir = 'exp/aircraft_unetpp'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'best_model_dice=*.pth'))

if checkpoint_files:
    # Get the file with highest dice score
    checkpoint_path = max(checkpoint_files, key=lambda x: float(x.split('dice=')[1].split('.pth')[0]))
    print(f"Found best checkpoint: {{os.path.basename(checkpoint_path)}}")
else:
    #Fallback to generic name if it exists
    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')
    if not os.path.exists(checkpoint_path):
        print("Error: No checkpoint found!")
        print(f"Looking in: {{checkpoint_dir}}")
        import sys
        sys.exit(1)

checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(DEVICE)
model.eval()

print(f"Model loaded from epoch {{checkpoint['epoch']}}")
if 'dice' in checkpoint:
    print(f"Best Dice score: {{checkpoint['dice']:.4f}}")

#Load validation dataset
val_transform = T.Compose([
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_file = os.path.join(DATA_ROOT, 'ImageSets/Segmentation/val.txt')
val_dataset = EvalDataset(DATA_ROOT, val_file, val_transform, size=512)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)

print(f"\\nEvaluating on {{len(val_dataset)}} validation images...")

# Evaluate
os.makedirs('results', exist_ok=True)
all_ious = []
all_dices = []
all_precisions = []
all_recalls = []
all_accuracies = []
all_losses = []
all_preds = []
all_targets = []

#Loss function for evaluation
criterion = torch.nn.CrossEntropyLoss(ignore_index=255)

with torch.no_grad():
    for idx, (image, mask, img_orig, img_id) in enumerate(tqdm(val_loader)):
        image = image.to(DEVICE)
        mask = mask.to(DEVICE)

        output = model(image)

        #Compute loss
        loss = criterion(output, mask)
        all_losses.append(loss.item())

        pred = output.argmax(dim=1).cpu().numpy()[0]
        mask_np = mask.cpu().numpy()[0]

        # Store for confusion matrix
        valid_mask = (mask_np != 255)
        all_preds.extend(pred[valid_mask].flatten())
        all_targets.extend(mask_np[valid_mask].flatten())

        #compute all metrics
        metrics = compute_all_metrics(torch.from_numpy(pred), torch.from_numpy(mask_np), NUM_CLASSES)
        all_ious.append(metrics['mean_iou'])
        all_dices.append(metrics['mean_dice'])
        all_precisions.append(metrics['mean_precision'])
        all_recalls.append(metrics['mean_recall'])
        all_accuracies.append(metrics['accuracy'])

        #Save visualizations for first 10 images
        if idx < 10:
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            # Original image
            axes[0].imshow(img_orig[0])
            axes[0].set_title('Input Image')
            axes[0].axis('off')

            axes[1].imshow(mask_np, cmap='tab20', vmin=0, vmax=NUM_CLASSES-1)
            axes[1].set_title('Ground Truth')
            axes[1].axis('off')

            axes[2].imshow(pred, cmap='tab20', vmin=0, vmax=NUM_CLASSES-1)
            axes[2].set_title(f'Prediction\\nIoU: {{metrics["mean_iou"]:.3f}} | Dice: {{metrics["mean_dice"]:.3f}}')
            axes[2].axis('off')

            plt.tight_layout()
            plt.savefig(f'results/prediction_{{idx}}.png', dpi=150, bbox_inches='tight')
            plt.close()

mean_iou = np.mean(all_ious)
mean_dice = np.mean(all_dices)
mean_precision = np.mean(all_precisions)
mean_recall = np.mean(all_recalls)
mean_accuracy = np.mean(all_accuracies)
mean_loss = np.mean(all_losses)

#Create confusion matrix
cm = confusion_matrix(all_targets, all_preds, labels=list(range(NUM_CLASSES)))

#Normalize confusion matrix for percentages
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

#Plot confusion matrix with both counts and percentages
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
class_names = ['Background', 'Aircraft'] if NUM_CLASSES == 2 else [f'Class {{i}}' for i in range(NUM_CLASSES)]

#Confusion Matrix Raw Counts
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
            xticklabels=class_names,
            yticklabels=class_names,
            cbar_kws={{'label': 'Count'}},
            annot_kws={{'size': 14, 'weight': 'bold'}})
ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold', pad=15)
ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax1.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

# Confusion Matrix Percentages
sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens', ax=ax2,
            xticklabels=class_names,
            yticklabels=class_names,
            cbar_kws={{'label': 'Percentage (%)'}},
            annot_kws={{'size': 14, 'weight': 'bold'}})
ax2.set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold', pad=15)
ax2.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax2.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

plt.suptitle(f'Model Performance: IoU={{mean_iou:.4f}} | Dice={{mean_dice:.4f}} | Accuracy={{mean_accuracy:.4f}}',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('results/confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.close()

print(f"\\n{{'='*70}}")
print(f"EVALUATION RESULTS")
print(f"{{'='*70}}")
print(f"\\n  Overall Metrics:")
print(f"{{'─'*70}}")
print(f"  Loss:           {{mean_loss:.4f}} (±{{np.std(all_losses):.4f}})")
print(f"  Accuracy:       {{mean_accuracy:.4f}} (±{{np.std(all_accuracies):.4f}})")
print(f"  Mean IoU:       {{mean_iou:.4f}} (±{{np.std(all_ious):.4f}})")
print(f"  Mean Dice:      {{mean_dice:.4f}} (±{{np.std(all_dices):.4f}})")
print(f"  Mean Precision: {{mean_precision:.4f}} (±{{np.std(all_precisions):.4f}})")
print(f"  Mean Recall:    {{mean_recall:.4f}} (±{{np.std(all_recalls):.4f}})")
print(f"{{'─'*70}}")
print(f"\\n Best Performance:")
print(f"{{'─'*70}}")
print(f"  Max IoU:        {{np.max(all_ious):.4f}}")
print(f"  Max Dice:       {{np.max(all_dices):.4f}}")
print(f"  Max Precision:  {{np.max(all_precisions):.4f}}")
print(f"  Max Recall:     {{np.max(all_recalls):.4f}}")
print(f"  Max Accuracy:   {{np.max(all_accuracies):.4f}}")
print(f"{{'─'*70}}")
print(f"\\n Per-Class Performance:")
print(f"{{'─'*70}}")
for i, class_name in enumerate(class_names):
    tp = cm[i, i]
    fp = cm[:, i].sum() - tp
    fn = cm[i, :].sum() - tp
    class_iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
    class_accuracy = tp / cm[i, :].sum() if cm[i, :].sum() > 0 else 0

    print(f"\\n  {{class_name}}:")
    print(f"    True Positives:  {{tp:,}}")
    print(f"    False Positives: {{fp:,}}")
    print(f"    False Negatives: {{fn:,}}")
    print(f"    Class IoU:       {{class_iou:.4f}}")
    print(f"    Class Accuracy:  {{class_accuracy:.4f}}")
print(f"\\n{{'='*70}}")
print("\\n Evaluation Complete!")
print(f"{{'─'*70}}")
print("  Saved Files:")
print("     Predictions: results/prediction_*.png (first 10 samples)")
print("     Confusion Matrix: results/confusion_matrix.png")
print(f"{{'='*70}}")
'''

with open('/content/my_project/eval_model.py', 'w') as f:
    f.write(eval_script)

print("Created evaluation script: eval_model.py")

"""#  Sparsity Aware Loss + Strong-Weak

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile STABILIZED_lsa.py
# #!/usr/bin/env python3
# import torch
# import torch.nn.functional as F
# import numpy as np
# from scipy.ndimage import label as cc_label
# 
# def _check_region_size(region_size_list):
#     if len(region_size_list) == 0:
#         return []
#     unique, counts = np.unique(region_size_list, return_counts=True)
#     size_to_count = dict(zip(unique, counts))
#     return [size_to_count[size] for size in region_size_list]
# 
# def _lsa_single_CONSERVATIVE(prob_map, pad=10, threshold=0.5):
#     H, W = prob_map.shape
#     binary = prob_map > threshold
#     labeled_seg, num_classes = cc_label(binary)
# 
#     region_size_list = []
#     for k in range(1, num_classes + 1):
#         coords = np.where(labeled_seg == k)
#         if coords[0].size == 0:
#             continue
#         x, y = coords[0], coords[1]
#         xmin, xmax = x.min(), x.max()
#         ymin, ymax = y.min(), y.max()
# 
#         xmin = max(xmin - pad, 0)
#         ymin = max(ymin - pad, 0)
#         xmax = min(xmax + pad, H - 1)
#         ymax = min(ymax + pad, W - 1)
# 
#         Mk = prob_map[xmin:xmax+1, ymin:ymax+1]
#         region_size = Mk.size
#         if region_size < 5:
#             continue
#         region_size_list.append(region_size)
# 
#     if len(region_size_list) == 0:
#         return 0.0
# 
#     same_region_size_list = _check_region_size(region_size_list)
#     crack_pixels = float(binary.sum())
#     total_pixels = float(H * W)
#     background_pixels = total_pixels - crack_pixels
#     if crack_pixels == 0:
#         return 0.0
# 
#     ir = background_pixels / (crack_pixels + 1e-6)
#     lsa_sum = 0.0
#     for sk, rk in zip(same_region_size_list, region_size_list):
#         term = (sk / float(rk + 1e-6)) ** 2
#         lsa_sum += term
#     lsa_sum /= float(len(region_size_list))
# 
#     LSA = (1.0 / (1.0 + ir)) * lsa_sum
#     LSA *= 0.1
#     return float(np.clip(LSA, 0.0, 0.1))
# 
# def lsa_loss_batch_CONSERVATIVE(pred_prob, pad=10, threshold=0.5):
#     device = pred_prob.device
#     B, C, H, W = pred_prob.shape
#     crack_prob = pred_prob[:, 1] if C > 1 else pred_prob[:, 0]
# 
#     lsa_vals = []
#     for b in range(B):
#         prob_map = crack_prob[b].detach().cpu().numpy()
#         lsa_val = _lsa_single_CONSERVATIVE(prob_map, pad=pad, threshold=threshold)
#         lsa_vals.append(lsa_val)
# 
#     return torch.tensor(lsa_vals, dtype=torch.float32, device=device).mean()
# 
# def _dice_loss_from_logits(pred, target):
#     probs = torch.softmax(pred, dim=1)[:, 1]
#     targets = (target == 1).float()
#     probs = probs.flatten(1)
#     targets = targets.flatten(1)
#     inter = (probs * targets).sum(1)
#     union = probs.sum(1) + targets.sum(1)
#     dice = (2 * inter + 1e-6) / (union + 1e-6)
#     return (1 - dice).mean()
# 
# def compute_hybrid_loss_with_warmup(
#     pred, target, epoch,
#     use_lsa=True,
#     lsa_warmup_epochs=5,
#     lambda_weight=0.3,
#     ignore_index=255,
#     device="cuda"
# ):
#     ce_loss = F.cross_entropy(pred, target, ignore_index=ignore_index)
#     dice_loss_val = _dice_loss_from_logits(pred, target)
# 
#     if (not use_lsa) or (epoch < lsa_warmup_epochs):
#         total_loss = 0.5*ce_loss + 0.5*dice_loss_val
#         return total_loss, 0.0, float(ce_loss), float(dice_loss_val), 0.0
# 
#     warmup_progress = min(1.0, (epoch - lsa_warmup_epochs) / 5.0)
#     effective_lambda = lambda_weight * warmup_progress
# 
#     pred_prob = torch.softmax(pred, dim=1)
#     lsa_loss_val = lsa_loss_batch_CONSERVATIVE(pred_prob)
#     lsa_val = float(lsa_loss_val)
# 
#     total_loss = effective_lambda*(lsa_loss_val + ce_loss) + \
#                  (1-effective_lambda)*(0.5*ce_loss + 0.5*dice_loss_val)
# 
#     return total_loss, lsa_val, float(ce_loss), float(dice_loss_val), effective_lambda
#

from STABILIZED_lsa import compute_hybrid_loss_with_warmup
#!/usr/bin/env python3

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from scipy.ndimage import label as cc_label


# LSA IMPLEMENTATION

def _check_region_size(region_size_list):

    if len(region_size_list) == 0:
        return []
    unique, counts = np.unique(region_size_list, return_counts=True)
    size_to_count = dict(zip(unique, counts))
    return [size_to_count[size] for size in region_size_list]


def _lsa_single_CONSERVATIVE(prob_map: np.ndarray, pad: int = 10, threshold: float = 0.5) -> float:
    #ULTRA-CONSERVATIVE LSA for a single probability map.  prob_map: (H, W) array with values in [0,1] (crack class probability)
    #due to issues in older version with very low results
    H, W = prob_map.shape

    # Binary prediction from probabilities
    binary = prob_map > threshold
    labeled_seg, num_classes = cc_label(binary)

    region_size_list = []

    # Collect region sizes with padding around the bounding box
    for k in range(1, num_classes + 1):
        coords = np.where(labeled_seg == k)
        if coords[0].size == 0:
            continue

        x, y = coords[0], coords[1]
        xmin, xmax = x.min(), x.max()
        ymin, ymax = y.min(), y.max()

        xmin = max(xmin - pad, 0)
        ymin = max(ymin - pad, 0)
        xmax = min(xmax + pad, H - 1)
        ymax = min(ymax + pad, W - 1)

        Mk = prob_map[xmin:xmax + 1, ymin:ymax + 1]
        region_size = Mk.size

        #skip tiny noise regions
        if region_size < 5:
            continue

        region_size_list.append(region_size)

    if len(region_size_list) == 0:
        return 0.0

    same_region_size_list = _check_region_size(region_size_list)

    # Imbalance ratio IR = background / crack
    crack_pixels = float(binary.sum())
    total_pixels = float(H * W)
    background_pixels = total_pixels - crack_pixels

    if crack_pixels == 0:
        #No crack at all
        return 0.0

    ir = background_pixels / (crack_pixels + 1e-6)

    #Sum over regions: (M(Ak) / Ak)^2
    lsa_sum = 0.0
    for sk, rk in zip(same_region_size_list, region_size_list):
        term = (sk / float(rk + 1e-6)) ** 2
        lsa_sum += term

    #Normalize by number of regions
    lsa_sum /= float(len(region_size_list))

    LSA = (1.0 / (1.0 + ir)) * lsa_sum
    LSA *= 1.0   # remove the extra dampening
    LSA = np.clip(LSA, 0.0, 1.0)


    return float(LSA)


def lsa_loss_batch_CONSERVATIVE(
    pred_prob: torch.Tensor,
    pad: int = 10,
    threshold: float = 0.5,
) -> torch.Tensor:
    assert pred_prob.ndim == 4, "pred_prob must be (B, C, H, W)"
    device = pred_prob.device
    B, C, H, W = pred_prob.shape

    # usee foreground/crack class probabilities
    if C > 1:
        crack_prob = pred_prob[:, 1, :, :]
    else:
        crack_prob = pred_prob[:, 0, :, :]

    lsa_vals = []
    for b in range(B):
        prob_map = crack_prob[b].detach().cpu().numpy()
        lsa_val = _lsa_single_CONSERVATIVE(prob_map, pad=pad, threshold=threshold)
        lsa_vals.append(lsa_val)

    lsa_tensor = torch.tensor(lsa_vals, dtype=torch.float32, device=device)
    return lsa_tensor.mean()


# HYBRID LOSS WITH LSA

def _dice_loss_from_logits(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    # Probabilities of class 1 (foreground/crack)
    probs = torch.softmax(pred, dim=1)[:, 1, :, :]
    # Binary target mask for foreground
    targets = (target == 1).float()

    probs = probs.contiguous().view(probs.size(0), -1)
    targets = targets.contiguous().view(targets.size(0), -1)

    intersection = (probs * targets).sum(dim=1)
    union = probs.sum(dim=1) + targets.sum(dim=1)
    dice = (2.0 * intersection + 1e-6) / (union + 1e-6)
    return (1.0 - dice).mean()


def compute_hybrid_loss_with_warmup(
    pred: torch.Tensor,
    target: torch.Tensor,
    epoch: int,
    use_lsa: bool = True,
    lsa_warmup_epochs: int = 5,
    lambda_weight: float = 0.3,
    ignore_index: int = 255,
    device: str | torch.device = "cuda",
):
    # Cross-entropy loss (multi-class with ignore_index)
    ce_loss = F.cross_entropy(pred, target, ignore_index=ignore_index)

    # Dice loss
    dice_loss_val = _dice_loss_from_logits(pred, target)

    if (not use_lsa) or (epoch < lsa_warmup_epochs):
        # Phase 1: baseline only
        total_loss = 0.5 * ce_loss + 0.5 * dice_loss_val
        lsa_val = 0.0
        effective_lambda = 0.0
    else:
        # Phase 2+: gradually introduce LSA
        warmup_progress = min(1.0, (epoch - lsa_warmup_epochs) / 5.0)
        effective_lambda = float(lambda_weight * warmup_progress)

        # LSA loss on probabilities
        pred_prob = torch.softmax(pred, dim=1)
        lsa_loss_val = lsa_loss_batch_CONSERVATIVE(pred_prob, pad=10, threshold=0.5)
        lsa_val = float(lsa_loss_val.item())

        # Combine:
        #start was: 0.5 * CE + 0.5 * DiceLoss
        #end is:   λ(LSA + CE) + (1-λ)(0.5CE + 0.5DiceLoss)
        total_loss = effective_lambda * (lsa_loss_val + ce_loss) + \
                     (1.0 - effective_lambda) * (0.5 * ce_loss + 0.5 * dice_loss_val)

    return total_loss, lsa_val, float(ce_loss.item()), float(dice_loss_val.item()), effective_lambda

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast

from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2
from STABILIZED_lsa import compute_hybrid_loss_with_warmup  #already tested


class AverageMeter:
    def __init__(self):
        self.reset()
    def reset(self):
        self.val = 0.0
        self.avg = 0.0
        self.sum = 0.0
        self.count = 0
    def update(self, val, n=1):
        self.val = float(val)
        self.sum += float(val) * n
        self.count += n
        self.avg = self.sum / self.count if self.count > 0 else 0.0


def compute_batch_metrics_from_logits(pred, target, ignore_index=255):

    with torch.no_grad():
        probs = F.softmax(pred, dim=1)[:, 1]  # crack class
        preds_bin = (probs > 0.5).long()

        #mask out ignore_index
        valid_mask = (target != ignore_index)
        if valid_mask.sum() == 0:
            return 0.0, 0.0, 0.0

        preds_flat = preds_bin[valid_mask]
        target_flat = target[valid_mask]

        tp = ((preds_flat == 1) & (target_flat == 1)).sum().float()
        fp = ((preds_flat == 1) & (target_flat == 0)).sum().float()
        fn = ((preds_flat == 0) & (target_flat == 1)).sum().float()
        tn = ((preds_flat == 0) & (target_flat == 0)).sum().float()

        eps = 1e-6
        dice = (2 * tp + eps) / (2 * tp + fp + fn + eps)
        iou = (tp + eps) / (tp + fp + fn + eps)
        acc = (tp + tn + eps) / (tp + tn + fp + fn + eps)

        return float(dice.item()), float(iou.item()), float(acc.item())


def get_config():
    return {
        # Data / model
        'data_root': "/content/drive/MyDrive/aircraft_voc",
        'num_classes': 2,
        'crop_size': 512,

        # Training
        'epochs': 40,
        'batch_size': 4,
        'lr': 5e-4,
        'min_lr': 1e-6,
        'weight_decay': 0.0,
        'num_workers': 4,
        'mixed_precision': True,
        'ignore_index': 255,

        # Semi-supervised
        'labeled_ratio': 0.7,
        'pseudo_threshold': 0.7,
        'unsup_weight': 1.0,

        # LSA / hybrid loss
        'use_lsa': True,
        'lsa_warmup_epochs': 5,
        'lambda_weight': 0.3,  # conservative

        # Logging
        'print_interval': 10,
    }


# MAIN TRAINING FUNCTION WITH METRIC TRACKING
def main():
    config = get_config()

    print("=" * 80)
    print("SEMI-SUPERVISED TRAINING WITH LSA + STRONG/WEAK AUG + METRICS")
    print("=" * 80)
    print(f"Data root      : {config['data_root']}")
    print(f"Num classes    : {config['num_classes']}")
    print(f"Batch size     : {config['batch_size']}")
    print(f"Epochs         : {config['epochs']}")
    print(f"LR             : {config['lr']}")
    print(f"Use LSA        : {config['use_lsa']}")
    print(f"LSA warmup     : {config['lsa_warmup_epochs']} epochs")
    print(f"Lambda (λ)     : {config['lambda_weight']}")
    print(f"Pseudo thr     : {config['pseudo_threshold']}")
    print("=" * 80)

    # Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n  Using device: {device}")

    # 1. Create semi-supervised splits
    print("\n Creating semi-supervised splits...")
    ok = create_split_files(config['data_root'], labeled_ratio=config['labeled_ratio'])
    if not ok:
        print("create_split_files failed. Check your function.")
        return
    print(" Splits created.")

    # 2. Model
    print("\n Creating UNet++ MobileNetV2 model...")
    model = UNetPlusPlusMobileNetV2(
        num_classes=config['num_classes'],
        pretrained=True
    ).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {total_params:,}")

    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config['lr'],
        weight_decay=config['weight_decay']
    )

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config['epochs'],
        eta_min=config['min_lr']
    )

    # 3. Datasets / Dataloaders
    print(f"\n Loading datasets from {config['data_root']}...")

    # These should be defined in your project already
    trainset_labeled = LabeledDataset(
        config['data_root'],
        'splits/aircraft/labeled.txt',
        size=config['crop_size']
    )

    trainset_unlabeled_weak = WeakAugDataset(
        config['data_root'],
        'splits/aircraft/unlabeled.txt',
        size=config['crop_size']
    )

    trainset_unlabeled_strong = StrongAugDataset(
        config['data_root'],
        'splits/aircraft/unlabeled.txt',
        size=config['crop_size']
    )

    trainloader_labeled = DataLoader(
        trainset_labeled,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        drop_last=True,
        pin_memory=True,
    )

    trainloader_unlabeled_weak = DataLoader(
        trainset_unlabeled_weak,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        drop_last=True,
        pin_memory=True,
    )

    trainloader_unlabeled_strong = DataLoader(
        trainset_unlabeled_strong,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        drop_last=True,
        pin_memory=True,
    )

    print(f"   Labeled samples   : {len(trainset_labeled)}")
    print(f"   Unlabeled samples : {len(trainset_unlabeled_weak)}")

    scaler = GradScaler() if config['mixed_precision'] else None

    # 4. Training loop with metric tracking
    print(f"\n Starting training for {config['epochs']} epochs...")
    print("=" * 80)

    for epoch in range(config['epochs']):
        print(f"\nEpoch [{epoch + 1}/{config['epochs']}]")
        print("-" * 80)

        model.train()

        #Loss meters
        losses_sup = AverageMeter()
        losses_unsup = AverageMeter()
        losses_total = AverageMeter()
        lsa_meter = AverageMeter()
        ce_meter = AverageMeter()
        dice_loss_meter = AverageMeter()

        # Metric meters (on labeled batches)
        dice_score_meter = AverageMeter()
        iou_meter = AverageMeter()
        acc_meter = AverageMeter()

        pseudo_accept = AverageMeter()

        loader_l_iter = iter(trainloader_labeled)
        loader_u_weak_iter = iter(trainloader_unlabeled_weak)
        loader_u_strong_iter = iter(trainloader_unlabeled_strong)

        num_batches = min(
            len(trainloader_labeled),
            len(trainloader_unlabeled_weak),
            len(trainloader_unlabeled_strong)
        )

        for i in range(num_batches):
            # Fetch batches
            try:
                img_l, mask_l = next(loader_l_iter)
            except StopIteration:
                loader_l_iter = iter(trainloader_labeled)
                img_l, mask_l = next(loader_l_iter)

            try:
                img_u_weak, _ = next(loader_u_weak_iter)
            except StopIteration:
                loader_u_weak_iter = iter(trainloader_unlabeled_weak)
                img_u_weak, _ = next(loader_u_weak_iter)

            try:
                img_u_strong, _ = next(loader_u_strong_iter)
            except StopIteration:
                loader_u_strong_iter = iter(trainloader_unlabeled_strong)
                img_u_strong, _ = next(loader_u_strong_iter)

            img_l, mask_l = img_l.to(device), mask_l.to(device)
            img_u_weak = img_u_weak.to(device)
            img_u_strong = img_u_strong.to(device)

            optimizer.zero_grad(set_to_none=True)
            bs = img_l.size(0)

            # Forward + loss
            if scaler is not None:
                with autocast():
                    #supervised loss (LSA + CE + Dice with warmup)
                    pred_l = model(img_l)
                    loss_sup, lsa_val, ce_val, dice_loss_val, lam_eff = compute_hybrid_loss_with_warmup(
                        pred_l,
                        mask_l,
                        epoch=epoch,
                        use_lsa=config['use_lsa'],
                        lsa_warmup_epochs=config['lsa_warmup_epochs'],
                        lambda_weight=config['lambda_weight'],
                        ignore_index=config['ignore_index'],
                        device=device,
                    )

                    #pseudo-labels from weak aug
                    with torch.no_grad():
                        pred_u_weak = model(img_u_weak)
                        prob_u_weak = F.softmax(pred_u_weak, dim=1)
                        conf_u, pseudo_u = torch.max(prob_u_weak, dim=1)
                        mask_conf = (conf_u >= config['pseudo_threshold']).float()

                    #consistency on strong aug
                    pred_u_strong = model(img_u_strong)
                    criterion_u = nn.CrossEntropyLoss(
                        ignore_index=config['ignore_index'],
                        reduction='none'
                    )
                    loss_unsup_map = criterion_u(pred_u_strong, pseudo_u)
                    loss_unsup = (loss_unsup_map * mask_conf).sum() / (mask_conf.sum() + 1e-8)

                    total_loss = loss_sup + config['unsup_weight'] * loss_unsup

                scaler.scale(total_loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                pred_l = model(img_l)
                loss_sup, lsa_val, ce_val, dice_loss_val, lam_eff = compute_hybrid_loss_with_warmup(
                    pred_l,
                    mask_l,
                    epoch=epoch,
                    use_lsa=config['use_lsa'],
                    lsa_warmup_epochs=config['lsa_warmup_epochs'],
                    lambda_weight=config['lambda_weight'],
                    ignore_index=config['ignore_index'],
                    device=device,
                )

                with torch.no_grad():
                    pred_u_weak = model(img_u_weak)
                    prob_u_weak = F.softmax(pred_u_weak, dim=1)
                    conf_u, pseudo_u = torch.max(prob_u_weak, dim=1)
                    mask_conf = (conf_u >= config['pseudo_threshold']).float()

                pred_u_strong = model(img_u_strong)
                criterion_u = nn.CrossEntropyLoss(
                    ignore_index=config['ignore_index'],
                    reduction='none'
                )
                loss_unsup_map = criterion_u(pred_u_strong, pseudo_u)
                loss_unsup = (loss_unsup_map * mask_conf).sum() / (mask_conf.sum() + 1e-8)

                total_loss = loss_sup + config['unsup_weight'] * loss_unsup

                total_loss.backward()
                optimizer.step()

            # Update meterics
            losses_sup.update(loss_sup.item(), bs)
            losses_unsup.update(loss_unsup.item(), bs)
            losses_total.update(total_loss.item(), bs)
            lsa_meter.update(lsa_val, bs)
            ce_meter.update(ce_val, bs)
            dice_loss_meter.update(dice_loss_val, bs)
            pseudo_accept.update(mask_conf.mean().item(), bs)

            #training metrics (on labeled data)
            dice_score, iou, acc = compute_batch_metrics_from_logits(
                pred_l, mask_l, ignore_index=config['ignore_index']
            )
            dice_score_meter.update(dice_score, bs)
            iou_meter.update(iou, bs)
            acc_meter.update(acc, bs)

            # Logs
            if (i + 1) % config['print_interval'] == 0:
                print(f"  Iter [{i + 1}/{num_batches}] "
                      f"Total: {losses_total.avg:.4f} | "
                      f"Sup: {losses_sup.avg:.4f} | Unsup: {losses_unsup.avg:.4f}")
                print(f"    LSA={lsa_meter.avg:.5f}, CE={ce_meter.avg:.4f}, "
                      f"DiceLoss={dice_loss_meter.avg:.4f}, λ_eff={lam_eff:.3f}")
                print(f"    Train Dice={dice_score_meter.avg:.4f}, "
                      f"IoU={iou_meter.avg:.4f}, "
                      f"Acc={acc_meter.avg:.4f}")
                print(f"    Pseudo accept = {pseudo_accept.avg * 100:.1f}%")

        # Step LR scheduler
        scheduler.step()
        print(f"\n  Epoch {epoch + 1} done.")
        print(f"    Train Dice={dice_score_meter.avg:.4f}, "
              f"IoU={iou_meter.avg:.4f}, "
              f"Acc={acc_meter.avg:.4f}")
        print(f"    Avg LSA={lsa_meter.avg:.5f}, "
              f"Avg CE={ce_meter.avg:.4f}, "
              f"Avg DiceLoss={dice_loss_meter.avg:.4f}")
        print(f"    LR now = {optimizer.param_groups[0]['lr']:.6e}")
        print("=" * 80)

    print("\n Training finished.")


if __name__ == "__main__":
    main()

!python eval_model.py