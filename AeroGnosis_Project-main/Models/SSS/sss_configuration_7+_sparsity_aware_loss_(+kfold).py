# -*- coding: utf-8 -*-
"""sss configuration 7+ sparsity aware loss (+kfold)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Tw_iZH3XN-vjvMYmL5D3goBeWqfUyps

# 1. Setting up the Enivronment
"""

#mount drive
from google.colab import drive
drive.mount('/content/drive')

#install dependencies
!pip install -q torch torchvision
!pip install -q segmentation-models-pytorch
!pip install -q tensorboard
!pip install -q pyyaml
!pip install -q opencv-python
!pip install -q matplotlib

"""# 2. Dataset Configuration and Verification"""

DATA_ROOT = "/content/drive/MyDrive/aircraft_voc"

#verifying dataset structure
import os
from pathlib import Path

def verify_dataset_structure(data_root):
    required_dirs = [
        'JPEGImages',
        'SegmentationClass',
        'ImageSets/Segmentation'
    ]

    all_exist = True

    for dir_path in required_dirs:
        full_path = os.path.join(data_root, dir_path)
        exists = os.path.exists(full_path)

        if exists:
            # Count files
            if os.path.isdir(full_path):
                num_files = len([f for f in os.listdir(full_path) if os.path.isfile(os.path.join(full_path, f))])
                print(f"  {dir_path}: {num_files} files")
        else:
            print(f" {dir_path} is NOT FOUND")
            all_exist = False

    return all_exist

dataset_valid = verify_dataset_structure(DATA_ROOT)

if not dataset_valid:
    print("\n Something wrong")
else:
    print("\n Dataset all good")

"""#3. Create UNet++ with MobileNetV2"""

#create junior UNet++ model
unetpp_model_code = '''import torch
import torch.nn as nn
import torch.nn.functional as F
import segmentation_models_pytorch as smp

class UNetPlusPlus(nn.Module):
    #UNet++ model with MobileNetV2 encoder for semantic segmentation

    def __init__(self, num_classes, encoder_name='mobilenet_v2', pretrained=True):
        super(UNetPlusPlus, self).__init__()

        self.num_classes = num_classes
        self.encoder_name = encoder_name

        #creation of UNet++ model with MobileNetV2 encoder
        self.backbone = smp.UnetPlusPlus(
            encoder_name=encoder_name,
            encoder_weights='imagenet' if pretrained else None,
            in_channels=3,
            classes=num_classes,
            activation=None,  #no activation, return logits
        )

    def forward(self, x, need_fp=False):
           # x: input tensor [B, 3, H, W]
           # need_fp: whether to return feature maps (for consistency regularization)
        #Returns:
            #logits [B, num_classes, H, W] or (logits, features) if need_fp
        # Get output from UNet++
        out = self.backbone(x)

        if need_fp:
            # Use the output as feature representation for consistency loss
            return out, out

        return out

    def decode(self, features):
        return features

def UNetPlusPlusMobileNetV2(num_classes, pretrained=True):
    return UNetPlusPlus(num_classes=num_classes, encoder_name='mobilenet_v2', pretrained=pretrained)
'''

#writing UNet++ model to file
os.makedirs('/content/my_project/model/semseg', exist_ok=True)
with open('/content/my_project/model/semseg/unetplusplus.py', 'w') as f:
    f.write(unetpp_model_code)

print("Created UNet++ model file: model/semseg/unetplusplus.py")
print("  Encoder: MobileNetV2 (lightweight and efficient)")
print("  Decoder: UNet++ nested skip connections")

"""# 6. Evaluation"""

eval_script = f'''#!/usr/bin/env python3
"""
Evaluation script for trained UNet++ model
"""

import os
import sys
import torch
import numpy as np
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as T
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

sys.path.insert(0, '/content/my_project')
from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2

DATA_ROOT = "{DATA_ROOT}"
NUM_CLASSES = 2
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class EvalDataset(Dataset):
    def __init__(self, data_root, split_file, transform=None, size=512):
        self.data_root = data_root
        self.transform = transform
        self.size = size

        with open(split_file, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]

        # Load image
        img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.jpg")
        if not os.path.exists(img_path):
            img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.png")
        img = Image.open(img_path).convert('RGB')

        #Load mask
        mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.png")
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.jpg")
        mask = Image.open(mask_path)

        # Store original as numpy array (not PIL Image)
        img_orig = np.array(img.copy())

        #Resize
        img = img.resize((self.size, self.size), Image.BILINEAR)
        mask = mask.resize((self.size, self.size), Image.NEAREST)

        if self.transform:
            img = self.transform(img)

        mask = torch.from_numpy(np.array(mask)).long()

        return img, mask, img_orig, img_id

def compute_iou(pred, target, num_classes, ignore_index=255):
    """Compute mean IoU"""
    ious = []
    pred = pred.flatten()
    target = target.flatten()

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        # Ignore pixels marked as ignore_index
        valid = (target != ignore_index)
        pred_cls = pred_cls & valid
        target_cls = target_cls & valid

        intersection = (pred_cls & target_cls).sum()
        union = (pred_cls | target_cls).sum()

        if union == 0:
            continue

        iou = intersection.float() / union.float()
        ious.append(iou.item())

    return np.mean(ious) if ious else 0.0

def compute_all_metrics(pred, target, num_classes, ignore_index=255):
    #Compute IoU, Dice, Precision, Recall, Accuracy
    pred = pred.flatten()
    target = target.flatten()

    # Filter out ignore index
    valid_mask = (target != ignore_index)
    pred = pred[valid_mask]
    target = target[valid_mask]

    ious = []
    dices = []
    precisions = []
    recalls = []

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        #True Positives, False Positives, False Negatives
        tp = (pred_cls & target_cls).sum().float()
        fp = (pred_cls & ~target_cls).sum().float()
        fn = (~pred_cls & target_cls).sum().float()

        # IoU
        union = tp + fp + fn
        iou = tp / (union + 1e-8)
        ious.append(iou.item())

        #Dice (F1)
        dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)
        dices.append(dice.item())

        #Precision
        precision = tp / (tp + fp + 1e-8)
        precisions.append(precision.item())

        #Recall
        recall = tp / (tp + fn + 1e-8)
        recalls.append(recall.item())

    # Overall Accuracy
    accuracy = (pred == target).float().mean().item()

    return {{
        'iou': ious,
        'dice': dices,
        'precision': precisions,
        'recall': recalls,
        'accuracy': accuracy,
        'mean_iou': np.mean(ious),
        'mean_dice': np.mean(dices),
        'mean_precision': np.mean(precisions),
        'mean_recall': np.mean(recalls)
    }}

#Load model
print("Loading trained model...")
model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=False)

# Find the best model file (with dice score in filename)
import glob
checkpoint_dir = 'exp/aircraft_unetpp'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'best_model_dice=*.pth'))

if checkpoint_files:
    # Get the file with highest dice score
    checkpoint_path = max(checkpoint_files, key=lambda x: float(x.split('dice=')[1].split('.pth')[0]))
    print(f"Found best checkpoint: {{os.path.basename(checkpoint_path)}}")
else:
    #Fallback to generic name if it exists
    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')
    if not os.path.exists(checkpoint_path):
        print("Error: No checkpoint found!")
        print(f"Looking in: {{checkpoint_dir}}")
        import sys
        sys.exit(1)

checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(DEVICE)
model.eval()

print(f"Model loaded from epoch {{checkpoint['epoch']}}")
if 'dice' in checkpoint:
    print(f"Best Dice score: {{checkpoint['dice']:.4f}}")

#Load validation dataset
val_transform = T.Compose([
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_file = os.path.join(DATA_ROOT, 'ImageSets/Segmentation/val.txt')
val_dataset = EvalDataset(DATA_ROOT, val_file, val_transform, size=512)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)

print(f"\\nEvaluating on {{len(val_dataset)}} validation images...")

# Evaluate
os.makedirs('results', exist_ok=True)
all_ious = []
all_dices = []
all_precisions = []
all_recalls = []
all_accuracies = []
all_losses = []
all_preds = []
all_targets = []

#Loss function for evaluation
criterion = torch.nn.CrossEntropyLoss(ignore_index=255)

with torch.no_grad():
    for idx, (image, mask, img_orig, img_id) in enumerate(tqdm(val_loader)):
        image = image.to(DEVICE)
        mask = mask.to(DEVICE)

        output = model(image)

        #Compute loss
        loss = criterion(output, mask)
        all_losses.append(loss.item())

        pred = output.argmax(dim=1).cpu().numpy()[0]
        mask_np = mask.cpu().numpy()[0]

        # Store for confusion matrix
        valid_mask = (mask_np != 255)
        all_preds.extend(pred[valid_mask].flatten())
        all_targets.extend(mask_np[valid_mask].flatten())

        #compute all metrics
        metrics = compute_all_metrics(torch.from_numpy(pred), torch.from_numpy(mask_np), NUM_CLASSES)
        all_ious.append(metrics['mean_iou'])
        all_dices.append(metrics['mean_dice'])
        all_precisions.append(metrics['mean_precision'])
        all_recalls.append(metrics['mean_recall'])
        all_accuracies.append(metrics['accuracy'])

        #Save visualizations for first 10 images
        if idx < 10:
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            # Original image
            axes[0].imshow(img_orig[0])
            axes[0].set_title('Input Image')
            axes[0].axis('off')

            axes[1].imshow(mask_np, cmap='tab20', vmin=0, vmax=NUM_CLASSES-1)
            axes[1].set_title('Ground Truth')
            axes[1].axis('off')

            axes[2].imshow(pred, cmap='tab20', vmin=0, vmax=NUM_CLASSES-1)
            axes[2].set_title(f'Prediction\\nIoU: {{metrics["mean_iou"]:.3f}} | Dice: {{metrics["mean_dice"]:.3f}}')
            axes[2].axis('off')

            plt.tight_layout()
            plt.savefig(f'results/prediction_{{idx}}.png', dpi=150, bbox_inches='tight')
            plt.close()

mean_iou = np.mean(all_ious)
mean_dice = np.mean(all_dices)
mean_precision = np.mean(all_precisions)
mean_recall = np.mean(all_recalls)
mean_accuracy = np.mean(all_accuracies)
mean_loss = np.mean(all_losses)

#Create confusion matrix
cm = confusion_matrix(all_targets, all_preds, labels=list(range(NUM_CLASSES)))

#Normalize confusion matrix for percentages
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

#Plot confusion matrix with both counts and percentages
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
class_names = ['Background', 'Aircraft'] if NUM_CLASSES == 2 else [f'Class {{i}}' for i in range(NUM_CLASSES)]

#Confusion Matrix Raw Counts
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
            xticklabels=class_names,
            yticklabels=class_names,
            cbar_kws={{'label': 'Count'}},
            annot_kws={{'size': 14, 'weight': 'bold'}})
ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold', pad=15)
ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax1.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

# Confusion Matrix Percentages
sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens', ax=ax2,
            xticklabels=class_names,
            yticklabels=class_names,
            cbar_kws={{'label': 'Percentage (%)'}},
            annot_kws={{'size': 14, 'weight': 'bold'}})
ax2.set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold', pad=15)
ax2.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax2.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

plt.suptitle(f'Model Performance: IoU={{mean_iou:.4f}} | Dice={{mean_dice:.4f}} | Accuracy={{mean_accuracy:.4f}}',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('results/confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.close()

print(f"\\n{{'='*70}}")
print(f"EVALUATION RESULTS")
print(f"{{'='*70}}")
print(f"\\n  Overall Metrics:")
print(f"{{'─'*70}}")
print(f"  Loss:           {{mean_loss:.4f}} (±{{np.std(all_losses):.4f}})")
print(f"  Accuracy:       {{mean_accuracy:.4f}} (±{{np.std(all_accuracies):.4f}})")
print(f"  Mean IoU:       {{mean_iou:.4f}} (±{{np.std(all_ious):.4f}})")
print(f"  Mean Dice:      {{mean_dice:.4f}} (±{{np.std(all_dices):.4f}})")
print(f"  Mean Precision: {{mean_precision:.4f}} (±{{np.std(all_precisions):.4f}})")
print(f"  Mean Recall:    {{mean_recall:.4f}} (±{{np.std(all_recalls):.4f}})")
print(f"{{'─'*70}}")
print(f"\\n Best Performance:")
print(f"{{'─'*70}}")
print(f"  Max IoU:        {{np.max(all_ious):.4f}}")
print(f"  Max Dice:       {{np.max(all_dices):.4f}}")
print(f"  Max Precision:  {{np.max(all_precisions):.4f}}")
print(f"  Max Recall:     {{np.max(all_recalls):.4f}}")
print(f"  Max Accuracy:   {{np.max(all_accuracies):.4f}}")
print(f"{{'─'*70}}")
print(f"\\n Per-Class Performance:")
print(f"{{'─'*70}}")
for i, class_name in enumerate(class_names):
    tp = cm[i, i]
    fp = cm[:, i].sum() - tp
    fn = cm[i, :].sum() - tp
    class_iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
    class_accuracy = tp / cm[i, :].sum() if cm[i, :].sum() > 0 else 0

    print(f"\\n  {{class_name}}:")
    print(f"    True Positives:  {{tp:,}}")
    print(f"    False Positives: {{fp:,}}")
    print(f"    False Negatives: {{fn:,}}")
    print(f"    Class IoU:       {{class_iou:.4f}}")
    print(f"    Class Accuracy:  {{class_accuracy:.4f}}")
print(f"\\n{{'='*70}}")
print("\\n Evaluation Complete!")
print(f"{{'─'*70}}")
print("  Saved Files:")
print("     Predictions: results/prediction_*.png (first 10 samples)")
print("     Confusion Matrix: results/confusion_matrix.png")
print(f"{{'='*70}}")
'''

with open('/content/my_project/eval_model.py', 'w') as f:
    f.write(eval_script)

print("Created evaluation script: eval_model.py")

"""# Training with Sparsity Aware Loss + kfold



"""

#!/usr/bin/env python3
import argparse
import os
import random
import numpy as np
import torch
from torch import nn
from torch.optim import SGD, Adam
from torch.utils.data import DataLoader, Dataset, Subset
from PIL import Image
import torchvision.transforms as T
import albumentations as A
from albumentations.pytorch import ToTensorV2
import sys
from scipy import ndimage
from sklearn.model_selection import KFold
sys.path.insert(0, '/content/my_project')
from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
import json
from collections import defaultdict

DATA_ROOT = "/content/drive/MyDrive/aircraft_voc"
NUM_CLASSES = 2

class SimpleLogger:
    def __init__(self, name):
        self.name = name

    def info(self, msg):
        print(f"[INFO] {msg}")

class AverageMeter:
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def dice_loss(preds, targets, smooth=1e-6):
    probs = torch.softmax(preds, dim=1)[:, 1, :, :]  # class 1 (crack)
    targets = (targets == 1).float()

    probs = probs.contiguous().view(-1)
    targets = targets.contiguous().view(-1)

    intersection = (probs * targets).sum()
    dice = (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)

    return 1 - dice

def compute_sparsity_aware_loss(pred_prob, device='cuda', return_components=False, threshold=0.5):

    batch_size = pred_prob.shape[0]
    total_lsa = 0.0
    total_regions = 0
    total_region_size = 0
    total_region_count = 0

    for b in range(batch_size):
        prob_map = pred_prob[b, 1, :, :].detach().cpu().numpy()

        binary_mask = (prob_map > threshold).astype(np.uint8)

        crack_pixels = binary_mask.sum()
        total_pixels = binary_mask.size
        background_pixels = total_pixels - crack_pixels

        if crack_pixels == 0:
            continue

        ir = background_pixels / crack_pixels

        structure = np.ones((3, 3), dtype=np.uint8)
        labeled_array, num_regions = ndimage.label(binary_mask, structure=structure)

        if num_regions == 0:
            continue

        total_regions += num_regions

        region_sizes = []
        region_size_counts = {}

        for region_id in range(1, num_regions + 1):
            y_coords, x_coords = np.where(labeled_array == region_id)

            if len(x_coords) == 0:
                continue

            xmin = max(x_coords.min() - 10, 0)
            xmax = min(x_coords.max() + 10, prob_map.shape[1] - 1)
            ymin = max(y_coords.min() - 10, 0)
            ymax = min(y_coords.max() + 10, prob_map.shape[0] - 1)

            region = prob_map[ymin:ymax+1, xmin:xmax+1]
            region_size = region.size

            region_sizes.append(region_size)
            total_region_size += region_size
            total_region_count += 1

            if region_size in region_size_counts:
                region_size_counts[region_size] += 1
            else:
                region_size_counts[region_size] = 1

        lsa_sum = 0.0
        for region_size in region_sizes:
            m_ai = region_size_counts[region_size]  # M(Ai)
            ai = region_size  # Ai

            lsa_sum += (m_ai / ai) ** 2

        lsa = (1.0 / (1.0 + ir)) * lsa_sum
        total_lsa += lsa

    if total_regions > 0:
        avg_lsa = total_lsa / batch_size
        avg_regions = total_regions / batch_size
        avg_region_size = total_region_size / max(total_region_count, 1)
    else:
        avg_lsa = 0.001
        avg_regions = 0
        avg_region_size = 0

    if return_components:
        return torch.tensor(avg_lsa, dtype=torch.float32, device=device), avg_regions, avg_region_size

    return torch.tensor(avg_lsa, dtype=torch.float32, device=device)

def compute_lsssa_loss(pred, target, lambda_weight=0.4, device='cuda', lsa_threshold=0.5):
    #LSSSA = λ(LSA + LBCE) + (1-λ)LDice

    # Compute supervised losses
    criterion_bce = nn.CrossEntropyLoss(ignore_index=255)

    bce_loss = criterion_bce(pred, target)
    dice_loss_val = dice_loss(pred, target)

    # Compute probability map for LSA
    pred_prob = F.softmax(pred, dim=1)

    # FIXED: Compute unsupervised Sparsity-Aware loss with proper threshold
    lsa_loss, num_regions, avg_size = compute_sparsity_aware_loss(
        pred_prob, device=device, return_components=True, threshold=lsa_threshold
    )

    # Combine according to equation (5): LSSSA = λ(LSA + LBCE) + (1-λ)LDice
    lsssa_loss = lambda_weight * (lsa_loss + bce_loss) + (1 - lambda_weight) * dice_loss_val

    return lsssa_loss, lsa_loss.item(), bce_loss.item(), dice_loss_val.item(), num_regions

def compute_metrics(pred, target, num_classes=2, ignore_index=255):
    """Compute IoU, Dice, Accuracy, Precision, Recall"""
    pred = pred.flatten()
    target = target.flatten()

    # Filter out ignore index
    valid_mask = (target != ignore_index)
    pred = pred[valid_mask]
    target = target[valid_mask]

    metrics = {}

    # Per-class metrics
    ious = []
    dices = []
    precisions = []
    recalls = []

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        # True Positives, False Positives, False Negatives
        tp = (pred_cls & target_cls).sum().float()
        fp = (pred_cls & ~target_cls).sum().float()
        fn = (~pred_cls & target_cls).sum().float()

        # IoU
        union = tp + fp + fn
        iou = tp / (union + 1e-8)
        ious.append(iou.item())

        # Dice (F1)
        dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)
        dices.append(dice.item())

        # Precision
        precision = tp / (tp + fp + 1e-8)
        precisions.append(precision.item())

        # Recall
        recall = tp / (tp + fn + 1e-8)
        recalls.append(recall.item())

    # Accuracy
    accuracy = (pred == target).float().mean().item()

    metrics['iou'] = np.mean(ious)
    metrics['dice'] = np.mean(dices)
    metrics['accuracy'] = accuracy
    metrics['precision'] = np.mean(precisions)
    metrics['recall'] = np.mean(recalls)

    return metrics

def validate(model, dataloader, device, num_classes, lambda_weight=0.4, lsa_threshold=0.5):
    """Validation function with all metrics"""
    model.eval()
    all_metrics = {'iou': [], 'dice': [], 'accuracy': [], 'precision': [], 'recall': []}
    all_losses = []
    all_lsa_values = []
    all_regions = []

    with torch.no_grad():
        for images, masks in dataloader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)

            # Compute LSSSA loss
            loss, lsa_val, _, _, num_regions = compute_lsssa_loss(
                outputs, masks, lambda_weight, device, lsa_threshold
            )
            all_losses.append(loss.item())
            all_lsa_values.append(lsa_val)
            all_regions.append(num_regions)

            preds = outputs.argmax(dim=1)

            # Compute metrics for this batch
            batch_metrics = compute_metrics(preds, masks, num_classes)
            for k in all_metrics:
                all_metrics[k].append(batch_metrics[k])

    # Average all metrics
    avg_metrics = {k: np.mean(v) for k, v in all_metrics.items()}
    avg_metrics['loss'] = np.mean(all_losses)
    avg_metrics['lsa'] = np.mean(all_lsa_values)
    avg_metrics['regions'] = np.mean(all_regions)

    return avg_metrics

def parse_args():
    parser = argparse.ArgumentParser(description='Semi-Supervised Semantic Segmentation with LSSA + K-Fold')
    parser.add_argument('--config', type=str, default='configs/aircraft.yaml')
    parser.add_argument('--labeled-id-path', type=str, default='splits/aircraft/labeled.txt')
    parser.add_argument('--unlabeled-id-path', type=str, default='splits/aircraft/unlabeled.txt')
    parser.add_argument('--save-path', type=str, default='exp/aircraft_unetpp_lssa_kfold')
    parser.add_argument('--n-folds', type=int, default=5, help='Number of folds for cross-validation')
    parser.add_argument('--local_rank', type=int, default=0)
    parser.add_argument('--port', default=None, type=int)
    args = parser.parse_args([])
    return args

def create_kfold_splits(data_root, n_folds=5, labeled_ratio=0.7, seed=42):
    """Create k-fold splits for cross-validation"""
    import random

    random.seed(seed)
    np.random.seed(seed)

    train_file = os.path.join(data_root, 'ImageSets/Segmentation/train.txt')
    if not os.path.exists(train_file):
        train_file = os.path.join(data_root, 'ImageSets/Segmentation/trainval.txt')
        if not os.path.exists(train_file):
            print(f"Error: Neither train.txt nor trainval.txt found")
            return None

    with open(train_file, 'r') as f:
        all_ids = [line.strip() for line in f.readlines()]

    if len(all_ids) == 0:
        print("Error: No image IDs found in train file!")
        return None

    # Shuffle the data
    random.shuffle(all_ids)

    # Initialize k-fold
    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)

    # Create directory for splits
    os.makedirs('splits/aircraft/kfold', exist_ok=True)

    fold_info = []

    for fold, (train_idx, val_idx) in enumerate(kfold.split(all_ids)):
        train_ids = [all_ids[i] for i in train_idx]
        val_ids = [all_ids[i] for i in val_idx]

        # Split training data into labeled and unlabeled
        num_labeled = max(1, int(len(train_ids) * labeled_ratio))
        labeled_ids = train_ids[:num_labeled]
        unlabeled_ids = train_ids[num_labeled:]

        # Save splits
        fold_dir = f'splits/aircraft/kfold/fold_{fold}'
        os.makedirs(fold_dir, exist_ok=True)

        with open(f'{fold_dir}/labeled.txt', 'w') as f:
            f.write('\n'.join(labeled_ids))

        with open(f'{fold_dir}/unlabeled.txt', 'w') as f:
            f.write('\n'.join(unlabeled_ids))

        with open(f'{fold_dir}/val.txt', 'w') as f:
            f.write('\n'.join(val_ids))

        fold_info.append({
            'fold': fold,
            'labeled': len(labeled_ids),
            'unlabeled': len(unlabeled_ids),
            'val': len(val_ids),
            'total_train': len(train_ids)
        })

        print(f"  Fold {fold}: {len(labeled_ids)} labeled, {len(unlabeled_ids)} unlabeled, {len(val_ids)} val")

    # Save fold information
    with open('splits/aircraft/kfold/fold_info.json', 'w') as f:
        json.dump(fold_info, f, indent=2)

    print(f"\n Created {n_folds}-fold cross-validation splits")
    print(f"  Labeled ratio: {labeled_ratio*100:.1f}%")

    return fold_info

def get_default_config():
    """Default configuration for training with LSSA"""
    return {
        'dataset': 'aircraft',
        'data_root': DATA_ROOT,
        'batch_size': 4,
        'lr': 0.0005,
        'min_lr': 0.000001,
        'epochs': 40,
        'crop_size': 512,
        'backbone': 'unetplusplus',
        'num_classes': NUM_CLASSES,
        # Semi-supervised settings
        'labeled_ratio': 0.7,
        'unsup_weight': 1.0,
        'pseudo_threshold': 0.95,
        'use_strong_aug': True,
        # LSSA-specific settings
        'lambda_weight': 0.4,
        'lsa_threshold': 0.5,
        # Optimizer
        'optimizer': 'adam',
        'momentum': 0.9,
        'weight_decay': 0,
        'lr_decay_type': 'cos',
        # Training settings
        'num_workers': 4,
        'mixed_precision': True,
        'sync_bn': True,
        # Early stopping
        'early_stopping': True,
        'patience': 10,
        'monitor': 'val_dice',
        # Logging
        'print_interval': 10,
        'eval_interval': 1,
        'save_period': 20,
        # K-fold settings
        'n_folds': 5,
        'seed': 42,
    }

class SimpleDataset(Dataset):
    """Dataset with  augmentation"""
    def __init__(self, data_root, split_file, phase='train', size=512):
        self.data_root = data_root
        self.phase = phase
        self.size = size

        with open(split_file, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]

        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]

        if phase == 'train':
            self.transform = A.Compose([
                A.RandomScale(scale_limit=0.5, p=0.5),
                A.Rotate(limit=10, p=0.5),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.Resize(size, size),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])
        else:
            self.transform = A.Compose([
                A.Resize(size, size),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]

        img_path = os.path.join(self.data_root, 'JPEGImages', f"{img_id}.jpg")
        if not os.path.exists(img_path):
            img_path = os.path.join(self.data_root, 'JPEGImages', f"{img_id}.png")
        img = Image.open(img_path).convert('RGB')

        mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{img_id}.png")
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{img_id}.jpg")
        mask = Image.open(mask_path)

        img = img.resize((self.size, self.size), Image.BILINEAR)
        mask = mask.resize((self.size, self.size), Image.NEAREST)

        img = np.array(img)
        mask = np.array(mask)

        augmented = self.transform(image=img, mask=mask)
        img = augmented['image']
        mask = augmented['mask'].long()

        return img, mask

def train_one_fold(fold, cfg, args, device):
    """Train model for one fold"""
    print(f"\n{'='*80}")
    print(f"TRAINING FOLD {fold + 1}/{cfg['n_folds']}")
    print(f"{'='*80}\n")

    # Set seed for reproducibility
    random.seed(cfg['seed'] + fold)
    np.random.seed(cfg['seed'] + fold)
    torch.manual_seed(cfg['seed'] + fold)
    torch.cuda.manual_seed_all(cfg['seed'] + fold)

    # Create model
    model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=True)
    model = model.to(device)

    # Optimizer
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=cfg['lr'],
        weight_decay=cfg['weight_decay']
    )

    # Scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg['epochs'],
        eta_min=cfg['min_lr']
    )

    # Loss
    criterion_u = nn.CrossEntropyLoss(ignore_index=255, reduction='none').to(device)

    # Mixed precision
    scaler = GradScaler() if cfg['mixed_precision'] else None

    # Load datasets for this fold
    fold_dir = f'splits/aircraft/kfold/fold_{fold}'
    trainset_l = SimpleDataset(DATA_ROOT, f'{fold_dir}/labeled.txt', phase='train', size=cfg['crop_size'])
    trainset_u = SimpleDataset(DATA_ROOT, f'{fold_dir}/unlabeled.txt', phase='train', size=cfg['crop_size'])
    valset = SimpleDataset(DATA_ROOT, f'{fold_dir}/val.txt', phase='val', size=cfg['crop_size'])

    trainloader_l = DataLoader(trainset_l, batch_size=cfg['batch_size'],
                               shuffle=True, num_workers=cfg['num_workers'], drop_last=True, pin_memory=True)
    trainloader_u = DataLoader(trainset_u, batch_size=cfg['batch_size'],
                               shuffle=True, num_workers=cfg['num_workers'], drop_last=True, pin_memory=True)
    valloader = DataLoader(valset, batch_size=cfg['batch_size'],
                          shuffle=False, num_workers=cfg['num_workers'], pin_memory=True)

    print(f"Fold {fold} datasets:")
    print(f"  Labeled: {len(trainset_l)}, Unlabeled: {len(trainset_u)}, Val: {len(valset)}")

    best_dice = 0.0
    patience_counter = 0
    fold_history = []

    for epoch in range(cfg['epochs']):
        model.train()
        losses_l = AverageMeter()
        losses_u = AverageMeter()
        losses_total = AverageMeter()
        lsa_meter = AverageMeter()
        bce_meter = AverageMeter()
        dice_meter = AverageMeter()
        regions_meter = AverageMeter()

        loader_l_iter = iter(trainloader_l)
        loader_u_iter = iter(trainloader_u)

        num_batches = min(len(trainloader_l), len(trainloader_u))

        for i in range(num_batches):
            try:
                img_l, mask_l = next(loader_l_iter)
            except StopIteration:
                loader_l_iter = iter(trainloader_l)
                img_l, mask_l = next(loader_l_iter)

            try:
                img_u, _ = next(loader_u_iter)
            except StopIteration:
                loader_u_iter = iter(trainloader_u)
                img_u, _ = next(loader_u_iter)

            img_l, mask_l = img_l.to(device), mask_l.to(device)
            img_u = img_u.to(device)

            optimizer.zero_grad()

            if cfg['mixed_precision']:
                with autocast():
                    pred_l = model(img_l)
                    loss_l, lsa_val, bce_val, dice_val, num_regions = compute_lsssa_loss(
                        pred_l, mask_l, cfg['lambda_weight'], device, cfg['lsa_threshold']
                    )

                    with torch.no_grad():
                        pred_u = model(img_u)
                        prob_u = F.softmax(pred_u, dim=1)
                        conf_u, pseudo_u = torch.max(prob_u, dim=1)
                        mask_u = (conf_u >= cfg['pseudo_threshold']).float()

                    pred_u_strong = model(img_u)
                    loss_u = criterion_u(pred_u_strong, pseudo_u)
                    loss_u = (loss_u * mask_u).sum() / (mask_u.sum() + 1e-8)

                    loss = loss_l + cfg['unsup_weight'] * loss_u

                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                pred_l = model(img_l)
                loss_l, lsa_val, bce_val, dice_val, num_regions = compute_lsssa_loss(
                    pred_l, mask_l, cfg['lambda_weight'], device, cfg['lsa_threshold']
                )

                with torch.no_grad():
                    pred_u = model(img_u)
                    prob_u = F.softmax(pred_u, dim=1)
                    conf_u, pseudo_u = torch.max(prob_u, dim=1)
                    mask_u = (conf_u >= cfg['pseudo_threshold']).float()

                pred_u_strong = model(img_u)
                loss_u = criterion_u(pred_u_strong, pseudo_u)
                loss_u = (loss_u * mask_u).sum() / (mask_u.sum() + 1e-8)

                loss = loss_l + cfg['unsup_weight'] * loss_u

                loss.backward()
                optimizer.step()

            losses_l.update(loss_l.item())
            losses_u.update(loss_u.item())
            losses_total.update(loss.item())
            lsa_meter.update(lsa_val)
            bce_meter.update(bce_val)
            dice_meter.update(dice_val)
            regions_meter.update(num_regions)

            if (i + 1) % cfg['print_interval'] == 0:
                print(f"Fold {fold} Epoch [{epoch+1}/{cfg['epochs']}] Iter [{i+1}/{num_batches}] "
                      f"Loss: {losses_total.avg:.4f} (L: {losses_l.avg:.4f}, U: {losses_u.avg:.4f}) "
                      f"[LSA: {lsa_meter.avg:.4f}, Dice: {dice_meter.avg:.4f}]")

        # Validation
        if (epoch + 1) % cfg['eval_interval'] == 0:
            val_metrics = validate(model, valloader, device, cfg['num_classes'],
                                 cfg['lambda_weight'], cfg['lsa_threshold'])
            model.train()

            print(f"\nFold {fold} Epoch {epoch+1} Results:")
            print(f"  Train Loss: {losses_total.avg:.4f}")
            print(f"  Val Loss: {val_metrics['loss']:.4f}")
            print(f"  Val IoU: {val_metrics['iou']:.4f}")
            print(f"  Val Dice: {val_metrics['dice']:.4f}")
            print(f"  Val Accuracy: {val_metrics['accuracy']:.4f}")
            print(f"  Val Precision: {val_metrics['precision']:.4f}")
            print(f"  Val Recall: {val_metrics['recall']:.4f}")

            # Save fold history
            fold_history.append({
                'epoch': epoch + 1,
                'train_loss': losses_total.avg,
                'val_loss': val_metrics['loss'],
                'val_iou': val_metrics['iou'],
                'val_dice': val_metrics['dice'],
                'val_accuracy': val_metrics['accuracy'],
                'val_precision': val_metrics['precision'],
                'val_recall': val_metrics['recall'],
            })

            # Save best model
            if val_metrics['dice'] > best_dice:
                best_dice = val_metrics['dice']
                patience_counter = 0

                fold_save_path = os.path.join(args.save_path, f'fold_{fold}')
                os.makedirs(fold_save_path, exist_ok=True)

                checkpoint = {
                    'fold': fold,
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'dice': best_dice,
                    'metrics': val_metrics,
                }
                torch.save(checkpoint, os.path.join(fold_save_path, f'best_model.pth'))
                print(f"  Saved best model for fold {fold} (dice: {best_dice:.4f})")
            else:
                patience_counter += 1
                print(f"  No improvement (patience: {patience_counter}/{cfg['patience']})")

            if cfg['early_stopping'] and patience_counter >= cfg['patience']:
                print(f"\n Early stopping triggered for fold {fold}!")
                break

        scheduler.step()

    print(f"\n{'='*80}")
    print(f"FOLD {fold + 1} COMPLETED - Best Dice: {best_dice:.4f}")
    print(f"{'='*80}\n")

    return {
        'fold': fold,
        'best_dice': best_dice,
        'best_val_metrics': val_metrics,
        'history': fold_history
    }

def main():
    args = parse_args()
    cfg = get_default_config()
    cfg['n_folds'] = args.n_folds

    os.makedirs(args.save_path, exist_ok=True)
    logger = SimpleLogger('global')

    print("\n" + "="*80)
    print("SEMI-SUPERVISED TRAINING WITH LSSA LOSS + K-FOLD CROSS-VALIDATION")
    print("="*80)
    print(f"\n Configuration:")
    print(f"  K-Folds: {cfg['n_folds']}")
    print(f"  LSA threshold: {cfg['lsa_threshold']}")
    print(f"  Lambda (λ): {cfg['lambda_weight']}")
    print(f"  Labeled ratio: {cfg['labeled_ratio']*100:.1f}%")
    print(f"  Epochs per fold: {cfg['epochs']}")
    print(f"  Early stopping: {cfg['early_stopping']} (patience={cfg['patience']})")
    print("="*80)

    # Create k-fold splits
    print("\nCreating k-fold cross-validation splits...")
    fold_info = create_kfold_splits(
        DATA_ROOT,
        n_folds=cfg['n_folds'],
        labeled_ratio=cfg['labeled_ratio'],
        seed=cfg['seed']
    )

    if fold_info is None:
        print("Error creating k-fold splits!")
        return

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nUsing device: {device}")

    # Train each fold
    all_fold_results = []

    for fold in range(cfg['n_folds']):
        fold_result = train_one_fold(fold, cfg, args, device)
        all_fold_results.append(fold_result)

    # Aggregate results across all folds
    print("\n" + "="*80)
    print("K-FOLD CROSS-VALIDATION RESULTS")
    print("="*80)

    all_dice_scores = [r['best_dice'] for r in all_fold_results]
    all_ious = [r['best_val_metrics']['iou'] for r in all_fold_results]
    all_accuracies = [r['best_val_metrics']['accuracy'] for r in all_fold_results]
    all_precisions = [r['best_val_metrics']['precision'] for r in all_fold_results]
    all_recalls = [r['best_val_metrics']['recall'] for r in all_fold_results]

    print("\nPer-Fold Results:")
    for i, result in enumerate(all_fold_results):
        print(f"  Fold {i}: Dice={result['best_dice']:.4f}, "
              f"IoU={result['best_val_metrics']['iou']:.4f}, "
              f"Acc={result['best_val_metrics']['accuracy']:.4f}")

    print(f"\nAggregated Metrics ({cfg['n_folds']}-Fold CV):")
    print(f"  Dice Score: {np.mean(all_dice_scores):.4f} ± {np.std(all_dice_scores):.4f}")
    print(f"  IoU:        {np.mean(all_ious):.4f} ± {np.std(all_ious):.4f}")
    print(f"  Accuracy:   {np.mean(all_accuracies):.4f} ± {np.std(all_accuracies):.4f}")
    print(f"  Precision:  {np.mean(all_precisions):.4f} ± {np.std(all_precisions):.4f}")
    print(f"  Recall:     {np.mean(all_recalls):.4f} ± {np.std(all_recalls):.4f}")

    # Save overall results
    results_summary = {
        'n_folds': cfg['n_folds'],
        'config': cfg,
        'fold_results': all_fold_results,
        'aggregated_metrics': {
            'dice_mean': float(np.mean(all_dice_scores)),
            'dice_std': float(np.std(all_dice_scores)),
            'iou_mean': float(np.mean(all_ious)),
            'iou_std': float(np.std(all_ious)),
            'accuracy_mean': float(np.mean(all_accuracies)),
            'accuracy_std': float(np.std(all_accuracies)),
            'precision_mean': float(np.mean(all_precisions)),
            'precision_std': float(np.std(all_precisions)),
            'recall_mean': float(np.mean(all_recalls)),
            'recall_std': float(np.std(all_recalls)),
        }
    }

    with open(os.path.join(args.save_path, 'kfold_results.json'), 'w') as f:
        json.dump(results_summary, f, indent=2)

    print(f"\n Results saved to: {os.path.join(args.save_path, 'kfold_results.json')}")
    print("="*80)

if __name__ == '__main__':
    main()

!python eval_model.py