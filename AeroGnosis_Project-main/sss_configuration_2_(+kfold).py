# -*- coding: utf-8 -*-
"""sss_configuration_2 (+kfold).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fh3saSDyhNIv0nFxC36Yz5i1axMob3gp

# 1. Setting up the Enivronment
"""

#mount drive
from google.colab import drive
drive.mount('/content/drive')

#install dependencies
!pip install -q torch torchvision
!pip install -q segmentation-models-pytorch
!pip install -q tensorboard
!pip install -q pyyaml
!pip install -q opencv-python
!pip install -q matplotlib

"""# 2. Dataset Configuration and Verification"""

DATA_ROOT = "/content/drive/MyDrive/aircraft_voc"

#verifying dataset structure
import os
from pathlib import Path

def verify_dataset_structure(data_root):
    required_dirs = [
        'JPEGImages',
        'SegmentationClass',
        'ImageSets/Segmentation'
    ]

    all_exist = True

    for dir_path in required_dirs:
        full_path = os.path.join(data_root, dir_path)
        exists = os.path.exists(full_path)

        if exists:
            # Count files
            if os.path.isdir(full_path):
                num_files = len([f for f in os.listdir(full_path) if os.path.isfile(os.path.join(full_path, f))])
                print(f"  {dir_path}: {num_files} files")
        else:
            print(f" {dir_path} is NOT FOUND")
            all_exist = False

    return all_exist

dataset_valid = verify_dataset_structure(DATA_ROOT)

if not dataset_valid:
    print("\n Something wrong")
else:
    print("\n Dataset all good")

"""#3. Create UNet++ with MobileNetV2"""

#create junior UNet++ model
unetpp_model_code = '''import torch
import torch.nn as nn
import torch.nn.functional as F
import segmentation_models_pytorch as smp

class UNetPlusPlus(nn.Module):
    #UNet++ model with MobileNetV2 encoder for semantic segmentation

    def __init__(self, num_classes, encoder_name='mobilenet_v2', pretrained=True):
        super(UNetPlusPlus, self).__init__()

        self.num_classes = num_classes
        self.encoder_name = encoder_name

        #creation of UNet++ model with MobileNetV2 encoder
        self.backbone = smp.UnetPlusPlus(
            encoder_name=encoder_name,
            encoder_weights='imagenet' if pretrained else None,
            in_channels=3,
            classes=num_classes,
            activation=None,  #no activation, return logits
        )

    def forward(self, x, need_fp=False):
           # x: input tensor [B, 3, H, W]
           # need_fp: whether to return feature maps (for consistency regularization)
        #Returns:
            #logits [B, num_classes, H, W] or (logits, features) if need_fp
        # Get output from UNet++
        out = self.backbone(x)

        if need_fp:
            # Use the output as feature representation for consistency loss
            return out, out

        return out

    def decode(self, features):
        return features

def UNetPlusPlusMobileNetV2(num_classes, pretrained=True):
    return UNetPlusPlus(num_classes=num_classes, encoder_name='mobilenet_v2', pretrained=pretrained)
'''

#writing UNet++ model to file
os.makedirs('/content/my_project/model/semseg', exist_ok=True)
with open('/content/my_project/model/semseg/unetplusplus.py', 'w') as f:
    f.write(unetpp_model_code)

print("Created UNet++ model file: model/semseg/unetplusplus.py")
print("  Encoder: MobileNetV2 (lightweight and efficient)")
print("  Decoder: UNet++ nested skip connections")

"""# 5. Create Semi-Supervised Training Script"""

#comprehensive semi-supervised training script
training_script = f'''#!/usr/bin/env python3

#config 1 of Semi-Supervised Training UNet++ only implements Pseudo-labeling with confidence thresholding

import argparse
import os
import random

import numpy as np
import torch
from torch import nn
from torch.optim import SGD, Adam
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as T
import albumentations as A
from albumentations.pytorch import ToTensorV2
import sys

sys.path.insert(0, '/content/my_project')

from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler #mixed precision training

DATA_ROOT = "{DATA_ROOT}"
NUM_CLASSES = 2

# Simple logger and utility classes
class SimpleLogger:
    def __init__(self, name):
        self.name = name

    def info(self, msg):
        print(f"[INFO] {{msg}}")

class AverageMeter:
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def dice_loss(preds, targets, smooth=1e-6):
    #Dice loss for binary segmentation matching our crack detection pipeline EXACTLY
    probs = torch.softmax(preds, dim=1)[:, 1, :, :]  #class 1 (crack)
    targets = (targets == 1).float()

    probs = probs.contiguous().view(-1)
    targets = targets.contiguous().view(-1)

    intersection = (probs * targets).sum()
    dice = (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)

    return 1 - dice

def compute_metrics(pred, target, num_classes=2, ignore_index=255):
    pred = pred.flatten()
    target = target.flatten()

    # Filter out ignore index
    valid_mask = (target != ignore_index)
    pred = pred[valid_mask]
    target = target[valid_mask]

    metrics = {{}}

    #per-class metrics
    ious = []
    dices = []
    precisions = []
    recalls = []

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        #True Positives, False Positives, False Negatives
        tp = (pred_cls & target_cls).sum().float()
        fp = (pred_cls & ~target_cls).sum().float()
        fn = (~pred_cls & target_cls).sum().float()

        #IoU
        union = tp + fp + fn
        iou = tp / (union + 1e-8)
        ious.append(iou.item())

        #Dice
        dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)
        dices.append(dice.item())

        #Precision
        precision = tp / (tp + fp + 1e-8)
        precisions.append(precision.item())

        #Recall
        recall = tp / (tp + fn + 1e-8)
        recalls.append(recall.item())

    #Accuracy
    accuracy = (pred == target).float().mean().item()

    metrics['iou'] = np.mean(ious)
    metrics['dice'] = np.mean(dices)
    metrics['accuracy'] = accuracy
    metrics['precision'] = np.mean(precisions)
    metrics['recall'] = np.mean(recalls)

    return metrics

def validate(model, dataloader, device, num_classes):
    model.eval()
    all_metrics = {{'iou': [], 'dice': [], 'accuracy': [], 'precision': [], 'recall': []}}

    with torch.no_grad():
        for images, masks in dataloader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)

            batch_metrics = compute_metrics(preds, masks, num_classes)
            for k in all_metrics:
                all_metrics[k].append(batch_metrics[k])

    return {{k: np.mean(v) for k, v in all_metrics.items()}}

def parse_args():
    parser = argparse.ArgumentParser(description='Semi-Supervised Semantic Segmentation')
    parser.add_argument('--config', type=str, default='configs/aircraft.yaml')
    parser.add_argument('--labeled-id-path', type=str, default='splits/aircraft/labeled.txt')
    parser.add_argument('--unlabeled-id-path', type=str, default='splits/aircraft/unlabeled.txt')
    parser.add_argument('--save-path', type=str, default='exp/aircraft_unetpp')
    parser.add_argument('--local_rank', type=int, default=0)
    parser.add_argument('--port', default=None, type=int)

    args = parser.parse_args([])  # Empty list for Colab
    return args

def create_split_files(data_root, labeled_ratio=0.1):
    """Create labeled/unlabeled splits for semi-supervised learning"""
    import random

    #Read all training images
    train_file = os.path.join(data_root, 'ImageSets/Segmentation/train.txt')

    with open(train_file, 'r') as f:
        all_ids = [line.strip() for line in f.readlines()]

    if len(all_ids) == 0:
        print("Error: No image IDs found in train file!")
        return False

    # Shuffle and split
    random.shuffle(all_ids)
    num_labeled = max(1, int(len(all_ids) * labeled_ratio))
    labeled_ids = all_ids[:num_labeled]
    unlabeled_ids = all_ids[num_labeled:]

    # Create split directory
    os.makedirs('splits/aircraft', exist_ok=True)

    # Save splits
    with open('splits/aircraft/labeled.txt', 'w') as f:
        f.write('\\n'.join(labeled_ids))

    with open('splits/aircraft/unlabeled.txt', 'w') as f:
        f.write('\\n'.join(unlabeled_ids))

    print(f" Created splits: {{num_labeled}} labeled, {{len(unlabeled_ids)}} unlabeled")
    print(f"  Labeled ratio: {{labeled_ratio*100:.1f}}%")

    return True

def get_default_config():
    """Default configuration for training. Matching our aircraft crack pipeline"""
    return {{
        'dataset': 'aircraft',
        'data_root': DATA_ROOT,
        'batch_size': 4,
        'lr': 0.0005,  # max_lr
        'min_lr': 0.000001,
        'epochs': 40,
        'crop_size': 512,
        'backbone': 'unetplusplus',
        'num_classes': NUM_CLASSES,

        # Semi-supervised settings
        'labeled_ratio': 0.7,
        'unsup_weight': 1.0,
        'pseudo_threshold': 0.95,
        'use_strong_aug': True,

        # Optimizer
        'optimizer': 'adam',
        'momentum': 0.9,
        'weight_decay': 0,
        'lr_decay_type': 'cos',

        # Training settings
        'num_workers': 4,
        'mixed_precision': True,  # 16-bit AMP
        'sync_bn': True,

        # Early stopping
        'early_stopping': True,
        'patience': 10,
        'monitor': 'val_dice',  # Monitor dice score

        # Logging
        'print_interval': 10,
        'eval_interval': 1,  # Validate every epoch
        'save_period': 20,
    }}

class SimpleDataset(Dataset):
    #Dataset with augmentations matching our pipeline EXACTLY"""
    def __init__(self, data_root, split_file, phase='train', size=512):
        self.data_root = data_root
        self.phase = phase
        self.size = size

        with open(split_file, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]

        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]

        if phase == 'train':
            self.transform = A.Compose([
                A.RandomScale(scale_limit=0.5, p=0.5),
                A.Rotate(limit=10, p=0.5),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.Resize(size, size),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])
        else:
            self.transform = A.Compose([
                A.Resize(size, size),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]

        #Load image
        img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.jpg")
        if not os.path.exists(img_path):
            img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.png")
        img = Image.open(img_path).convert('RGB')

        #Load mask
        mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.png")
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.jpg")
        mask = Image.open(mask_path)

        #IMP FIX: Resize BEFORE converting to numpy for same dimensions
        img = img.resize((self.size, self.size), Image.BILINEAR)
        mask = mask.resize((self.size, self.size), Image.NEAREST)

        #Convert to numpy for Albumentations
        img = np.array(img)
        mask = np.array(mask)

        #Apply transforms (for both image and mask)
        augmented = self.transform(image=img, mask=mask)
        img = augmented['image']
        mask = augmented['mask'].long()

        return img, mask

def main():
    args = parse_args()
    cfg = get_default_config()

    #Setup logging
    os.makedirs(args.save_path, exist_ok=True)
    logger = SimpleLogger('global')

    #Set random seed
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)

    # Create labeled/unlabeled splits
    print("\\nCreating semi-supervised data splits...")
    if not create_split_files(DATA_ROOT, labeled_ratio=cfg['labeled_ratio']):
        print("Error creating splits!")
        return

    #Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\\nUsing device: {{device}}")

    #Create model
    print("\\nCreating UNet++ model with MobileNetV2 encoder...")
    model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=True)
    model = model.to(device)

    #Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  Total parameters: {{total_params:,}}")

    #Optimizer
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=cfg['lr'],
        weight_decay=cfg['weight_decay']
    )

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg['epochs'],
        eta_min=cfg['min_lr']
    )

    # Loss functions. HYBRID LOSS like our pipeline
    criterion_ce = nn.CrossEntropyLoss(ignore_index=255).to(device)  # CrossEntropy
    criterion_u = nn.CrossEntropyLoss(ignore_index=255, reduction='none').to(device)

    # Mixed precision training scaler
    scaler = GradScaler() if cfg['mixed_precision'] else None

    # Load datasets
    print(f"\\nLoading datasets from {{DATA_ROOT}}...")

    trainset_l = SimpleDataset(DATA_ROOT, 'splits/aircraft/labeled.txt', phase='train', size=cfg['crop_size'])
    trainset_u = SimpleDataset(DATA_ROOT, 'splits/aircraft/unlabeled.txt', phase='train', size=cfg['crop_size'])

    trainloader_l = DataLoader(trainset_l, batch_size=cfg['batch_size'],
                               shuffle=True, num_workers=cfg['num_workers'], drop_last=True, pin_memory=True)
    trainloader_u = DataLoader(trainset_u, batch_size=cfg['batch_size'],
                               shuffle=True, num_workers=cfg['num_workers'], drop_last=True, pin_memory=True)

    print(f"  Labeled samples: {{len(trainset_l)}}")
    print(f"  Unlabeled samples: {{len(trainset_u)}}")

    #Training loop
    print(f"\\nStarting semi-supervised training for {{cfg['epochs']}} epochs...")
    print(f"Mixed Precision: {{cfg['mixed_precision']}}")
    print(f"Early Stopping: {{cfg['early_stopping']}} (patience={{cfg['patience']}})")
    print("=" * 80)

    best_loss = float('inf')
    best_dice = 0.0
    patience_counter = 0

    for epoch in range(cfg['epochs']):
        model.train()

        losses_l = AverageMeter()
        losses_u = AverageMeter()
        losses_total = AverageMeter()

        loader_l_iter = iter(trainloader_l)
        loader_u_iter = iter(trainloader_u)

        num_batches = min(len(trainloader_l), len(trainloader_u))

        for i in range(num_batches):
            # Get labeled batch
            try:
                img_l, mask_l = next(loader_l_iter)
            except StopIteration:
                loader_l_iter = iter(trainloader_l)
                img_l, mask_l = next(loader_l_iter)

            # Get unlabeled batch
            try:
                img_u, _ = next(loader_u_iter)
            except StopIteration:
                loader_u_iter = iter(trainloader_u)
                img_u, _ = next(loader_u_iter)

            img_l, mask_l = img_l.to(device), mask_l.to(device)
            img_u = img_u.to(device)

            optimizer.zero_grad()

            # Mixed precision training
            if cfg['mixed_precision']:
                with autocast():
                    # Forward pass on labeled data with HYBRID LOSS
                    pred_l = model(img_l)
                    ce_loss = criterion_ce(pred_l, mask_l)
                    d_loss = dice_loss(pred_l, mask_l)
                    loss_l = ce_loss + 0.5 * d_loss  # HYBRID: CE + 0.5*Dice

                    # Forward pass on unlabeled data (pseudo-labeling)
                    with torch.no_grad():
                        pred_u = model(img_u)
                        prob_u = F.softmax(pred_u, dim=1)
                        conf_u, pseudo_u = torch.max(prob_u, dim=1)
                        mask_u = (conf_u >= cfg['pseudo_threshold']).float()

                    # Supervised loss on pseudo-labels
                    pred_u_strong = model(img_u)
                    loss_u = criterion_u(pred_u_strong, pseudo_u)
                    loss_u = (loss_u * mask_u).sum() / (mask_u.sum() + 1e-8)

                    # Total loss
                    loss = loss_l + cfg['unsup_weight'] * loss_u

                # Backward with gradient scaling
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                #Standard training (no mixed precision) with HYBRID LOSS
                pred_l = model(img_l)
                ce_loss = criterion_ce(pred_l, mask_l)
                d_loss = dice_loss(pred_l, mask_l)
                loss_l = ce_loss + 0.5 * d_loss  # HYBRID: CE + 0.5*Dice

                with torch.no_grad():
                    pred_u = model(img_u)
                    prob_u = F.softmax(pred_u, dim=1)
                    conf_u, pseudo_u = torch.max(prob_u, dim=1)
                    mask_u = (conf_u >= cfg['pseudo_threshold']).float()

                pred_u_strong = model(img_u)
                loss_u = criterion_u(pred_u_strong, pseudo_u)
                loss_u = (loss_u * mask_u).sum() / (mask_u.sum() + 1e-8)

                loss = loss_l + cfg['unsup_weight'] * loss_u

                loss.backward()
                optimizer.step()

            #Update metrics
            losses_l.update(loss_l.item())
            losses_u.update(loss_u.item())
            losses_total.update(loss.item())

            if (i + 1) % cfg['print_interval'] == 0:
                print(f"Epoch [{{epoch+1}}/{{cfg['epochs']}}] "
                      f"Iter [{{i+1}}/{{num_batches}}] "
                      f"Loss: {{losses_total.avg:.4f}} "
                      f"(L: {{losses_l.avg:.4f}}, U: {{losses_u.avg:.4f}})")

        # Validation phase
        if (epoch + 1) % cfg['eval_interval'] == 0:
            model.eval()
            val_metrics = validate(model, trainloader_l, device, cfg['num_classes'])
            model.train()

            print(f"\\nEpoch {{epoch+1}} completed:")
            print(f"  Train Loss: {{losses_total.avg:.4f}} (Labeled: {{losses_l.avg:.4f}}, Unlabeled: {{losses_u.avg:.4f}})")
            print(f"  Val IoU: {{val_metrics['iou']:.4f}}")
            print(f"  Val Dice: {{val_metrics['dice']:.4f}}")
            print(f"  Val Accuracy: {{val_metrics['accuracy']:.4f}}")
            print(f"  Val Precision: {{val_metrics['precision']:.4f}}")
            print(f"  Val Recall: {{val_metrics['recall']:.4f}}")
            print(f"  Learning Rate: {{optimizer.param_groups[0]['lr']:.6f}}")

            # Save best model based on dice score
            if val_metrics['dice'] > best_dice:
                best_dice = val_metrics['dice']
                patience_counter = 0
                checkpoint = {{
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'dice': best_dice,
                    'loss': losses_total.avg,
                }}
                torch.save(checkpoint, os.path.join(args.save_path, f'best_model_dice={{best_dice:.2f}}.pth'))
                print(f"  Saved best model (dice: {{best_dice:.4f}})")
            else:
                patience_counter += 1
                print(f"  No improvement (patience: {{patience_counter}}/{{cfg['patience']}})")

            # Early stopping
            if cfg['early_stopping'] and patience_counter >= cfg['patience']:
                print(f"\\n  Early stopping triggered! No improvement for {{cfg['patience']}} epochs.")
                break

        # Step the learning rate scheduler
        scheduler.step()

        # Periodic checkpoint saving
        if (epoch + 1) % cfg['save_period'] == 0:
            checkpoint = {{
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': losses_total.avg,
            }}
            torch.save(checkpoint, os.path.join(args.save_path, f'epoch_{{epoch+1}}.pth'))
            print(f"  Saved periodic checkpoint: epoch_{{epoch+1}}.pth")

        print("=" * 80)

    print("\\n Training complete!")
    print(f"Best loss: {{best_loss:.4f}}")
    print(f"Model saved to: {{args.save_path}}/best_model.pth")

if __name__ == '__main__':
    main()
'''

with open('/content/my_project/train_semi.py', 'w') as f:
    f.write(training_script)

print("✓ Created semi-supervised training script: train_semi.py")

"""# 6. Evaluation"""

eval_script = f'''#!/usr/bin/env python3
"""
Evaluation script for trained UNet++ model
"""

import os
import sys
import torch
import numpy as np
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as T
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

sys.path.insert(0, '/content/my_project')
from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2

DATA_ROOT = "{DATA_ROOT}"
NUM_CLASSES = 2
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class EvalDataset(Dataset):
    def __init__(self, data_root, split_file, transform=None, size=512):
        self.data_root = data_root
        self.transform = transform
        self.size = size

        with open(split_file, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]

        # Load image
        img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.jpg")
        if not os.path.exists(img_path):
            img_path = os.path.join(self.data_root, 'JPEGImages', f"{{img_id}}.png")
        img = Image.open(img_path).convert('RGB')

        #Load mask
        mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.png")
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{{img_id}}.jpg")
        mask = Image.open(mask_path)

        # Store original as numpy array (not PIL Image)
        img_orig = np.array(img.copy())

        #Resize
        img = img.resize((self.size, self.size), Image.BILINEAR)
        mask = mask.resize((self.size, self.size), Image.NEAREST)

        if self.transform:
            img = self.transform(img)

        mask = torch.from_numpy(np.array(mask)).long()

        return img, mask, img_orig, img_id

def compute_iou(pred, target, num_classes, ignore_index=255):
    """Compute mean IoU"""
    ious = []
    pred = pred.flatten()
    target = target.flatten()

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        # Ignore pixels marked as ignore_index
        valid = (target != ignore_index)
        pred_cls = pred_cls & valid
        target_cls = target_cls & valid

        intersection = (pred_cls & target_cls).sum()
        union = (pred_cls | target_cls).sum()

        if union == 0:
            continue

        iou = intersection.float() / union.float()
        ious.append(iou.item())

    return np.mean(ious) if ious else 0.0

def compute_all_metrics(pred, target, num_classes, ignore_index=255):
    #Compute IoU, Dice, Precision, Recall, Accuracy
    pred = pred.flatten()
    target = target.flatten()

    # Filter out ignore index
    valid_mask = (target != ignore_index)
    pred = pred[valid_mask]
    target = target[valid_mask]

    ious = []
    dices = []
    precisions = []
    recalls = []

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        #True Positives, False Positives, False Negatives
        tp = (pred_cls & target_cls).sum().float()
        fp = (pred_cls & ~target_cls).sum().float()
        fn = (~pred_cls & target_cls).sum().float()

        # IoU
        union = tp + fp + fn
        iou = tp / (union + 1e-8)
        ious.append(iou.item())

        #Dice (F1)
        dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)
        dices.append(dice.item())

        #Precision
        precision = tp / (tp + fp + 1e-8)
        precisions.append(precision.item())

        #Recall
        recall = tp / (tp + fn + 1e-8)
        recalls.append(recall.item())

    # Overall Accuracy
    accuracy = (pred == target).float().mean().item()

    return {{
        'iou': ious,
        'dice': dices,
        'precision': precisions,
        'recall': recalls,
        'accuracy': accuracy,
        'mean_iou': np.mean(ious),
        'mean_dice': np.mean(dices),
        'mean_precision': np.mean(precisions),
        'mean_recall': np.mean(recalls)
    }}

#Load model
print("Loading trained model...")
model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=False)

# Find the best model file (with dice score in filename)
import glob
checkpoint_dir = 'exp/aircraft_unetpp'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'best_model_dice=*.pth'))

if checkpoint_files:
    # Get the file with highest dice score
    checkpoint_path = max(checkpoint_files, key=lambda x: float(x.split('dice=')[1].split('.pth')[0]))
    print(f"Found best checkpoint: {{os.path.basename(checkpoint_path)}}")
else:
    #Fallback to generic name if it exists
    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')
    if not os.path.exists(checkpoint_path):
        print("Error: No checkpoint found!")
        print(f"Looking in: {{checkpoint_dir}}")
        import sys
        sys.exit(1)

checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(DEVICE)
model.eval()

print(f"Model loaded from epoch {{checkpoint['epoch']}}")
if 'dice' in checkpoint:
    print(f"Best Dice score: {{checkpoint['dice']:.4f}}")

#Load validation dataset
val_transform = T.Compose([
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_file = os.path.join(DATA_ROOT, 'ImageSets/Segmentation/val.txt')
val_dataset = EvalDataset(DATA_ROOT, val_file, val_transform, size=512)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)

print(f"\\nEvaluating on {{len(val_dataset)}} validation images...")

# Evaluate
os.makedirs('results', exist_ok=True)
all_ious = []
all_dices = []
all_precisions = []
all_recalls = []
all_accuracies = []
all_losses = []
all_preds = []
all_targets = []

#Loss function for evaluation
criterion = torch.nn.CrossEntropyLoss(ignore_index=255)

with torch.no_grad():
    for idx, (image, mask, img_orig, img_id) in enumerate(tqdm(val_loader)):
        image = image.to(DEVICE)
        mask = mask.to(DEVICE)

        output = model(image)

        #Compute loss
        loss = criterion(output, mask)
        all_losses.append(loss.item())

        pred = output.argmax(dim=1).cpu().numpy()[0]
        mask_np = mask.cpu().numpy()[0]

        # Store for confusion matrix
        valid_mask = (mask_np != 255)
        all_preds.extend(pred[valid_mask].flatten())
        all_targets.extend(mask_np[valid_mask].flatten())

        #compute all metrics
        metrics = compute_all_metrics(torch.from_numpy(pred), torch.from_numpy(mask_np), NUM_CLASSES)
        all_ious.append(metrics['mean_iou'])
        all_dices.append(metrics['mean_dice'])
        all_precisions.append(metrics['mean_precision'])
        all_recalls.append(metrics['mean_recall'])
        all_accuracies.append(metrics['accuracy'])

        #Save visualizations for first 10 images
        if idx < 10:
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            # Original image
            axes[0].imshow(img_orig[0])
            axes[0].set_title('Input Image')
            axes[0].axis('off')

            axes[1].imshow(mask_np, cmap='tab20', vmin=0, vmax=NUM_CLASSES-1)
            axes[1].set_title('Ground Truth')
            axes[1].axis('off')

            axes[2].imshow(pred, cmap='tab20', vmin=0, vmax=NUM_CLASSES-1)
            axes[2].set_title(f'Prediction\\nIoU: {{metrics["mean_iou"]:.3f}} | Dice: {{metrics["mean_dice"]:.3f}}')
            axes[2].axis('off')

            plt.tight_layout()
            plt.savefig(f'results/prediction_{{idx}}.png', dpi=150, bbox_inches='tight')
            plt.close()

mean_iou = np.mean(all_ious)
mean_dice = np.mean(all_dices)
mean_precision = np.mean(all_precisions)
mean_recall = np.mean(all_recalls)
mean_accuracy = np.mean(all_accuracies)
mean_loss = np.mean(all_losses)

#Create confusion matrix
cm = confusion_matrix(all_targets, all_preds, labels=list(range(NUM_CLASSES)))

#Normalize confusion matrix for percentages
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

#Plot confusion matrix with both counts and percentages
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
class_names = ['Background', 'Aircraft'] if NUM_CLASSES == 2 else [f'Class {{i}}' for i in range(NUM_CLASSES)]

#Confusion Matrix Raw Counts
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
            xticklabels=class_names,
            yticklabels=class_names,
            cbar_kws={{'label': 'Count'}},
            annot_kws={{'size': 14, 'weight': 'bold'}})
ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold', pad=15)
ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax1.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

# Confusion Matrix Percentages
sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens', ax=ax2,
            xticklabels=class_names,
            yticklabels=class_names,
            cbar_kws={{'label': 'Percentage (%)'}},
            annot_kws={{'size': 14, 'weight': 'bold'}})
ax2.set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold', pad=15)
ax2.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax2.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

plt.suptitle(f'Model Performance: IoU={{mean_iou:.4f}} | Dice={{mean_dice:.4f}} | Accuracy={{mean_accuracy:.4f}}',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('results/confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.close()

print(f"\\n{{'='*70}}")
print(f"EVALUATION RESULTS")
print(f"{{'='*70}}")
print(f"\\n  Overall Metrics:")
print(f"{{'─'*70}}")
print(f"  Loss:           {{mean_loss:.4f}} (±{{np.std(all_losses):.4f}})")
print(f"  Accuracy:       {{mean_accuracy:.4f}} (±{{np.std(all_accuracies):.4f}})")
print(f"  Mean IoU:       {{mean_iou:.4f}} (±{{np.std(all_ious):.4f}})")
print(f"  Mean Dice:      {{mean_dice:.4f}} (±{{np.std(all_dices):.4f}})")
print(f"  Mean Precision: {{mean_precision:.4f}} (±{{np.std(all_precisions):.4f}})")
print(f"  Mean Recall:    {{mean_recall:.4f}} (±{{np.std(all_recalls):.4f}})")
print(f"{{'─'*70}}")
print(f"\\n Best Performance:")
print(f"{{'─'*70}}")
print(f"  Max IoU:        {{np.max(all_ious):.4f}}")
print(f"  Max Dice:       {{np.max(all_dices):.4f}}")
print(f"  Max Precision:  {{np.max(all_precisions):.4f}}")
print(f"  Max Recall:     {{np.max(all_recalls):.4f}}")
print(f"  Max Accuracy:   {{np.max(all_accuracies):.4f}}")
print(f"{{'─'*70}}")
print(f"\\n Per-Class Performance:")
print(f"{{'─'*70}}")
for i, class_name in enumerate(class_names):
    tp = cm[i, i]
    fp = cm[:, i].sum() - tp
    fn = cm[i, :].sum() - tp
    class_iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
    class_accuracy = tp / cm[i, :].sum() if cm[i, :].sum() > 0 else 0

    print(f"\\n  {{class_name}}:")
    print(f"    True Positives:  {{tp:,}}")
    print(f"    False Positives: {{fp:,}}")
    print(f"    False Negatives: {{fn:,}}")
    print(f"    Class IoU:       {{class_iou:.4f}}")
    print(f"    Class Accuracy:  {{class_accuracy:.4f}}")
print(f"\\n{{'='*70}}")
print("\\n Evaluation Complete!")
print(f"{{'─'*70}}")
print("  Saved Files:")
print("     Predictions: results/prediction_*.png (first 10 samples)")
print("     Confusion Matrix: results/confusion_matrix.png")
print(f"{{'='*70}}")
'''

with open('/content/my_project/eval_model.py', 'w') as f:
    f.write(eval_script)

print("Created evaluation script: eval_model.py")

"""# 7. Run and Evaluate"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/my_project
!python train_semi.py

!python eval_model.py

from IPython.display import Image, display
import glob

for img_path in sorted(glob.glob('results/prediction_*.png')):
    display(Image(img_path))

"""# 9. K-Fold Validation

Training Script
"""

#!/usr/bin/env python3
#K-Fold Cross-Validation for Semi-Supervised Training with UNet++
import argparse
import os
import random
import numpy as np
import torch
from torch import nn
from torch.optim import SGD, Adam
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as T
import albumentations as A
from albumentations.pytorch import ToTensorV2
import sys
sys.path.insert(0, '/content/my_project')
from model.semseg.unetplusplus import UNetPlusPlusMobileNetV2
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
import json
from sklearn.model_selection import KFold

DATA_ROOT = "/content/drive/MyDrive/aircraft_voc"
NUM_CLASSES = 2

#  logger and utility classes
class SimpleLogger:
    def __init__(self, name):
        self.name = name
    def info(self, msg):
        print(f"[INFO] {msg}")

class AverageMeter:
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def dice_loss(preds, targets, smooth=1e-6):
    probs = torch.softmax(preds, dim=1)[:, 1, :, :]
    targets = (targets == 1).float()

    probs = probs.contiguous().view(-1)
    targets = targets.contiguous().view(-1)

    intersection = (probs * targets).sum()
    dice = (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)
    return 1 - dice

def compute_metrics(pred, target, num_classes=2, ignore_index=255):
    pred = pred.flatten()
    target = target.flatten()

    valid_mask = (target != ignore_index)
    pred = pred[valid_mask]
    target = target[valid_mask]

    metrics = {}

    ious = []
    dices = []
    precisions = []
    recalls = []

    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)

        tp = (pred_cls & target_cls).sum().float()
        fp = (pred_cls & ~target_cls).sum().float()
        fn = (~pred_cls & target_cls).sum().float()

        union = tp + fp + fn
        iou = tp / (union + 1e-8)
        ious.append(iou.item())

        dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)
        dices.append(dice.item())

        precision = tp / (tp + fp + 1e-8)
        precisions.append(precision.item())

        recall = tp / (tp + fn + 1e-8)
        recalls.append(recall.item())

    accuracy = (pred == target).float().mean().item()

    metrics['iou'] = np.mean(ious)
    metrics['dice'] = np.mean(dices)
    metrics['accuracy'] = accuracy
    metrics['precision'] = np.mean(precisions)
    metrics['recall'] = np.mean(recalls)

    return metrics

def validate(model, dataloader, device, num_classes):
    model.eval()
    all_metrics = {'iou': [], 'dice': [], 'accuracy': [], 'precision': [], 'recall': []}

    with torch.no_grad():
        for images, masks in dataloader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)

            batch_metrics = compute_metrics(preds, masks, num_classes)
            for k in all_metrics:
                all_metrics[k].append(batch_metrics[k])

    return {k: np.mean(v) for k, v in all_metrics.items()}

def parse_args():
    parser = argparse.ArgumentParser(description='K-Fold Semi-Supervised Semantic Segmentation')
    parser.add_argument('--n-folds', type=int, default=5, help='Number of folds for cross-validation')
    parser.add_argument('--save-path', type=str, default='exp/aircraft_unetpp_kfold')
    parser.add_argument('--local_rank', type=int, default=0)
    args = parser.parse_args([])
    return args

def get_default_config():
    return {
        'dataset': 'aircraft',
        'data_root': DATA_ROOT,
        'batch_size': 4,
        'lr': 0.0005,
        'min_lr': 0.000001,
        'epochs': 40,
        'crop_size': 512,
        'backbone': 'unetplusplus',
        'num_classes': NUM_CLASSES,
        #SSS settings
        'labeled_ratio': 0.7,
        'unsup_weight': 1.0,
        'pseudo_threshold': 0.95,
        'use_strong_aug': True,
        #Optimizer
        'optimizer': 'adam',
        'momentum': 0.9,
        'weight_decay': 0,
        'lr_decay_type': 'cos',
        #Training settings
        'num_workers': 4,
        'mixed_precision': True,
        'sync_bn': True,
        #Early stopping
        'early_stopping': True,
        'patience': 10,
        'monitor': 'val_dice',
        #Logging
        'print_interval': 10,
        'eval_interval': 1,
        'save_period': 20,
    }

def create_kfold_splits(data_root, n_folds=5, labeled_ratio=0.7, seed=42):
    #Create k-fold splits for cross-validation


    #Read all training images
    train_file = os.path.join(data_root, 'ImageSets/Segmentation/train.txt')

    with open(train_file, 'r') as f:
        all_ids = [line.strip() for line in f.readlines()]

    if len(all_ids) == 0:
        print("Error: No image IDs found!")
        return None

    print(f"\n Creating {n_folds}-Fold Cross-Validation Splits")
    print(f"  Total samples: {len(all_ids)}")
    print(f"  Labeled ratio per fold: {labeled_ratio*100:.1f}%")
    print("="*80)

    #create k-fold splits
    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)

    fold_splits = []
    for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(all_ids)):
        #get train and validation ids for this fold
        train_ids = [all_ids[i] for i in train_indices]
        val_ids = [all_ids[i] for i in val_indices]

        #split train into labeled and unlabeled
        random.seed(seed + fold_idx)  #diff seed per fold for variety
        random.shuffle(train_ids)

        num_labeled = max(1, int(len(train_ids) * labeled_ratio))
        labeled_ids = train_ids[:num_labeled]
        unlabeled_ids = train_ids[num_labeled:]

        fold_splits.append({
            'fold': fold_idx + 1,
            'train_ids': train_ids,
            'val_ids': val_ids,
            'labeled_ids': labeled_ids,
            'unlabeled_ids': unlabeled_ids
        })

        print(f"\nFold {fold_idx + 1}/{n_folds}:")
        print(f"  Train: {len(train_ids)} samples")
        print(f"     Labeled: {len(labeled_ids)} ({len(labeled_ids)/len(train_ids)*100:.1f}%)")
        print(f"     Unlabeled: {len(unlabeled_ids)} ({len(unlabeled_ids)/len(train_ids)*100:.1f}%)")
        print(f"  Validation: {len(val_ids)} samples")

    return fold_splits

def save_fold_split(fold_split, fold_dir):
   #Save fold split to text files
    os.makedirs(fold_dir, exist_ok=True)

    with open(os.path.join(fold_dir, 'labeled.txt'), 'w') as f:
        f.write('\n'.join(fold_split['labeled_ids']))

    with open(os.path.join(fold_dir, 'unlabeled.txt'), 'w') as f:
        f.write('\n'.join(fold_split['unlabeled_ids']))

    with open(os.path.join(fold_dir, 'val.txt'), 'w') as f:
        f.write('\n'.join(fold_split['val_ids']))

class SimpleDataset(Dataset):
    #Dataset with augmentation
    def __init__(self, data_root, split_file, phase='train', size=512):
        self.data_root = data_root
        self.phase = phase
        self.size = size

        # Read IDs from file
        with open(split_file, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]

        # Albumentations transforms
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]

    #same transforms as before
        if phase == 'train':
            self.transform = A.Compose([
                A.RandomScale(scale_limit=0.5, p=0.5),
                A.Rotate(limit=10, p=0.5),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.Resize(size, size),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])
        else:
            self.transform = A.Compose([
                A.Resize(size, size),
                A.Normalize(mean=mean, std=std),
                ToTensorV2()
            ])

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]

        #get image
        img_path = os.path.join(self.data_root, 'JPEGImages', f"{img_id}.jpg")
        if not os.path.exists(img_path):
            img_path = os.path.join(self.data_root, 'JPEGImages', f"{img_id}.png")
        img = Image.open(img_path).convert('RGB')

        #get mask
        mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{img_id}.png")
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.data_root, 'SegmentationClass', f"{img_id}.jpg")
        mask = Image.open(mask_path)

        # Resize
        img = img.resize((self.size, self.size), Image.BILINEAR)
        mask = mask.resize((self.size, self.size), Image.NEAREST)

        #Convert to numpy
        img = np.array(img)
        mask = np.array(mask)

        #Apply transforms
        augmented = self.transform(image=img, mask=mask)
        img = augmented['image']
        mask = augmented['mask'].long()

        return img, mask

def train_one_fold(fold_split, fold_idx, cfg, device, save_path):
    print(f"\n{'='*80}")
    print(f"TRAINING FOLD {fold_idx}/{cfg['n_folds']}")
    print(f"{'='*80}\n")

    fold_dir = os.path.join(save_path, f'fold_{fold_idx}')
    os.makedirs(fold_dir, exist_ok=True)

    save_fold_split(fold_split, fold_dir)

    #create model
    print("Creating UNet++ model with MobileNetV2 encoder...")
    model = UNetPlusPlusMobileNetV2(num_classes=NUM_CLASSES, pretrained=True)
    model = model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"  Total parameters: {total_params:,}")

    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=cfg['lr'],
        weight_decay=cfg['weight_decay']
    )

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg['epochs'],
        eta_min=cfg['min_lr']
    )

    criterion_ce = nn.CrossEntropyLoss(ignore_index=255).to(device)
    criterion_u = nn.CrossEntropyLoss(ignore_index=255, reduction='none').to(device)

    scaler = GradScaler() if cfg['mixed_precision'] else None

    #Load datasets
    print(f"\nLoading datasets for fold {fold_idx}...")
    trainset_l = SimpleDataset(DATA_ROOT, os.path.join(fold_dir, 'labeled.txt'),
                               phase='train', size=cfg['crop_size'])
    trainset_u = SimpleDataset(DATA_ROOT, os.path.join(fold_dir, 'unlabeled.txt'),
                               phase='train', size=cfg['crop_size'])
    valset = SimpleDataset(DATA_ROOT, os.path.join(fold_dir, 'val.txt'),
                          phase='val', size=cfg['crop_size'])

    trainloader_l = DataLoader(trainset_l, batch_size=cfg['batch_size'],
                               shuffle=True, num_workers=cfg['num_workers'],
                               drop_last=True, pin_memory=True)
    trainloader_u = DataLoader(trainset_u, batch_size=cfg['batch_size'],
                               shuffle=True, num_workers=cfg['num_workers'],
                               drop_last=True, pin_memory=True)
    valloader = DataLoader(valset, batch_size=cfg['batch_size'],
                          shuffle=False, num_workers=cfg['num_workers'],
                          pin_memory=True)

    print(f"  Labeled samples: {len(trainset_l)}")
    print(f"  Unlabeled samples: {len(trainset_u)}")
    print(f"  Validation samples: {len(valset)}")

    #training loop
    print(f"\nStarting training for {cfg['epochs']} epochs...")
    print("="*80)

    best_dice = 0.0
    patience_counter = 0
    fold_history = []

    for epoch in range(cfg['epochs']):
        model.train()
        losses_l = AverageMeter()
        losses_u = AverageMeter()
        losses_total = AverageMeter()

        loader_l_iter = iter(trainloader_l)
        loader_u_iter = iter(trainloader_u)

        num_batches = min(len(trainloader_l), len(trainloader_u))

        for i in range(num_batches):
            #get batches
            try:
                img_l, mask_l = next(loader_l_iter)
            except StopIteration:
                loader_l_iter = iter(trainloader_l)
                img_l, mask_l = next(loader_l_iter)

            try:
                img_u, _ = next(loader_u_iter)
            except StopIteration:
                loader_u_iter = iter(trainloader_u)
                img_u, _ = next(loader_u_iter)

            img_l, mask_l = img_l.to(device), mask_l.to(device)
            img_u = img_u.to(device)

            optimizer.zero_grad()

            if cfg['mixed_precision']:
                with autocast():
                    #supervised loss
                    pred_l = model(img_l)
                    ce_loss = criterion_ce(pred_l, mask_l)
                    d_loss = dice_loss(pred_l, mask_l)
                    loss_l = ce_loss + 0.5 * d_loss

                    #pseudo-labeling
                    with torch.no_grad():
                        pred_u = model(img_u)
                        prob_u = F.softmax(pred_u, dim=1)
                        conf_u, pseudo_u = torch.max(prob_u, dim=1)
                        mask_u = (conf_u >= cfg['pseudo_threshold']).float()

                    pred_u_strong = model(img_u)
                    loss_u = criterion_u(pred_u_strong, pseudo_u)
                    loss_u = (loss_u * mask_u).sum() / (mask_u.sum() + 1e-8)

                    loss = loss_l + cfg['unsup_weight'] * loss_u

                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                #standard training
                pred_l = model(img_l)
                ce_loss = criterion_ce(pred_l, mask_l)
                d_loss = dice_loss(pred_l, mask_l)
                loss_l = ce_loss + 0.5 * d_loss

                with torch.no_grad():
                    pred_u = model(img_u)
                    prob_u = F.softmax(pred_u, dim=1)
                    conf_u, pseudo_u = torch.max(prob_u, dim=1)
                    mask_u = (conf_u >= cfg['pseudo_threshold']).float()

                pred_u_strong = model(img_u)
                loss_u = criterion_u(pred_u_strong, pseudo_u)
                loss_u = (loss_u * mask_u).sum() / (mask_u.sum() + 1e-8)

                loss = loss_l + cfg['unsup_weight'] * loss_u

                loss.backward()
                optimizer.step()

            losses_l.update(loss_l.item())
            losses_u.update(loss_u.item())
            losses_total.update(loss.item())

            if (i + 1) % cfg['print_interval'] == 0:
                print(f"Fold {fold_idx} | Epoch [{epoch+1}/{cfg['epochs']}] "
                      f"Iter [{i+1}/{num_batches}] "
                      f"Loss: {losses_total.avg:.4f} "
                      f"(L: {losses_l.avg:.4f}, U: {losses_u.avg:.4f})")

        #Validation
        if (epoch + 1) % cfg['eval_interval'] == 0:
            val_metrics = validate(model, valloader, device, cfg['num_classes'])

            print(f"\nFold {fold_idx} | Epoch {epoch+1} completed:")
            print(f"  Train Loss: {losses_total.avg:.4f} (L: {losses_l.avg:.4f}, U: {losses_u.avg:.4f})")
            print(f"  Val IoU: {val_metrics['iou']:.4f}")
            print(f"  Val Dice: {val_metrics['dice']:.4f}")
            print(f"  Val Accuracy: {val_metrics['accuracy']:.4f}")
            print(f"  Val Precision: {val_metrics['precision']:.4f}")
            print(f"  Val Recall: {val_metrics['recall']:.4f}")

            # history
            fold_history.append({
                'epoch': epoch + 1,
                'train_loss': losses_total.avg,
                'val_iou': val_metrics['iou'],
                'val_dice': val_metrics['dice'],
                'val_accuracy': val_metrics['accuracy'],
                'val_precision': val_metrics['precision'],
                'val_recall': val_metrics['recall']
            })

            #Save best model
            if val_metrics['dice'] > best_dice:
                best_dice = val_metrics['dice']
                patience_counter = 0

                checkpoint = {
                    'fold': fold_idx,
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'dice': best_dice,
                    'metrics': val_metrics
                }
                torch.save(checkpoint, os.path.join(fold_dir, f'best_model.pth'))
                print(f"  Saved best model (dice: {best_dice:.4f})")
            else:
                patience_counter += 1
                print(f"  No improvement (patience: {patience_counter}/{cfg['patience']})")

            # Early stopping
            if cfg['early_stopping'] and patience_counter >= cfg['patience']:
                print(f"\n Early stopping triggered for fold {fold_idx}")
                break

        scheduler.step()
        print("-"*80)

    # Save fold history
    with open(os.path.join(fold_dir, 'history.json'), 'w') as f:
        json.dump(fold_history, f, indent=2)

    # Return best validation metrics
    best_metrics = validate(model, valloader, device, cfg['num_classes'])

    # Load best model
    checkpoint = torch.load(os.path.join(fold_dir, 'best_model.pth'),
                           map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])

    # Evaluate best model
    final_metrics = validate(model, valloader, device, cfg['num_classes'])

    print(f"\n{'='*80}")
    print(f"FOLD {fold_idx} COMPLETED")
    print(f"{'='*80}")
    print(f"Best Dice: {best_dice:.4f}")
    print(f"Final Metrics:")
    for k, v in final_metrics.items():
        print(f"  {k}: {v:.4f}")
    print(f"{'='*80}\n")

    return final_metrics

def main():
    args = parse_args()
    cfg = get_default_config()
    cfg['n_folds'] = args.n_folds

    #setup
    os.makedirs(args.save_path, exist_ok=True)

    # Set random seed
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nUsing device: {device}")

    # Create k-fold splits
    fold_splits = create_kfold_splits(
        DATA_ROOT,
        n_folds=args.n_folds,
        labeled_ratio=cfg['labeled_ratio'],
        seed=42
    )

    if fold_splits is None:
        print("Error creating folds!")
        return

    #train on each fold
    all_fold_metrics = []

    for fold_idx, fold_split in enumerate(fold_splits, 1):
        fold_metrics = train_one_fold(
            fold_split,
            fold_idx,
            cfg,
            device,
            args.save_path
        )
        all_fold_metrics.append(fold_metrics)

    #compute cross-validation statistics
    print(f"\n{'='*80}")
    print(f"K-FOLD CROSS-VALIDATION RESULTS ({args.n_folds} FOLDS)")
    print(f"{'='*80}\n")

    #avg metrics
    metrics_summary = {}
    for metric_name in all_fold_metrics[0].keys():
        values = [fold[metric_name] for fold in all_fold_metrics]
        metrics_summary[metric_name] = {
            'mean': np.mean(values),
            'std': np.std(values),
            'min': np.min(values),
            'max': np.max(values),
            'values': values
        }

    #summary
    print("Cross-Validation Summary:")
    print("-"*80)
    for metric_name, stats in metrics_summary.items():
        print(f"\n{metric_name.upper()}:")
        print(f"  Mean ± Std: {stats['mean']:.4f} ± {stats['std']:.4f}")
        print(f"  Min: {stats['min']:.4f}")
        print(f"  Max: {stats['max']:.4f}")
        print(f"  Per-fold: {[f'{v:.4f}' for v in stats['values']]}")

    # Save summary
    with open(os.path.join(args.save_path, 'cv_summary.json'), 'w') as f:
        json.dump(metrics_summary, f, indent=2)

    print(f"\n{'='*80}")
    print("✓ K-Fold Cross-Validation Complete!")
    print(f"Results saved to: {args.save_path}")
    print(f"{'='*80}\n")

if __name__ == '__main__':
    main()

#!/usr/bin/env python3
#Visualize results
import os
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def plot_kfold_results(save_path='/content/my_project/exp/aircraft_unetpp_kfold'):

    # Load CV summary
    summary_path = '/content/my_project/exp/aircraft_unetpp_kfold/cv_summary.json';
    if not os.path.exists(summary_path):
        print(f"Error: {summary_path} not found!")
        return

    with open(summary_path, 'r') as f:
        cv_summary = json.load(f)

    # Extract metrics
    metrics = list(cv_summary.keys())
    n_folds = len(cv_summary[metrics[0]]['values'])

    print(f"\n Analyzing {n_folds}-Fold Cross-Validation Results")
    print("="*80)

#(1) bar plot
    fig, ax = plt.subplots(figsize=(12, 6))

    metric_names = []
    means = []
    stds = []

    for metric_name, stats in cv_summary.items():
        metric_names.append(metric_name.upper())
        means.append(stats['mean'])
        stds.append(stats['std'])

    x_pos = np.arange(len(metric_names))
    bars = ax.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7,
                  color='steelblue', edgecolor='black', linewidth=1.5)

    #Add labels on bars
    for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                f'{mean:.3f}±{std:.3f}',
                ha='center', va='bottom', fontweight='bold', fontsize=10)

    ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')
    ax.set_ylabel('Score', fontsize=12, fontweight='bold')
    ax.set_title(f'{n_folds}-Fold Cross-Validation Results\nMean ± Standard Deviation',
                 fontsize=14, fontweight='bold', pad=20)
    ax.set_xticks(x_pos)
    ax.set_xticklabels(metric_names, rotation=45, ha='right')
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim(0, 1.1)

    plt.tight_layout()
    plt.savefig('/content/my_project/exp/aircraft_unetpp_kfold/cv_bar_plot.png', dpi=150, bbox_inches='tight')
    plt.close()

    print(" Saved: cv_bar_plot.png")

    # (2) Box plot
    fig, ax = plt.subplots(figsize=(12, 6))

    data_for_box = []
    labels_for_box = []

    for metric_name, stats in cv_summary.items():
        data_for_box.append(stats['values'])
        labels_for_box.append(metric_name.upper())

    bp = ax.boxplot(data_for_box, labels=labels_for_box, patch_artist=True,
                    showmeans=True, meanline=True,
                    boxprops=dict(facecolor='lightblue', alpha=0.7),
                    medianprops=dict(color='red', linewidth=2),
                    meanprops=dict(color='green', linewidth=2, linestyle='--'),
                    whiskerprops=dict(linewidth=1.5),
                    capprops=dict(linewidth=1.5))

    ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')
    ax.set_ylabel('Score', fontsize=12, fontweight='bold')
    ax.set_title(f'{n_folds}-Fold Cross-Validation - Distribution Across Folds\n'
                 f'Red: Median | Green Dashed: Mean',
                 fontsize=14, fontweight='bold', pad=20)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig('/content/my_project/exp/aircraft_unetpp_kfold/cv_box_plot.png', dpi=150, bbox_inches='tight')
    plt.close()

    print("✓ Saved: cv_box_plot.png")

    # (3) Heatmap with per-fold performance
    fig, ax = plt.subplots(figsize=(10, 6))

    fold_data = []
    for fold_idx in range(n_folds):
        fold_metrics = []
        for metric_name in cv_summary.keys():
            fold_metrics.append(cv_summary[metric_name]['values'][fold_idx])
        fold_data.append(fold_metrics)

    fold_data = np.array(fold_data)

    # Create heatmap
    sns.heatmap(fold_data, annot=True, fmt='.3f', cmap='YlGnBu',
                xticklabels=[m.upper() for m in cv_summary.keys()],
                yticklabels=[f'Fold {i+1}' for i in range(n_folds)],
                cbar_kws={'label': 'Score'},
                linewidths=0.5, linecolor='gray',
                ax=ax)

    ax.set_title(f'{n_folds}-Fold Cross-Validation - Per-Fold Performance',
                 fontsize=14, fontweight='bold', pad=20)
    ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')
    ax.set_ylabel('Fold', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig('/content/my_project/exp/aircraft_unetpp_kfold/cv_heatmap.png', dpi=150, bbox_inches='tight')
    plt.close()

    print("✓ Saved: cv_heatmap.png")

    # (4) Load and plot training history for each fold
    print("\n Loading training history...")

    # Check if any history files exist
    history_files_found = 0
    for fold_idx in range(1, n_folds + 1):
        history_path = os.path.join(save_path, f'fold_{fold_idx}', 'history.json')
        if os.path.exists(history_path):
            history_files_found += 1
            print(f"   Found: fold_{fold_idx}/history.json")
        else:
            print(f"   Missing: fold_{fold_idx}/history.json")

    if history_files_found == 0:
        print("\n  No history files found! Skipping training history plot.")
        print("   This happens if training was interrupted or eval_interval > epochs")
    else:
        print(f"\n Creating training history plot from {history_files_found} folds...")

        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        axes = axes.flatten()

        metrics_to_plot = ['val_dice', 'val_iou', 'val_accuracy',
                           'val_precision', 'val_recall', 'train_loss']

        for metric_idx, metric_name in enumerate(metrics_to_plot):
            ax = axes[metric_idx]
            plots_added = 0

            for fold_idx in range(1, n_folds + 1):
                history_path = os.path.join(save_path, f'fold_{fold_idx}', 'history.json')
                if os.path.exists(history_path):
                    try:
                        with open(history_path, 'r') as f:
                            history = json.load(f)

                        if len(history) > 0 and metric_name in history[0]:
                            epochs = [h['epoch'] for h in history]
                            values = [h[metric_name] for h in history]

                            ax.plot(epochs, values, marker='o', label=f'Fold {fold_idx}',
                                   linewidth=2, markersize=4, alpha=0.7)
                            plots_added += 1
                        else:
                            print(f"      Fold {fold_idx}: No data for {metric_name}")
                    except Exception as e:
                        print(f"      Fold {fold_idx}: Error reading history - {e}")

            ax.set_xlabel('Epoch', fontsize=10, fontweight='bold')
            ax.set_ylabel(metric_name.replace('_', ' ').title(), fontsize=10, fontweight='bold')
            ax.set_title(metric_name.replace('_', ' ').title(), fontsize=12, fontweight='bold')

            if plots_added > 0:
                ax.legend(loc='best', fontsize=8)
            else:
                ax.text(0.5, 0.5, 'No Data', ha='center', va='center',
                       transform=ax.transAxes, fontsize=12)

            ax.grid(alpha=0.3, linestyle='--')

        plt.suptitle(f'{n_folds}-Fold Cross-Validation - Training History',
                     fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig('/content/my_project/exp/aircraft_unetpp_kfold/cv_training_history.png', dpi=150, bbox_inches='tight')
        plt.close()

        print("✓ Saved: cv_training_history.png")



    # (5) Summary Table
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.axis('tight')
    ax.axis('off')

    table_data = []
    table_data.append(['Metric', 'Mean', 'Std', 'Min', 'Max', '95% CI'])

    for metric_name, stats in cv_summary.items():
        mean = stats['mean']
        std = stats['std']
        ci_95 = 1.96 * std / np.sqrt(n_folds)  # 95% confidence interval

        table_data.append([
            metric_name.upper(),
            f"{mean:.4f}",
            f"{std:.4f}",
            f"{stats['min']:.4f}",
            f"{stats['max']:.4f}",
            f"[{mean-ci_95:.4f}, {mean+ci_95:.4f}]"
        ])

    table = ax.table(cellText=table_data, cellLoc='center', loc='center',
                    colWidths=[0.2, 0.15, 0.15, 0.15, 0.15, 0.2])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)

    for i in range(6):
        table[(0, i)].set_facecolor('#4472C4')
        table[(0, i)].set_text_props(weight='bold', color='white')

    for i in range(1, len(table_data)):
        for j in range(6):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#E7E6E6')

    plt.title(f'{n_folds}-Fold Cross-Validation - Statistical Summary',
              fontsize=14, fontweight='bold', pad=20)
    plt.savefig('/content/my_project/exp/aircraft_unetpp_kfold/cv_statistics_table.png', dpi=150, bbox_inches='tight')
    plt.close()

    print(" Saved: cv_statistics_table.png")

    print(f"\n{'='*80}")
    print(f"STATISTICAL SUMMARY")
    print(f"{'='*80}\n")

    for metric_name, stats in cv_summary.items():
        mean = stats['mean']
        std = stats['std']
        ci_95 = 1.96 * std / np.sqrt(n_folds)

        print(f"{metric_name.upper()}:")
        print(f"  Mean +- Std: {mean:.4f} ± {std:.4f}")
        print(f"  Range: [{stats['min']:.4f}, {stats['max']:.4f}]")
        print(f"  95% CI: [{mean-ci_95:.4f}, {mean+ci_95:.4f}]")
        print(f"  Coefficient of Variation: {(std/mean)*100:.2f}%")
        print()

    # find best fold
    best_fold_idx = np.argmax(cv_summary['dice']['values']) + 1
    best_dice = cv_summary['dice']['values'][best_fold_idx - 1]

    print(f"{'='*80}")
    print(f"BEST PERFORMING FOLD: Fold {best_fold_idx}")
    print(f"  Dice Score: {best_dice:.4f}")
    print(f"{'='*80}\n")

    print("All visualizations saved!")
    print(f"  Location: {save_path}/")

if __name__ == '__main__':
    import sys
    save_path = sys.argv[1] if len(sys.argv) > 1 else 'exp/aircraft_unetpp_kfold'
    plot_kfold_results(save_path)